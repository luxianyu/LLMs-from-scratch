{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b",
   "metadata": {
    "id": "c024bfa4-1a7a-4751-b5a1-827225a3478b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "以下代码为 <a href=\"http://mng.bz/orYv\">《从零开始构建大型语言模型》</a> 一书的补充代码，作者为 <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>中文翻译和代码详细注释由Lux整理，Github下载地址：<a href=\"https://github.com/luxianyu\">https://github.com/luxianyu</a>\n",
    "    \n",
    "<br>Lux的Github上还有吴恩达深度学习Pytorch版学习笔记及中文详细注释的代码下载\n",
    "    \n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabadb8-5935-45ff-b39c-db7a29012129",
   "metadata": {
    "id": "bfabadb8-5935-45ff-b39c-db7a29012129"
   },
   "source": [
    "# 第6章：文本分类的微调\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "9495f150-9d79-4910-d6e7-6c0d9aae4a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.0\n",
      "numpy version: 2.1.3\n",
      "tiktoken version: 0.12.0\n",
      "torch version: 2.9.0+cpu\n",
      "tensorflow version: 2.20.0\n",
      "pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",  # Plotting library\n",
    "        \"numpy\",       # PyTorch & TensorFlow dependency\n",
    "        \"tiktoken\",    # Tokenizer\n",
    "        \"torch\",       # Deep learning library\n",
    "        \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "        \"pandas\"       # Dataset loading\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445828a-ff10-4efa-9f60-a2e2aed4c87d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/01.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c",
   "metadata": {
    "id": "3a84cf35-b37f-4c15-8972-dfafc9fadc1c"
   },
   "source": [
    "## 6.1 微调的不同类型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3d731-5123-4f02-accd-c670ce50a5a3",
   "metadata": {
    "id": "ede3d731-5123-4f02-accd-c670ce50a5a3"
   },
   "source": [
    "- 本节没有代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac45579d-d485-47dc-829e-43be7f4db57b",
   "metadata": {},
   "source": [
    "- 微调语言模型的最常见方法是**指令微调（instruction-finetuning）**和**分类微调（classification finetuning）**。  \n",
    "- 下图所示的**指令微调**将是下一章的主题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29ef42-46d9-43d4-8bb4-94974e1665e4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/02.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f60321-95b8-46a9-97bf-1d07fda2c3dd",
   "metadata": {},
   "source": [
    "- **分类微调（classification finetuning）**是本章的主题。如果你有机器学习背景，这个过程可能会让你感到熟悉 —— 它类似于训练一个卷积神经网络（CNN）来识别手写数字。  \n",
    "- 在分类微调中，模型会输出若干预定义的类别标签（例如，“垃圾邮件（spam）”和“非垃圾邮件（not spam）”）。  \n",
    "- 分类微调后的模型只能预测在训练中出现过的类别（例如只能判断“垃圾邮件”或“非垃圾邮件”），而**指令微调模型（instruction-finetuned model）**通常可以执行多种不同任务。  \n",
    "- 我们可以将分类微调后的模型视为**高度专用的模型（specialized model）**；在实践中，创建一个专用模型往往比创建一个在多任务上都表现良好的**通用模型（generalist model）**要容易得多。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37a0c4-0bb1-4061-b1fe-eaa4416d52c3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/03.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
   "metadata": {
    "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
   },
   "source": [
    "## 6.2 数据集准备\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f628975-d2e8-4f7f-ab38-92bb868b7067",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/04.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d",
   "metadata": {
    "id": "9fbd459f-63fa-4d8c-8499-e23103156c7d"
   },
   "source": [
    "- 本节将准备用于**分类微调（classification finetuning）**的数据集。  \n",
    "- 我们将使用一个包含**垃圾短信（spam）**与**非垃圾短信（non-spam）**的文本数据集，对LLM进行微调，使其能够判断短信是否为垃圾信息。  \n",
    "- 首先，我们需要下载并解压该数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "424e4423-f623-443c-ab9e-656f9e867559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection\\SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 导入必要的 Python 库\n",
    "# ===============================\n",
    "import requests  # 用于 HTTP 下载\n",
    "import zipfile   # 用于解压 ZIP 文件\n",
    "import os        # 用于文件操作，如重命名\n",
    "from pathlib import Path  # 提供面向对象的路径操作\n",
    "\n",
    "# ===============================\n",
    "# 定义下载和解压所需的文件路径和 URL\n",
    "# ===============================\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"  \n",
    "# 数据集原始下载地址\n",
    "zip_path = \"sms_spam_collection.zip\"      # 下载到本地的 ZIP 文件名\n",
    "extracted_path = \"sms_spam_collection\"    # 解压后的文件夹路径\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"  \n",
    "# 最终目标文件（TSV 格式）\n",
    "\n",
    "# ===============================\n",
    "# 定义下载并解压数据集的函数\n",
    "# ===============================\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    \"\"\"\n",
    "    下载 SMS Spam Collection 数据集并解压。\n",
    "    \n",
    "    参数：\n",
    "    - url: 数据集下载 URL\n",
    "    - zip_path: 本地 ZIP 文件保存路径\n",
    "    - extracted_path: 解压文件夹路径\n",
    "    - data_file_path: 最终 TSV 文件路径\n",
    "    \"\"\"\n",
    "    if data_file_path.exists():  # 检查文件是否已存在\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return  # 如果文件存在则跳过下载和解压\n",
    "\n",
    "    # ------------------------------\n",
    "    # 下载文件\n",
    "    # ------------------------------\n",
    "    response = requests.get(url, stream=True, timeout=60)  \n",
    "    # stream=True 分块下载，timeout=60 秒防止长时间无响应\n",
    "    response.raise_for_status()  # 如果 HTTP 请求失败则抛出异常\n",
    "    with open(zip_path, \"wb\") as out_file:  # 以二进制写模式打开本地 ZIP 文件\n",
    "        for chunk in response.iter_content(chunk_size=8192):  \n",
    "            # 分块写入文件，每次 8192 字节\n",
    "            if chunk:  # 如果当前块非空\n",
    "                out_file.write(chunk)  # 写入文件\n",
    "\n",
    "    # ------------------------------\n",
    "    # 解压 ZIP 文件\n",
    "    # ------------------------------\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)  # 解压到指定文件夹\n",
    "\n",
    "    # ------------------------------\n",
    "    # 重命名解压后的文件为 TSV 格式\n",
    "    # ------------------------------\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)  # 添加 .tsv 后缀\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 尝试下载和解压，若主 URL 失败则使用备用 URL\n",
    "# ===============================\n",
    "try:\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (requests.exceptions.RequestException, TimeoutError) as e:\n",
    "    print(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "    url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "    download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 说明：\n",
    "# 原书使用 urllib.request 进行下载，但在使用 VPN 或 HTTPS 协议时可能失败\n",
    "# 使用 requests 库更稳定，并支持分块下载和超时控制\n",
    "# 上面的代码实现了：\n",
    "# 1️ 检查文件是否已存在，避免重复下载\n",
    "# 2️ 下载 ZIP 文件\n",
    "# 3️ 解压 ZIP 文件到指定目录\n",
    "# 4️ 重命名为带 .tsv 的文件，方便后续读取\n",
    "# ===============================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1",
   "metadata": {
    "id": "6aac2d19-06d0-4005-916b-0bd4b1ee50d1"
   },
   "source": [
    "- 该数据集以制表符分隔的文本文件（tab-separated text file）形式保存，我们可以将其加载到 pandas DataFrame 中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "da0ed4da-ac31-4e4d-8bdd-2153be4656a4",
    "outputId": "a16c5cde-d341-4887-a93f-baa9bec542ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# 导入 pandas 库\n",
    "# ===============================\n",
    "import pandas as pd  # 用于数据处理和分析\n",
    "\n",
    "# ===============================\n",
    "# 读取 SMS Spam Collection 数据集\n",
    "# ===============================\n",
    "df = pd.read_csv(\n",
    "    data_file_path,   # 数据文件路径，之前下载并重命名为 TSV 文件\n",
    "    sep=\"\\t\",         # 数据列之间的分隔符为制表符（tab）\n",
    "    header=None,      # 文件没有表头行，所以设置 header=None\n",
    "    names=[\"Label\", \"Text\"]  # 手动为两列命名：Label 为类别（spam 或 ham），Text 为短信内容\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 查看数据框\n",
    "# ===============================\n",
    "df  # 输出整个 DataFrame，用于在 Jupyter Notebook 中展示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109",
   "metadata": {
    "id": "e7b6e631-4f0b-4aab-82b9-8898e6663109"
   },
   "source": [
    "- 查看类别分布时，我们会发现数据中 **“ham”（即非垃圾邮件）** 的数量远多于 **“spam”（垃圾邮件）**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "495a5280-9d7c-41d4-9719-64ab99056d4c",
    "outputId": "761e0482-43ba-4f46-f4b7-6774dae51b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 查看标签类别的分布情况\n",
    "# ===============================\n",
    "# df[\"Label\"]：取出数据框中的 \"Label\" 列\n",
    "# .value_counts()：统计每个类别出现的次数\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773f054-0bdc-4aad-bbf6-397621bf63db",
   "metadata": {
    "id": "f773f054-0bdc-4aad-bbf6-397621bf63db"
   },
   "source": [
    "- 为了简化处理，并且出于教学目的我们希望使用一个较小的数据集（这样可以更快地微调 LLM），我们对数据集进行了**下采样（undersample）**，使每个类别各包含 747 个样本。  \n",
    "- （除了下采样之外，还有其他几种处理类别不平衡的方法，但这些内容超出了本书的范围；你可以在 [`imbalanced-learn` 用户指南](https://imbalanced-learn.org/stable/user_guide.html) 中找到示例和更多信息。）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be4a0a2-9704-4a96-b38f-240339818688",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7be4a0a2-9704-4a96-b38f-240339818688",
    "outputId": "396dc415-cb71-4a88-e85d-d88201c6d73f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 创建一个平衡的数据集（spam 与 ham 样本数量相同）\n",
    "# ===============================\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "    \"\"\"\n",
    "    输入：\n",
    "        df: 原始 pandas 数据框，包含 'Label' 和 'Text' 列\n",
    "    输出：\n",
    "        balanced_df: 平衡后的数据框，spam 与 ham 数量相等\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------\n",
    "    # 1️ 计算 spam 样本数量\n",
    "    # ------------------------------\n",
    "    # df[\"Label\"] == \"spam\"：布尔索引，筛选所有 spam 样本\n",
    "    # df[...].shape[0]：返回行数，即 spam 样本数\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2️ 随机抽取等量的 ham 样本\n",
    "    # ------------------------------\n",
    "    # df[df[\"Label\"] == \"ham\"]：筛选出所有 ham 样本\n",
    "    # .sample(num_spam, random_state=123)：随机抽取与 spam 数量相同的 ham 样本\n",
    "    # random_state=123：设置随机种子，保证每次抽样结果相同\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3️ 合并抽样的 ham 样本和所有 spam 样本\n",
    "    # ------------------------------\n",
    "    # pd.concat([...])：按行合并两个数据框，形成平衡数据集\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "# 创建平衡数据集\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "\n",
    "# 查看平衡后的标签分布\n",
    "print(balanced_df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6",
   "metadata": {
    "id": "d3fd2f5a-06d8-4d30-a2e3-230b86c559d6"
   },
   "source": [
    "- 接下来，我们将字符串类别标签 **“ham”** 和 **“spam”** 转换为整数类别标签 0 和 1：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd",
   "metadata": {
    "id": "c1b10c3d-5d57-42d0-8de8-cf80a06f5ffd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Label                                               Text\n",
      "4307      0  Awww dat is sweet! We can think of something t...\n",
      "4138      0                             Just got to  &lt;#&gt;\n",
      "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
      "4461      0  This is wishing you a great day. Moji told me ...\n",
      "5440      0      Thank you. do you generally date the brothas?\n",
      "Label\n",
      "0    747\n",
      "1    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 将标签从字符串转换为数值\n",
    "# ===============================\n",
    "\n",
    "# balanced_df[\"Label\"].map({...})：\n",
    "# 对 \"Label\" 列中的每个元素应用映射规则：\n",
    "# \"ham\" -> 0，\"spam\" -> 1\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# 检查转换结果\n",
    "print(balanced_df.head())\n",
    "print(balanced_df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f7f062-ef4e-4020-8275-71990cab4414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>1</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>1</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>1</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?\n",
       "...     ...                                                ...\n",
       "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f",
   "metadata": {
    "id": "5715e685-35b4-4b45-a86c-8a8694de9d6f"
   },
   "source": [
    "- 现在我们来定义一个函数，将数据集随机划分为训练集、验证集和测试集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uQl0Psdmx15D",
   "metadata": {
    "id": "uQl0Psdmx15D"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 随机划分数据集为训练集、验证集和测试集\n",
    "# ===============================\n",
    "\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    \"\"\"\n",
    "    将数据集随机划分为训练集、验证集和测试集\n",
    "\n",
    "    参数：\n",
    "    - df: pandas DataFrame，包含样本和标签\n",
    "    - train_frac: float，训练集占比（0~1）\n",
    "    - validation_frac: float，验证集占比（0~1）\n",
    "      测试集占比由剩余部分计算：1 - train_frac - validation_frac\n",
    "\n",
    "    返回：\n",
    "    - train_df: 训练集 DataFrame\n",
    "    - validation_df: 验证集 DataFrame\n",
    "    - test_df: 测试集 DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # 1️ 打乱整个 DataFrame，保证样本顺序随机\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    # sample(frac=1) -> 返回原 DataFrame 的全部样本，但顺序随机\n",
    "    # random_state=123 -> 固定随机种子，保证可复现\n",
    "    # reset_index(drop=True) -> 重置索引，丢弃原索引\n",
    "\n",
    "    # 2️ 计算切分索引\n",
    "    train_end = int(len(df) * train_frac)                # 训练集结束索引\n",
    "    validation_end = train_end + int(len(df) * validation_frac)  # 验证集结束索引\n",
    "\n",
    "    # 3️ 根据索引切分 DataFrame\n",
    "    train_df = df[:train_end]                 # 前 train_end 行作为训练集\n",
    "    validation_df = df[train_end:validation_end] # 接下来的行作为验证集\n",
    "    test_df = df[validation_end:]             # 剩余行作为测试集\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 调用函数进行划分\n",
    "# ===============================\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# 训练集占 70%，验证集占 10%，测试集占剩余 20%\n",
    "\n",
    "# ===============================\n",
    "# 保存 CSV 文件\n",
    "# ===============================\n",
    "train_df.to_csv(\"train.csv\", index=None)         # 保存训练集\n",
    "validation_df.to_csv(\"validation.csv\", index=None)  # 保存验证集\n",
    "test_df.to_csv(\"test.csv\", index=None)           # 保存测试集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7a0c5-1d5f-458a-b685-3f49520b0094",
   "metadata": {},
   "source": [
    "## 6.3 创建数据加载器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c",
   "metadata": {
    "id": "7126108a-75e7-4862-b0fb-cbf59a18bb6c"
   },
   "source": [
    "- 注意，短信的长度各不相同；如果我们希望将多个训练样本组合成一个批次（batch），必须选择以下两种方法之一：  \n",
    "  1. 将所有消息截断（truncate）到数据集中或批次中最短消息的长度  \n",
    "  2. 将所有消息填充（pad）到数据集中或批次中最长消息的长度  \n",
    "\n",
    "- 我们选择方法 2，将所有消息填充到数据集中最长消息的长度。  \n",
    "- 为此，我们使用 `<|endoftext|>` 作为填充 token，正如第 2 章所讨论的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829f33f-1428-4f22-9886-7fee633b3666",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/06.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
    "outputId": "b5b48439-32c8-4b37-cca2-c9dc8fa86563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 导入 tiktoken 并获取 GPT-2 编码器\n",
    "# ===============================\n",
    "import tiktoken\n",
    "\n",
    "# 获取 GPT-2 默认编码器对象\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# get_encoding(\"gpt2\") -> 返回一个编码器对象，可将文本转换为 token ID\n",
    "\n",
    "# ===============================\n",
    "# 编码特殊 token \"<|endoftext|>\"\n",
    "# ===============================\n",
    "encoded_ids = tokenizer.encode(\n",
    "    \"<|endoftext|>\",                      # 需要编码的文本\n",
    "    allowed_special={\"<|endoftext|>\"}    # 指定这是允许的特殊 token，避免报错\n",
    ")\n",
    "\n",
    "# 打印编码结果\n",
    "print(encoded_ids)\n",
    "# 输出类似 [50256]，表示 \"<|endoftext|>\" 对应的 token ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f582ff-68bf-450e-bd87-5fb61afe431c",
   "metadata": {
    "id": "04f582ff-68bf-450e-bd87-5fb61afe431c"
   },
   "source": [
    "- 下面的 `SpamDataset` 类会识别训练数据集中最长的序列，并将填充 token 添加到其他序列，使它们的长度与最长序列一致。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7791b52-af18-4ac4-afa9-b921068e383e",
   "metadata": {
    "id": "d7791b52-af18-4ac4-afa9-b921068e383e"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 自定义 PyTorch Dataset 用于 Spam 分类任务\n",
    "# ===============================\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset 类，用于将 SMS/Spam 文本数据编码为 token ID，并返回 label。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        \n",
    "        参数：\n",
    "        - csv_file: str, CSV 文件路径，文件应包含 \"Text\" 和 \"Label\" 列\n",
    "        - tokenizer: tiktoken 编码器对象，用于将文本转为 token ID\n",
    "        - max_length: int or None，序列最大长度，如果为 None，则自动使用最长文本长度\n",
    "        - pad_token_id: int，用于 padding 的 token ID（默认 50256，对应 GPT-2 的 <|endoftext|>）\n",
    "        \"\"\"\n",
    "        # 读取 CSV 数据\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # ------------------------------\n",
    "        # 预先对所有文本进行编码\n",
    "        # ------------------------------\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text)  # 将每条文本转换为 token ID 列表\n",
    "            for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # 处理序列长度\n",
    "        # ------------------------------\n",
    "        if max_length is None:\n",
    "            # 如果没有指定最大长度，使用数据集中最长文本长度\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # 如果文本长度超过 max_length，则截断\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # ------------------------------\n",
    "        # Padding: 将每条文本补齐到 max_length\n",
    "        # ------------------------------\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        返回指定索引的数据样本\n",
    "        \n",
    "        返回值：\n",
    "        - encoded: torch.LongTensor, 形状 [max_length]，文本的 token ID\n",
    "        - label: torch.LongTensor, 形状 [], 文本对应的标签 (0 或 1)\n",
    "        \"\"\"\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集总样本数\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        \"\"\"\n",
    "        计算数据集中最长的 token ID 序列长度\n",
    "        \"\"\"\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "        # 更 pythonic 的写法：\n",
    "        # return max(len(encoded_text) for encoded_text in self.encoded_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uzj85f8ou82h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzj85f8ou82h",
    "outputId": "d08f1cf0-c24d-445f-a3f8-793532c3716f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 创建训练数据集实例\n",
    "# ===============================\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",  # 训练集 CSV 文件路径\n",
    "    max_length=None,       # 不指定最大长度，自动使用训练集最长文本的 token 数\n",
    "    tokenizer=tokenizer    # 使用 GPT-2 的 tiktoken 编码器\n",
    ")\n",
    "\n",
    "# 输出训练集中的最长序列长度（以 token 数计）\n",
    "print(train_dataset.max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdd932-97eb-4b88-9cf9-d766ea4c3a60",
   "metadata": {},
   "source": [
    "- 我们还将验证集（validation set）和测试集（test set）填充到训练集中最长序列的长度。  \n",
    "- 注意，对于验证集和测试集中比训练集最长序列还长的样本，`SpamDataset` 代码会通过 `encoded_text[:self.max_length]` 将其截断。  \n",
    "- 这种做法完全是可选的，如果在验证集和测试集中将 `max_length=None`，同样也能很好地工作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e",
   "metadata": {
    "id": "bb0c502d-a75e-4248-8ea0-196e2b00c61e"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 创建验证集和测试集的数据集实例\n",
    "# ===============================\n",
    "\n",
    "# 验证集\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",        # 验证集 CSV 文件路径\n",
    "    max_length=train_dataset.max_length,  # 使用训练集的最长序列长度进行统一 padding/truncate\n",
    "    tokenizer=tokenizer               # 使用 GPT-2 的 tiktoken 编码器\n",
    ")\n",
    "\n",
    "# 测试集\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",              # 测试集 CSV 文件路径\n",
    "    max_length=train_dataset.max_length,  # 同样使用训练集的最长序列长度\n",
    "    tokenizer=tokenizer               # 使用 GPT-2 的 tiktoken 编码器\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20170d89-85a0-4844-9887-832f5d23432a",
   "metadata": {},
   "source": [
    "- 接下来，我们使用数据集实例化数据加载器，这与前几章创建数据加载器的方式类似\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bcc349-205f-48f8-9655-95ff21f5e72f",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/07.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
    "outputId": "3266c410-4fdb-4a8c-a142-7f707e2525ab"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 使用 PyTorch DataLoader 创建批量数据迭代器\n",
    "# ===============================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ------------------------------\n",
    "# 配置参数\n",
    "# ------------------------------\n",
    "num_workers = 0        # 数据加载的子进程数量，0 表示主进程加载数据\n",
    "batch_size = 8         # 每个 batch 的样本数量\n",
    "\n",
    "# 设置随机种子，保证数据 shuffle 可复现\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# ===============================\n",
    "# 训练集 DataLoader\n",
    "# ===============================\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,   # 数据源为训练集\n",
    "    batch_size=batch_size,   # 每个 batch 8 个样本\n",
    "    shuffle=True,            # 每个 epoch 训练前打乱顺序\n",
    "    num_workers=num_workers, # 数据加载子进程数量\n",
    "    drop_last=True,          # 如果最后一个 batch 不满 batch_size，则丢弃\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 验证集 DataLoader\n",
    "# ===============================\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,     # 数据源为验证集\n",
    "    batch_size=batch_size,   # 每个 batch 8 个样本\n",
    "    num_workers=num_workers, # 数据加载子进程数量\n",
    "    drop_last=False,         # 保留最后一个可能不满 batch 的 batch\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 测试集 DataLoader\n",
    "# ===============================\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,    # 数据源为测试集\n",
    "    batch_size=batch_size,   # 每个 batch 8 个样本\n",
    "    num_workers=num_workers, # 数据加载子进程数量\n",
    "    drop_last=False,         # 保留最后一个可能不满 batch 的 batch\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 说明\n",
    "# ===============================\n",
    "# 1. train_loader: 用于训练，每个 epoch 打乱顺序，保证模型不会看到固定顺序。\n",
    "# 2. val_loader & test_loader: 用于验证和测试，不打乱顺序，以保证评估结果稳定。\n",
    "# 3. batch_size: 控制每次梯度更新使用的样本数量。\n",
    "# 4. drop_last: 避免训练时最后一个不完整 batch 影响梯度计算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {},
   "source": [
    "- 作为验证步骤，我们遍历数据加载器（data loaders），确保每个批次（batch）包含 8 个训练样本，每个训练样本包含 120 个 token。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 查看训练集 DataLoader 输出的维度\n",
    "# ===============================\n",
    "\n",
    "print(\"Train loader:\")\n",
    "\n",
    "# 遍历训练集 DataLoader 取一个 batch\n",
    "for input_batch, target_batch in train_loader:\n",
    "    # input_batch: shape -> (batch_size, max_length)\n",
    "    # target_batch: shape -> (batch_size,)\n",
    "    pass  # 此处只取最后一个 batch 作为示例\n",
    "\n",
    "# 输出输入数据的形状\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "# 输出标签数据的形状\n",
    "print(\"Label batch dimensions\", target_batch.shape)\n",
    "\n",
    "# ===============================\n",
    "# 注解说明\n",
    "# ===============================\n",
    "# input_batch.shape -> (batch_size, max_length)\n",
    "#   - batch_size: 每个 batch 的样本数量 (本例为 8)\n",
    "#   - max_length: 文本序列的长度（已 pad/truncate 到相同长度）\n",
    "#\n",
    "# target_batch.shape -> (batch_size,)\n",
    "#   - batch_size: 每个 batch 的样本数量\n",
    "#   - 每个元素为 0 或 1，表示对应文本的标签（ham 或 spam）\n",
    "#\n",
    "# 注意：\n",
    "# - DataLoader 自动将 Dataset 返回的单个样本组合成 batch\n",
    "# - input_batch 已经是 tensor 类型，无需再手动转换\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {},
   "source": [
    "- 最后，我们打印每个数据集中的总批次数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "6934bbf2-9797-4fbe-d26b-1a246e18c2fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 输出每个 DataLoader 的 batch 数量\n",
    "# ===============================\n",
    "\n",
    "# len(train_loader) 返回训练集 DataLoader 中总共有多少个 batch\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "\n",
    "# len(val_loader) 返回验证集 DataLoader 中总共有多少个 batch\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "\n",
    "# len(test_loader) 返回测试集 DataLoader 中总共有多少个 batch\n",
    "print(f\"{len(test_loader)} test batches\")\n",
    "\n",
    "# ===============================\n",
    "# 注解说明\n",
    "# ===============================\n",
    "# DataLoader 会根据：\n",
    "# - 数据集总样本数 (len(dataset))\n",
    "# - batch_size\n",
    "# - drop_last 参数\n",
    "# 来计算总的 batch 数量\n",
    "#\n",
    "# 计算公式：\n",
    "# 如果 drop_last=False:\n",
    "#   batch_count = ceil(total_samples / batch_size)\n",
    "# 如果 drop_last=True:\n",
    "#   batch_count = floor(total_samples / batch_size)\n",
    "#\n",
    "# 例如：\n",
    "# - train_dataset 样本数 = 1000, batch_size = 8, drop_last=True\n",
    "#   -> 1000 // 8 = 125 个 batch\n",
    "# - val_dataset 样本数 = 100, batch_size = 8, drop_last=False\n",
    "#   -> ceil(100 / 8) = 13 个 batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c",
   "metadata": {
    "id": "d1c4f61a-5f5d-4b3b-97cf-151b617d1d6c"
   },
   "source": [
    "## 6.4 使用预训练权重初始化模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1af8b-8bd1-4b44-8b8b-dc031496e208",
   "metadata": {},
   "source": [
    "- 在本节中，我们初始化上一章中使用的预训练模型（pretrained model）。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/08.webp\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2992d779-f9fb-4812-a117-553eb790a5a9",
   "metadata": {
    "id": "2992d779-f9fb-4812-a117-553eb790a5a9"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 选择要使用的 GPT 模型和输入提示\n",
    "# ===============================\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"  # 选择模型\n",
    "INPUT_PROMPT = \"Every effort moves\"  # 输入文本，用于后续生成示例\n",
    "\n",
    "# ===============================\n",
    "# 基础配置\n",
    "# ===============================\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小，GPT-2 的默认词汇量\n",
    "    \"context_length\": 1024,  # 上下文窗口长度（最大可处理的 token 数）\n",
    "    \"drop_rate\": 0.0,        # dropout 比例，训练中可防止过拟合\n",
    "    \"qkv_bias\": True          # 是否在 Query/Key/Value 矩阵中添加偏置\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# 模型具体参数字典\n",
    "# ===============================\n",
    "# 对应不同大小的 GPT-2 模型，包括嵌入维度、层数、注意力头数\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# ===============================\n",
    "# 将基础配置更新为所选模型的配置\n",
    "# ===============================\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "# 现在 BASE_CONFIG 包含：\n",
    "# {\n",
    "#     \"vocab_size\": 50257,\n",
    "#     \"context_length\": 1024,\n",
    "#     \"drop_rate\": 0.0,\n",
    "#     \"qkv_bias\": True,\n",
    "#     \"emb_dim\": 768,\n",
    "#     \"n_layers\": 12,\n",
    "#     \"n_heads\": 12\n",
    "# }\n",
    "\n",
    "# ===============================\n",
    "# 检查训练数据集的最大长度是否超过模型的上下文长度\n",
    "# ===============================\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")\n",
    "# 如果训练数据的最大 token 数大于模型的 context_length，会抛出 AssertionError\n",
    "# 提示用户重新初始化数据集，设置 max_length 不超过模型的 context_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "022a649a-44f5-466c-8a8e-326c063384f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "022a649a-44f5-466c-8a8e-326c063384f5",
    "outputId": "7091e401-8442-4f47-a1d9-ecb42a1ef930"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 44.5kiB/s]\n",
      "encoder.json: 100%|██████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 1.28MiB/s]\n",
      "hparams.json: 100%|████████████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 24.1kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████████████████████████████████████████| 498M/498M [03:33<00:00, 2.33MiB/s]\n",
      "model.ckpt.index: 100%|██████████████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 2.64MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████████████████████████████████████████████| 471k/471k [00:00<00:00, 842kiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 795kiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# 从 gpt_download 模块导入下载和加载 GPT-2 权重的函数\n",
    "# ===============================\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "# ===============================\n",
    "# 从 previous_chapters 模块导入自定义的 GPT 模型类及加载权重函数\n",
    "# ===============================\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "# 注释：\n",
    "# 如果本地没有 previous_chapters.py，可以使用 llms-from-scratch 包\n",
    "# 例如：\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "\n",
    "# ===============================\n",
    "# 根据选择的模型名称提取模型大小（例如 \"124M\"）\n",
    "# ===============================\n",
    "# CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "# 这里使用字符串处理提取括号中的模型参数\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "# 结果: model_size = \"124M\"\n",
    "\n",
    "# ===============================\n",
    "# 下载并加载 GPT-2 预训练权重\n",
    "# ===============================\n",
    "# settings: 包含模型的配置设置，如嵌入维度、层数、注意力头等\n",
    "# params: 包含模型的权重参数字典\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "# models_dir 指定模型下载保存目录，这里是当前目录下的 \"gpt2\" 文件夹\n",
    "\n",
    "# ===============================\n",
    "# 使用自定义 GPTModel 类初始化模型实例\n",
    "# ===============================\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "# BASE_CONFIG 包含模型的词汇表大小、上下文长度、dropout、qkv_bias 以及嵌入维度、层数、注意力头数\n",
    "\n",
    "# ===============================\n",
    "# 将下载的 GPT-2 权重加载到自定义模型中\n",
    "# ===============================\n",
    "load_weights_into_gpt(model, params)\n",
    "# 该函数将下载的权重逐层赋值给模型，包括：\n",
    "# - 位置嵌入（pos_emb）\n",
    "# - 词嵌入（tok_emb）\n",
    "# - 多头注意力的 Q/K/V 权重与偏置\n",
    "# - 前馈网络的全连接层权重与偏置\n",
    "# - 层归一化参数\n",
    "# - 输出头权重与词嵌入共享\n",
    "\n",
    "# ===============================\n",
    "# 切换模型为评估模式（eval），在推理或生成文本时禁用 dropout\n",
    "# ===============================\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e056c-abe0-415f-b34d-df686204259e",
   "metadata": {},
   "source": [
    "- 为确保模型正确加载，我们来再次检查它是否能够生成连贯的文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c66be9f9-77da-4790-b73f-44f0503cb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# 详解：从 CHOOSE_MODEL 字符串中提取模型大小（如 \"124M\"）\n",
    "# -----------------------------------------------\n",
    "\n",
    "# 假设 CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "# 1️⃣ 先用 split(\" \") 按空格分割字符串：\n",
    "#    CHOOSE_MODEL.split(\" \") -> [\"gpt2-small\", \"(124M)\"]\n",
    "#    结果是一个列表，列表最后一个元素就是我们想要的 \"(124M)\"\n",
    "##last_element = CHOOSE_MODEL.split(\" \")[-1]\n",
    "# last_element = \"(124M)\"\n",
    "\n",
    "# 2️⃣ 使用 lstrip(\"(\") 去掉左侧的左括号 \"(\"\n",
    "#    \"(124M)\".lstrip(\"(\") -> \"124M)\"\n",
    "##stripped_left = last_element.lstrip(\"(\")\n",
    "# stripped_left = \"124M)\"\n",
    "\n",
    "# 3️⃣ 使用 rstrip(\")\") 去掉右侧的右括号 \")\"\n",
    "#    \"124M)\".rstrip(\")\") -> \"124M\"\n",
    "##model_size = stripped_left.rstrip(\")\")\n",
    "# model_size = \"124M\"\n",
    "\n",
    "# 🔹 最终结果：\n",
    "# model_size 变量就是我们想要的 GPT-2 模型大小字符串 \"124M\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8ac25ff-74b1-4149-8dc5-4c429d464330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 详解：使用 GPT-2 模型生成文本（简化版生成函数）\n",
    "# =======================================================\n",
    "\n",
    "# 导入上一章节提供的辅助函数：\n",
    "# 1️ generate_text_simple：根据模型和输入 token 生成后续 token\n",
    "# 2️ text_to_token_ids：将字符串文本转换为模型 token ID\n",
    "# 3️ token_ids_to_text：将 token ID 转换回可读文本\n",
    "from previous_chapters import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1️ 准备输入文本\n",
    "# -------------------------------------------------------\n",
    "text_1 = \"Every effort moves you\"  # 待生成文本的初始上下文（prompt）\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2️ 将文本转换为模型可处理的 token ID\n",
    "# -------------------------------------------------------\n",
    "# text_to_token_ids 函数会调用 GPT-2 tokenizer 将文本编码成整数 token 序列\n",
    "# 例如 \"Every\" -> 15496, \"effort\" -> 12345 等\n",
    "input_ids = text_to_token_ids(text_1, tokenizer)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3️ 使用 generate_text_simple 生成后续 token\n",
    "# -------------------------------------------------------\n",
    "# 参数详解：\n",
    "# - model: GPT-2 模型对象\n",
    "# - idx: 初始 token ID 序列\n",
    "# - max_new_tokens: 生成的最大 token 数量（这里是 15）\n",
    "# - context_size: 模型的上下文窗口大小（context length），决定了模型每次能看到多少历史 token\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=input_ids,                       # 输入的 token ID 序列\n",
    "    max_new_tokens=15,                   # 最多生成 15 个新 token\n",
    "    context_size=BASE_CONFIG[\"context_length\"]  # 上下文长度\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4️ 将生成的 token 序列转换回可读文本\n",
    "# -------------------------------------------------------\n",
    "# token_ids_to_text 会将整数 token 序列解码为字符串文本\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "# 输出生成的文本\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69162550-6a02-4ece-8db1-06c71d61946f",
   "metadata": {},
   "source": [
    "- 在将模型微调为分类器之前，让我们看看模型是否已经可以通过提示（prompting）对垃圾邮件消息进行分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94224aa9-c95a-4f8a-a420-76d01e3a800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 详解：用 GPT-2 对短信内容进行简单“是否垃圾短信”推理生成\n",
    "# =======================================================\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1️ 准备输入文本\n",
    "# -------------------------------------------------------\n",
    "# 我们构造一个带指示的 prompt，要求模型判断文本是否为垃圾短信（spam）\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "# prompt 的含义：\n",
    "# 1. 首句：\"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "#    - 指示模型执行二分类推理任务\n",
    "# 2. 第二句：短信内容\n",
    "#    - 模型根据短信内容判断其是否为垃圾短信\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2️ 将文本转换为模型 token ID\n",
    "# -------------------------------------------------------\n",
    "# text_to_token_ids 会将字符串转化为整数 token 序列\n",
    "# 例如 \"You\" -> 345, \"are\" -> 123 等\n",
    "input_ids = text_to_token_ids(text_2, tokenizer)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3️ 使用 generate_text_simple 生成模型回答\n",
    "# -------------------------------------------------------\n",
    "# 参数详解：\n",
    "# - model: GPT-2 模型对象\n",
    "# - idx: 初始 token ID 序列（prompt）\n",
    "# - max_new_tokens: 最多生成 23 个 token（即模型回答的长度）\n",
    "# - context_size: 模型上下文窗口大小（context length）\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=input_ids,                       # 输入 token ID\n",
    "    max_new_tokens=23,                   # 最大生成 23 个 token\n",
    "    context_size=BASE_CONFIG[\"context_length\"]  # 上下文长度\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4️ 将生成的 token 序列转换回可读文本\n",
    "# -------------------------------------------------------\n",
    "# token_ids_to_text 会将 token ID 解码为字符串\n",
    "output_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "# 输出模型生成的文本（即对垃圾短信的预测）\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce39ed0-2c77-410d-8392-dd15d4b22016",
   "metadata": {},
   "source": [
    "- 如我们所见，模型在遵循指令方面表现不佳。  \n",
    "- 这是预料之中的，因为它仅经过预训练（pretraining），尚未进行指令微调（instruction-finetuning，下一章将介绍）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522",
   "metadata": {
    "id": "4c9ae440-32f9-412f-96cf-fd52cc3e2522"
   },
   "source": [
    "## 6.5 添加分类头（Classification Head）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9d66f-76b2-40fc-9ec5-3f972a8db9c0",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/09.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bac05-78df-4412-bd80-612f8061c01d",
   "metadata": {},
   "source": [
    "- 在本节中，我们将对预训练的 LLM 进行修改，使其可以进行分类微调（classification finetuning）。  \n",
    "- 首先，让我们来看一下模型的架构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b23aff91-6bd0-48da-88f6-353657e6c981",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d8f7a01-b7c0-48d4-b1e7-8c12cc7ad932",
    "outputId": "b6a5b9b5-a92f-498f-d7cb-b58dd99e4497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f640a76-dd00-4769-9bc8-1aed0cec330d",
   "metadata": {},
   "source": [
    "- 上图展示了我们在第 4 章实现的模型架构，布局清晰。  \n",
    "- 我们的目标是替换并微调输出层（output layer）。  \n",
    "- 为此，我们首先**冻结模型（freeze the model）**，也就是将所有层设置为不可训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fkMWFl-0etea",
   "metadata": {
    "id": "fkMWFl-0etea"
   },
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# 详解：冻结 GPT-2 模型的所有参数\n",
    "# =======================================================\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1️ 遍历模型中所有参数\n",
    "# -------------------------------------------------------\n",
    "# model.parameters() 返回模型中所有可训练参数的生成器（generator）\n",
    "# 每个参数（param）是一个 torch.nn.Parameter 张量，\n",
    "# 包含了模型的权重矩阵或偏置项（bias）。\n",
    "for param in model.parameters():\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # 2️ 禁止这些参数在反向传播时被更新\n",
    "    # ---------------------------------------------------\n",
    "    # 每个参数都有一个属性 `requires_grad`，表示是否需要计算梯度。\n",
    "    # 当 requires_grad=True 时，PyTorch 会在反向传播时计算梯度；\n",
    "    # 当 requires_grad=False 时，该参数在训练时保持固定（冻结），不会被更新。\n",
    "    param.requires_grad = False\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3️ 目的与作用\n",
    "# -------------------------------------------------------\n",
    "# 这一步“冻结”模型所有参数，使得在后续微调（fine-tuning）过程中，\n",
    "# GPT-2 的原始权重保持不变，仅允许新增的层（例如分类头）参与训练。\n",
    "# \n",
    "#  优点：\n",
    "#   - 避免破坏已学到的语言知识。\n",
    "#   - 降低训练计算量和显存占用。\n",
    "#   - 提高小样本任务（如垃圾短信分类）训练稳定性。\n",
    "# \n",
    "#  典型应用：\n",
    "#   - 在预训练模型上仅添加一个线性分类器（linear head）；\n",
    "#   - 仅训练该分类器用于下游任务（如 spam detection）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72155f83-87d9-476a-a978-a15aa2d44147",
   "metadata": {},
   "source": [
    "- 接下来，我们替换输出层（`model.out_head`），该层原本将输入映射到 50,257 维（即词汇表大小）。  \n",
    "- 由于我们要对模型进行二分类微调（预测 2 个类别，“spam”和“not spam”），可以像下面这样替换输出层，该层默认是可训练的。  \n",
    "- 注意，我们使用 `BASE_CONFIG[\"emb_dim\"]`（在 `\"gpt2-small (124M)\"` 模型中等于 768）来使下面的代码更通用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e759fa0-0f69-41be-b576-17e5f20e04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 详解：替换 GPT 模型的输出层为二分类头（Spam / Ham）\n",
    "# ===========================================================\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1️ 设置随机种子，确保实验结果可复现\n",
    "# -----------------------------------------------------------\n",
    "# 在深度学习中，某些操作（如权重初始化、数据打乱等）具有随机性。\n",
    "# 通过固定随机种子，可以保证每次运行时模型初始化结果相同。\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2️ 定义分类任务的类别数量\n",
    "# -----------------------------------------------------------\n",
    "# 在垃圾短信分类任务中，我们只有两类：\n",
    "#   0 → \"ham\"（正常短信）\n",
    "#   1 → \"spam\"（垃圾短信）\n",
    "# 因此这里设置类别数为 2。\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3️ 替换 GPT 模型原有的输出层（out_head）\n",
    "# -----------------------------------------------------------\n",
    "# 原本 GPT 的 out_head 是一个线性层，用于将隐藏向量\n",
    "# 转换为词汇表中每个 token 的概率分布（输出维度 = vocab_size ≈ 50257）。\n",
    "# \n",
    "# 在这里我们把它改成一个新的线性层，用于分类任务。\n",
    "# 新的线性层输入特征数与 GPT 隐藏层维度一致，\n",
    "# 输出特征数为类别数（2个类别）。\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"],  # 输入维度（GPT 的隐藏层维度，如 768）\n",
    "    out_features=num_classes              # 输出维度（分类类别数：2）\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4️ 背景知识：为什么替换输出头？\n",
    "# -----------------------------------------------------------\n",
    "# GPT 原设计任务是语言建模（预测下一个词），\n",
    "# 所以原始输出层输出的是词汇表大小的概率分布。\n",
    "# 而现在我们要将 GPT 用作文本分类器，\n",
    "# 任务目标变为预测“这条短信是否为垃圾信息”，\n",
    "# 因此输出头需要输出“2个数”，代表两个类别的得分。\n",
    "# \n",
    "#  GPT 的主体（transformer blocks）继续作为“特征提取器”；\n",
    "#  新增的 out_head 层（线性层）则作为“分类头”；\n",
    "#  训练时只更新这个新层的参数，保持原 GPT 参数冻结。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be5475-ae77-4f97-8f3e-dec462b1339f",
   "metadata": {},
   "source": [
    "- 从技术上讲，仅训练输出层就足够了。  \n",
    "- 但是，正如我在 [Finetuning Large Language Models](https://magazine.sebastianraschka.com/p/finetuning-large-language-models) 中发现的，实验表明微调额外的层可以显著提升模型性能。  \n",
    "- 因此，我们还将最后一个 transformer block 以及连接最后一个 transformer block 与输出层的最终 `LayerNorm` 模块设置为可训练。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7c1eb-c46c-4065-8525-eea1b8c66d10",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/10.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aedc120-5ee3-48f6-92f2-ad9304ebcdc7",
   "metadata": {
    "id": "2aedc120-5ee3-48f6-92f2-ad9304ebcdc7"
   },
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# 详解：解冻 GPT 模型最后一层 Transformer Block 和最终归一化层\n",
    "# ===========================================================\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1️⃣ 冻结（Freeze）与解冻（Unfreeze）的概念\n",
    "# -----------------------------------------------------------\n",
    "# 在迁移学习中，我们通常不从头训练整个大模型，\n",
    "# 而是加载一个已经在大规模语料上预训练过的模型（如 GPT-2），\n",
    "# 然后只微调（fine-tune）其中的一小部分参数。\n",
    "#\n",
    "# 冻结参数的操作： param.requires_grad = False\n",
    "# 表示在反向传播时，这些参数不参与梯度计算，不会被更新。\n",
    "#\n",
    "# 解冻参数的操作： param.requires_grad = True\n",
    "# 表示这些参数在训练时会被更新。\n",
    "#\n",
    "# 这样做的好处是：\n",
    "# ✅ 节省计算资源；\n",
    "# ✅ 避免破坏预训练模型学到的通用语言特征；\n",
    "# ✅ 只调整模型的最后几层，使模型更好地适应当前任务（垃圾短信分类）。\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2️⃣ 解冻最后一个 Transformer Block\n",
    "# -----------------------------------------------------------\n",
    "# GPT 模型由多个 transformer block 堆叠而成（例如 GPT-2 small 有 12 层）。\n",
    "# 每个 block 都包含：\n",
    "#   - 自注意力层 (Self-Attention)\n",
    "#   - 前馈网络层 (Feed-Forward Network)\n",
    "#   - 残差连接与层归一化 (LayerNorm)\n",
    "#\n",
    "# 我们这里只解冻最后一层（即最靠近输出的那一层），\n",
    "# 因为它离任务输出最近，对分类任务的特征学习最有帮助。\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3️⃣ 解冻最终的归一化层（final_norm）\n",
    "# -----------------------------------------------------------\n",
    "# 在 GPT 模型的输出层之前，还有一个最终的 LayerNorm（final_norm）。\n",
    "# 该层的作用是：\n",
    "#   - 归一化最后一层 transformer 输出；\n",
    "#   - 保持数值稳定；\n",
    "#   - 使特征分布更加适合后续分类层。\n",
    "#\n",
    "# 我们解冻它，是为了允许模型对输出特征进行轻微调整，\n",
    "# 从而更好地适应新的任务（垃圾短信分类）。\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4️⃣ 总结：当前参数训练策略\n",
    "# -----------------------------------------------------------\n",
    "# ✅ 大部分 GPT 参数（前面层）被冻结，不会更新；\n",
    "# ✅ 仅以下部分会在训练中被更新：\n",
    "#    1. 新添加的分类头（out_head）\n",
    "#    2. 最后一层 Transformer Block（model.trf_blocks[-1]）\n",
    "#    3. 最终 LayerNorm（model.final_norm）\n",
    "#\n",
    "# 这种方式称为 **部分微调（partial fine-tuning）**，\n",
    "# 是实践中常用的技巧，用于高效适配大型语言模型到下游任务。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012b899-8284-4d3a-97c0-8a48eb33ba2e",
   "metadata": {},
   "source": [
    "- 我们仍然可以像前几章一样使用该模型  \n",
    "- 例如，我们给它输入一些文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f645c06a-7df6-451c-ad3f-eafb18224ebc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f645c06a-7df6-451c-ad3f-eafb18224ebc",
    "outputId": "27e041b1-d731-48a1-cf60-f22d4565304e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# 详解：将输入文本转换为 GPT 模型可处理的张量格式\n",
    "# ===========================================================\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1️⃣ 将字符串转换为 token ID 序列\n",
    "# -----------------------------------------------------------\n",
    "# tokenizer.encode() 会将自然语言字符串转换为 token ID 序列。\n",
    "# 在 GPT-2 中：\n",
    "#   - 每个单词、标点、甚至部分单词（子词）都有唯一的整数 ID；\n",
    "#   - 这些 ID 对应到模型词嵌入矩阵（embedding layer）中的向量。\n",
    "#\n",
    "# 例如：\n",
    "#   输入文本：\"Do you have time\"\n",
    "#   输出可能是：[  437,  345,  617,  1073 ]\n",
    "# （实际数字可能略有不同，取决于 tokenizer 的词表。）\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2️⃣ 将 token ID 列表转换为 PyTorch 张量\n",
    "# -----------------------------------------------------------\n",
    "# PyTorch 模型期望输入是一个张量（tensor），而不是 Python 列表。\n",
    "# 因此我们使用 torch.tensor() 将其转换。\n",
    "# 注意：此时得到的是一维张量，形状为 (num_tokens,)\n",
    "# 即： [437, 345, 617, 1073]\n",
    "inputs = torch.tensor(inputs)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3️⃣ 添加 batch 维度\n",
    "# -----------------------------------------------------------\n",
    "# GPT 模型输入的标准形状是：\n",
    "#    (batch_size, num_tokens)\n",
    "# 其中：\n",
    "#   - batch_size 表示一次输入的样本数量；\n",
    "#   - num_tokens 表示每个样本的 token 数。\n",
    "#\n",
    "# 由于这里只输入一个句子（单样本），\n",
    "# 我们使用 unsqueeze(0) 在最前面添加一个维度。\n",
    "# 结果形状变为 (1, num_tokens)\n",
    "# 示例： torch.Size([1, 4])\n",
    "inputs = inputs.unsqueeze(0)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4️⃣ 打印结果以验证\n",
    "# -----------------------------------------------------------\n",
    "print(\"Inputs:\", inputs)                # 打印 token ID 序列张量\n",
    "print(\"Inputs dimensions:\", inputs.shape)  # 打印形状，例如 (1, 4)\n",
    "# (1, num_tokens) 表示：\n",
    "#   batch_size = 1\n",
    "#   num_tokens = 输入文本的 token 数量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf8481-772d-467b-851c-a62b86d0cb1b",
   "metadata": {},
   "source": [
    "- 与前几章不同的是，现在输出层的维度为 2，而不是 50,257。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48dc84f1-85cc-4609-9cee-94ff539f00f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48dc84f1-85cc-4609-9cee-94ff539f00f4",
    "outputId": "9cae7448-253d-4776-973e-0af190b06354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 使用上下文管理器 torch.no_grad() 禁用梯度计算\n",
    "# ============================================\n",
    "# 说明：\n",
    "# - 在 PyTorch 中，模型的前向传播（forward）通常会自动记录计算图，\n",
    "#   以便在反向传播时计算梯度（即训练阶段需要的梯度更新信息）。\n",
    "# - 但在这里，我们只想执行“推理”（inference），\n",
    "#   即让模型对输入数据进行预测，而不进行参数更新。\n",
    "# - 使用 `with torch.no_grad():` 可以临时关闭 autograd（自动求导）功能，\n",
    "#   这样能显著减少内存占用并加快计算速度。\n",
    "# - 常用于模型评估阶段（evaluation phase）。\n",
    "\n",
    "with torch.no_grad():  \n",
    "    # 调用模型进行前向传播（forward pass）\n",
    "    # ---------------------------------------------\n",
    "    # 参数说明：\n",
    "    #   - model：这是我们定义好的 GPT 模型对象（GPTModel 类的实例）\n",
    "    #   - inputs：输入张量，形状为 (batch_size, num_tokens)\n",
    "    #              例如 (1, 4)，表示 1 条文本，共 4 个 token\n",
    "    # 功能说明：\n",
    "    #   - 模型会将输入 token id 序列通过嵌入层、Transformer 块、归一化层\n",
    "    #     和输出层（out_head，全连接层）进行前向计算。\n",
    "    #   - 由于我们之前把 `out_head` 改为 `nn.Linear(..., num_classes=2)`，\n",
    "    #     因此模型的输出是每个 token 对应的“二分类结果”（spam/ham 概率分数）。\n",
    "    # 返回值：\n",
    "    #   - outputs：张量，维度为 (batch_size, num_tokens, num_classes)\n",
    "    #               例如 (1, 4, 2)\n",
    "    #               表示每个 token 都有一个长度为 2 的输出向量，\n",
    "    #               分别对应“非垃圾邮件 (ham)”和“垃圾邮件 (spam)”两个类别的得分。\n",
    "    outputs = model(inputs)\n",
    "\n",
    "# ============================================\n",
    "# 打印模型的原始输出张量\n",
    "# ============================================\n",
    "# - 这一步是为了查看模型输出的实际内容（即 logits 数值）\n",
    "# - logits 通常是未经 softmax 的原始分数（实数），\n",
    "#   它们稍后会通过 softmax 转化为概率分布。\n",
    "# - 因为我们此时输出的是二分类任务的结果，\n",
    "#   所以每个 token 会对应两个值（代表“ham”和“spam”的原始得分）。\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "\n",
    "# ============================================\n",
    "# 打印输出张量的维度信息\n",
    "# ============================================\n",
    "# - 我们通过 `.shape` 属性查看 outputs 的张量形状\n",
    "# - 对于文本分类任务：\n",
    "#   (batch_size, num_tokens, num_classes)\n",
    "#   含义如下：\n",
    "#   → batch_size：一次输入的样本数（这里为 1）\n",
    "#   → num_tokens：输入序列中 token 的个数（例如 \"Do you have time\" 有 4 个 token）\n",
    "#   → num_classes：输出类别数量（这里是 2，对应 ham/spam）\n",
    "# - 这有助于验证模型的输出维度是否与任务匹配。\n",
    "\n",
    "print(\"Outputs dimensions:\", outputs.shape)  # shape: (batch_size, num_tokens, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75430a01-ef9c-426a-aca0-664689c4f461",
   "metadata": {},
   "source": [
    "- 如前几章所讨论，对于每个输入 token，模型都会输出一个向量。  \n",
    "- 由于我们输入了一个包含 4 个 token 的文本样本，上图中输出由 4 个 2 维输出向量组成。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df9144f-6817-4be4-8d4b-5d4dadfe4a9b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/11.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb8616-c791-4f5c-bac0-5302f663e46a",
   "metadata": {},
   "source": [
    "- 在第 3 章中，我们讨论了注意力机制（attention mechanism），它将每个输入 token 与其他所有输入 token 连接起来。  \n",
    "- 第 3 章中还介绍了 GPT 类模型中使用的因果注意力掩码（causal attention mask）；该因果掩码使当前 token 只能关注当前位置及之前的位置。  \n",
    "- 基于这种因果注意力机制，第 4 个（最后一个）token 包含了所有 token 中最多的信息，因为它是唯一一个包含了其他所有 token 信息的 token。  \n",
    "- 因此，我们特别关注这个最后的 token，并将其用于垃圾短信分类任务的微调。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49383a8c-41d5-4dab-98f1-238bca0c2ed7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49383a8c-41d5-4dab-98f1-238bca0c2ed7",
    "outputId": "e79eb155-fa1f-46ed-ff8c-d828c3a3fabd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 打印最后一个输出 token 对应的预测结果\n",
    "# ============================================\n",
    "# - 目的：从模型的输出张量中，取出每个序列的最后一个 token 的预测结果。\n",
    "# - 原理：Transformer（如 GPT 模型）会为输入序列中的每个 token\n",
    "#         生成一个对应的输出向量（即 logits）。\n",
    "#         例如，对于输入 \"Do you have time\"，共有 4 个 token，\n",
    "#         模型输出形状为 (1, 4, 2)：\n",
    "#           → batch_size = 1\n",
    "#           → num_tokens = 4\n",
    "#           → num_classes = 2\n",
    "#         输出示意：\n",
    "#           outputs = [\n",
    "#             [ [logit_1_1, logit_1_2],   # 第1个token (\"Do\")\n",
    "#               [logit_2_1, logit_2_2],   # 第2个token (\"you\")\n",
    "#               [logit_3_1, logit_3_2],   # 第3个token (\"have\")\n",
    "#               [logit_4_1, logit_4_2] ]  # 第4个token (\"time\") ← 我们取的就是这个\n",
    "#           ]\n",
    "# - 因此 outputs[:, -1, :] 表示：\n",
    "#   1️⃣ “ : ” 表示取所有批次样本（这里 batch_size=1）\n",
    "#   2️⃣ “ -1 ” 表示取最后一个 token（Python 索引规则）\n",
    "#   3️⃣ “ : ” 表示取该 token 的所有类别输出（这里是2个类别：ham/spam）\n",
    "\n",
    "print(\"Last output token:\", outputs[:, -1, :])\n",
    "# 输出结果形状为 (batch_size, num_classes)\n",
    "# 例如：tensor([[ 0.7213, -0.4527]])\n",
    "# 含义：\n",
    "# - 第一个值：该样本被预测为“ham”（非垃圾）的原始分数（logit）\n",
    "# - 第二个值：该样本被预测为“spam”（垃圾）的原始分数（logit）\n",
    "# - 这两个数通常会送入 softmax 函数，转化为概率形式，便于解释分类结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df08ae0-e664-4670-b7c5-8a2280d9b41b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/12.webp\" width=200px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa4aef-e1e9-491b-9adf-5aa973e59b8c",
   "metadata": {},
   "source": [
    "## 6.6 计算分类损失和准确率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e1fd1-ace8-44b4-b438-185ed0ba8b33",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/13.webp\" width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7df4ee-0a34-4a4d-896d-affbbf81e0b3",
   "metadata": {},
   "source": [
    "- 在解释损失计算之前，我们先简要看看模型输出是如何转换为类别标签的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557996dd-4c6b-49c4-ab83-f60ef7e1d69e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/14.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c77faab1-3461-4118-866a-6171f2b89aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 打印模型输出中 “最后一个 token” 的预测结果\n",
    "# ==========================================================\n",
    "\n",
    "# 语句解析：\n",
    "# outputs 是模型的前向传播输出张量（tensor），\n",
    "# 它的典型形状是：\n",
    "#   (batch_size, sequence_length, num_classes)\n",
    "# 例如：\n",
    "#   batch_size = 1         # 一次输入一句话\n",
    "#   sequence_length = 4     # 句子中共有4个token\n",
    "#   num_classes = 2         # 垃圾邮件分类任务中有两个类别：ham(0), spam(1)\n",
    "#\n",
    "# 输出示意：\n",
    "# tensor([\n",
    "#   [\n",
    "#     [logit_Do_ham, logit_Do_spam],         # 第1个token \"Do\"\n",
    "#     [logit_you_ham, logit_you_spam],       # 第2个token \"you\"\n",
    "#     [logit_have_ham, logit_have_spam],     # 第3个token \"have\"\n",
    "#     [logit_time_ham, logit_time_spam]      # 第4个token \"time\" ← 我们只取这个\n",
    "#   ]\n",
    "# ])\n",
    "\n",
    "# ==========================================================\n",
    "# outputs[:, -1, :]\n",
    "# ==========================================================\n",
    "# 含义详解：\n",
    "#   “: ” → 取所有批次样本（因为 batch_size 可能大于1）\n",
    "#   “-1” → 取序列中的最后一个 token（Python 中 -1 表示最后一个索引）\n",
    "#   “:”  → 取该 token 的所有类别输出（这里是2个类别的logit值）\n",
    "#\n",
    "# 因此：\n",
    "# outputs[:, -1, :] 的结果形状为：\n",
    "#   (batch_size, num_classes)\n",
    "# 例如：tensor([[ 0.7213, -0.4527]])\n",
    "#\n",
    "# 这两个数（logit 值）表示：\n",
    "#   第1个数 → 预测为“ham”的分数（未归一化概率）\n",
    "#   第2个数 → 预测为“spam”的分数（未归一化概率）\n",
    "#\n",
    "# GPT 类模型在文本分类任务中通常只取最后一个 token 的输出，\n",
    "# 因为该位置聚合了前文全部信息（Transformer 的自注意力机制保证了这一点），\n",
    "# 最后一个 token 对整个句子/消息的语义理解最完整。\n",
    "\n",
    "print(\"Last output token:\", outputs[:, -1, :])\n",
    "\n",
    "# 打印出的结果为一个二维张量，例如：\n",
    "# Last output token: tensor([[ 0.7213, -0.4527]])\n",
    "#\n",
    "# 后续通常会执行 softmax 运算，将该张量转为概率分布，\n",
    "# 以便确定哪一类（ham/spam）的概率更高。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd71fa-628a-4d00-b81d-6d8bcb2c341d",
   "metadata": {},
   "source": [
    "- 与第 5 章类似，我们通过 `softmax` 函数将输出（logits）转换为概率分数，然后使用 `argmax` 函数获取最大概率值对应的索引位置，从而得到类别标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b81efa92-9be1-4b9e-8790-ce1fc7b17f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 将模型输出的 logits 转换为概率，并获得最终分类标签\n",
    "# ==========================================================\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1️⃣ 计算类别概率分布\n",
    "# ----------------------------------------------------------\n",
    "# torch.softmax(input, dim)\n",
    "# 参数说明：\n",
    "#   - input：张量（这里是模型的输出 logits，即 outputs[:, -1, :]）\n",
    "#   - dim：在哪个维度上进行 softmax 操作\n",
    "#          这里 dim=-1 表示沿着“类别维度”计算概率\n",
    "#\n",
    "# softmax 函数定义：\n",
    "#   softmax(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "#\n",
    "# 它将任意实数向量（logits）转换为一个“概率分布”，\n",
    "# 所有类别的概率加起来等于 1。\n",
    "#\n",
    "# 举例：\n",
    "#   输入 logits = [2.0, 1.0]\n",
    "#   softmax → [0.731, 0.269]\n",
    "#   表示第1类概率更高。\n",
    "#\n",
    "# 输出张量形状：\n",
    "#   probas.shape = (batch_size, num_classes)\n",
    "#   例如 tensor([[0.73, 0.27]])\n",
    "\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2️⃣ 选取概率最大的类别索引\n",
    "# ----------------------------------------------------------\n",
    "# torch.argmax(input)\n",
    "# 参数说明：\n",
    "#   - input：输入张量（这里是 probas）\n",
    "#   - 返回：最大值所在的索引（整数）\n",
    "#\n",
    "# 对于二分类任务：\n",
    "#   若 argmax=0 → 预测为 “ham”（正常信息）\n",
    "#   若 argmax=1 → 预测为 “spam”（垃圾信息）\n",
    "\n",
    "label = torch.argmax(probas)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3️⃣ 打印分类结果\n",
    "# ----------------------------------------------------------\n",
    "# .item()：将单元素张量转换为 Python 标量（int/float），\n",
    "#          方便打印或后续逻辑判断。\n",
    "\n",
    "print(\"Class label:\", label.item())\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ✅ 输出示例：\n",
    "# ----------------------------------------------------------\n",
    "# Class label: 1\n",
    "#\n",
    "# 若结果为 0 → ham（正常短信）\n",
    "# 若结果为 1 → spam（垃圾短信）\n",
    "#\n",
    "# 注意：我们这里只取了一个样本（batch_size=1），\n",
    "# 若是批量预测，需对 label 向量遍历或批量输出。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a6f02-307e-4147-a416-14d115bf8179",
   "metadata": {},
   "source": [
    "- 注意，正如第 5 章所解释的，`softmax` 函数在这里是可选的，因为最大的输出值对应的就是最大的概率分数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9f9ad66-4969-4501-8239-3ccdb37e71a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 使用模型输出 logits 直接获得预测类别\n",
    "# ==========================================================\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1️⃣ 取序列中最后一个 token 的输出 logits\n",
    "# ----------------------------------------------------------\n",
    "# outputs.shape = (batch_size, num_tokens, num_classes)\n",
    "# - batch_size: 当前批次样本数量\n",
    "# - num_tokens: 输入序列长度\n",
    "# - num_classes: 分类类别数量（这里为 2：ham 或 spam）\n",
    "#\n",
    "# outputs[:, -1, :]：\n",
    "# - \":\" 表示选取所有样本\n",
    "# - \"-1\" 表示选取序列的最后一个 token\n",
    "# - \":\" 表示选取最后一层输出的所有类别 logits\n",
    "# \n",
    "# logits.shape = (batch_size, num_classes)\n",
    "# 每个样本对应一个向量，表示该 token 对每个类别的未归一化得分（logits）\n",
    "\n",
    "logits = outputs[:, -1, :]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2️⃣ 选择最大 logits 对应的类别索引\n",
    "# ----------------------------------------------------------\n",
    "# torch.argmax(input)\n",
    "# - input: 输入张量（这里是 logits）\n",
    "# - 返回：沿指定维度最大值的索引\n",
    "#   因为 logits 只有类别维度，所以默认对最后一维取最大\n",
    "# \n",
    "# 返回值是 tensor 类型，形状为 (batch_size,) 或单个标量\n",
    "\n",
    "label = torch.argmax(logits)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3️⃣ 打印分类标签\n",
    "# ----------------------------------------------------------\n",
    "# .item()：将单元素张量转为 Python 整数或浮点数\n",
    "# 在单样本情况下，可以直接打印\n",
    "# 对应标签：\n",
    "#   0 → ham（正常短信）\n",
    "#   1 → spam（垃圾短信）\n",
    "\n",
    "print(\"Class label:\", label.item())\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ✅ 总结：\n",
    "# 1. 直接使用最后一个 token 的 logits（未归一化得分）进行预测\n",
    "# 2. torch.argmax 选取最大值索引作为分类结果\n",
    "# 3. 输出 label.item() 即最终类别\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb20d3a-cbba-4ab1-8584-d94e16589505",
   "metadata": {},
   "source": [
    "- 我们可以应用这一概念来计算所谓的**分类准确率（classification accuracy）**，它计算给定数据集中正确预测的百分比。  \n",
    "- 要计算分类准确率，我们可以将前面基于 `argmax` 的预测代码应用于数据集中的所有示例，并按如下方式计算正确预测的比例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ecf9572-aed0-4a21-9c3b-7f9f2aec5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 计算数据加载器上模型的分类准确率\n",
    "# ==========================================================\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    计算模型在给定数据加载器上的分类准确率\n",
    "\n",
    "    参数：\n",
    "    - data_loader: torch.utils.data.DataLoader\n",
    "        经过封装的输入数据加载器，返回 (input_batch, target_batch)\n",
    "    - model: nn.Module\n",
    "        待评估的 PyTorch 模型\n",
    "    - device: torch.device\n",
    "        设备信息，例如 'cpu' 或 'cuda'\n",
    "    - num_batches: int 或 None\n",
    "        限制计算的批次数量。None 表示使用全部批次。\n",
    "\n",
    "    返回：\n",
    "    - accuracy: float\n",
    "        模型在给定数据加载器上的分类准确率\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 1️⃣ 设置模型为评估模式\n",
    "    # ------------------------------------------------------\n",
    "    # model.eval() 会关闭 Dropout 和 BatchNorm 等训练专用层\n",
    "    # 在评估或推理阶段必须设置\n",
    "    model.eval()\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 2️⃣ 初始化统计量\n",
    "    # ------------------------------------------------------\n",
    "    correct_predictions = 0  # 正确预测样本数\n",
    "    num_examples = 0          # 总样本数\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 3️⃣ 确定使用的批次数\n",
    "    # ------------------------------------------------------\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)  # 使用全部批次\n",
    "    else:\n",
    "        # 如果指定了 num_batches，则取最小值避免越界\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 4️⃣ 遍历数据加载器批次\n",
    "    # ------------------------------------------------------\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            # -----------------------------\n",
    "            # 4.1 将数据移动到指定设备\n",
    "            # -----------------------------\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            \n",
    "            # -----------------------------\n",
    "            # 4.2 前向推理，关闭梯度计算\n",
    "            # -----------------------------\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]  \n",
    "                # outputs.shape = (batch_size, seq_len, num_classes)\n",
    "                # 选择最后一个 token 的输出 logits\n",
    "                # logits.shape = (batch_size, num_classes)\n",
    "            \n",
    "            # -----------------------------\n",
    "            # 4.3 预测类别\n",
    "            # -----------------------------\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            # 对 logits 在最后一维取最大值的索引，得到预测类别\n",
    "            # predicted_labels.shape = (batch_size,)\n",
    "            \n",
    "            # -----------------------------\n",
    "            # 4.4 累加样本总数和正确预测数\n",
    "            # -----------------------------\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break  # 超过指定批次数则停止\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # 5️⃣ 计算并返回准确率\n",
    "    # ------------------------------------------------------\n",
    "    accuracy = correct_predictions / num_examples\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165fe46-a284-410b-957f-7524877d1a1a",
   "metadata": {},
   "source": [
    "- 让我们应用该函数来计算不同数据集的分类准确率：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "390e5255-8427-488c-adef-e1c10ab4fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 设备设置与模型评估示例\n",
    "# ==========================================================\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1️⃣ 选择设备（CPU 或 GPU）\n",
    "# ----------------------------------------------------------\n",
    "# 如果有 GPU 可用，则使用 GPU，否则使用 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 注意：\n",
    "# 以下是可选设置，用于在 Apple Silicon（M1/M2/M3）上使用 MPS 加速\n",
    "# 在 PyTorch 2.4 下，CPU 与 MPS 的结果基本一致\n",
    "# 但在早期版本可能略有差异\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")     # 优先使用 CUDA GPU\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")      # Apple Silicon GPU 加速\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")      # 默认 CPU\n",
    "# print(f\"Running on {device} device.\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2️⃣ 将模型移动到指定设备\n",
    "# ----------------------------------------------------------\n",
    "# 对于 nn.Module 对象，直接 model.to(device) 即可，无需重新赋值\n",
    "model.to(device)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3️⃣ 设置随机种子以保证可复现性\n",
    "# ----------------------------------------------------------\n",
    "# PyTorch 的 DataLoader 在 shuffle=True 时会打乱数据顺序\n",
    "# 设置随机种子可以保证每次实验的数据顺序一致\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4️⃣ 计算模型在不同数据集上的准确率\n",
    "# ----------------------------------------------------------\n",
    "# 这里仅取前 10 个批次进行快速评估\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    data_loader=train_loader,  # 训练集数据加载器\n",
    "    model=model,               # 待评估模型\n",
    "    device=device,             # 运行设备\n",
    "    num_batches=10             # 限制计算前 10 个批次\n",
    ")\n",
    "\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    data_loader=val_loader, \n",
    "    model=model, \n",
    "    device=device, \n",
    "    num_batches=10\n",
    ")\n",
    "\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    data_loader=test_loader, \n",
    "    model=model, \n",
    "    device=device, \n",
    "    num_batches=10\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5️⃣ 输出结果\n",
    "# ----------------------------------------------------------\n",
    "# 将准确率乘以 100 转换为百分比，并保留两位小数\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30345e2a-afed-4d22-9486-f4010f90a871",
   "metadata": {},
   "source": [
    "- 正如我们所见，预测准确率并不高，因为我们尚未对模型进行微调\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a9d15-8fc7-48a2-8734-d92a2f265328",
   "metadata": {},
   "source": [
    "- 在开始微调（finetuning）/训练之前，我们首先需要定义训练过程中要优化的损失函数。  \n",
    "- 我们的目标是最大化模型的垃圾短信分类准确率；然而，分类准确率并不是一个可微函数。  \n",
    "- 因此，我们改为最小化交叉熵损失（cross-entropy loss），作为最大化分类准确率的代理（你可以在我免费提供的 [Introduction to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression) 第 8 讲中了解更多相关内容）。\n",
    "\n",
    "- `calc_loss_batch` 函数与第 5 章相同，只是我们这里只关注优化最后一个 token `model(input_batch)[:, -1, :]`，而不是所有 token `model(input_batch)`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f1e9547-806c-41a9-8aba-3b2822baabe4",
   "metadata": {
    "id": "2f1e9547-806c-41a9-8aba-3b2822baabe4"
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 计算单个批次的交叉熵损失函数\n",
    "# ==========================================================\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "    计算一个批次（batch）的模型损失（交叉熵）。\n",
    "\n",
    "    参数：\n",
    "    - input_batch: torch.Tensor, 输入序列张量，形状为 (batch_size, seq_len)\n",
    "    - target_batch: torch.Tensor, 对应的目标标签，形状为 (batch_size,)\n",
    "    - model: nn.Module, 待训练或评估的模型\n",
    "    - device: torch.device, 计算设备（CPU 或 GPU）\n",
    "\n",
    "    返回：\n",
    "    - loss: torch.Tensor, 当前批次的平均交叉熵损失\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1️⃣ 将输入与目标移动到指定设备\n",
    "    # -----------------------------\n",
    "    # 保证输入数据与模型权重在同一设备上进行计算\n",
    "    input_batch = input_batch.to(device)      # 将输入序列移动到 device\n",
    "    target_batch = target_batch.to(device)    # 将目标标签移动到 device\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2️⃣ 模型前向计算\n",
    "    # -----------------------------\n",
    "    # 输出 logits 的形状为 (batch_size, seq_len, num_classes)\n",
    "    logits = model(input_batch)\n",
    "\n",
    "    # 只取序列中最后一个 token 的 logits 作为分类输出\n",
    "    # 形状变为 (batch_size, num_classes)\n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3️⃣ 计算交叉熵损失\n",
    "    # -----------------------------\n",
    "    # torch.nn.functional.cross_entropy() 接受：\n",
    "    # - input: 模型输出 logits，形状 (batch_size, num_classes)\n",
    "    # - target: 真实标签，形状 (batch_size,)\n",
    "    # 返回值为当前批次的平均损失（标量）\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4️⃣ 返回损失\n",
    "    # -----------------------------\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a013aab9-f854-4866-ad55-5b8350adb50a",
   "metadata": {},
   "source": [
    "- `calc_loss_loader` 函数与第 5 章完全相同。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7b83e10-5720-45e7-ac5e-369417ca846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 计算整个数据加载器 (DataLoader) 上的平均损失\n",
    "# ==========================================================\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    计算整个数据集或部分数据集（由 DataLoader 提供）上的平均损失。\n",
    "\n",
    "    参数：\n",
    "    - data_loader: torch.utils.data.DataLoader, 提供输入和目标批次\n",
    "    - model: nn.Module, 待训练或评估的模型\n",
    "    - device: torch.device, 计算设备（CPU 或 GPU）\n",
    "    - num_batches: int 或 None, 计算损失的批次数。None 表示使用整个数据集\n",
    "\n",
    "    返回：\n",
    "    - avg_loss: float, 数据集或指定批次的平均损失\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1️⃣ 初始化总损失\n",
    "    # -----------------------------\n",
    "    total_loss = 0.0  # 累积每个批次的损失\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2️⃣ 特殊情况处理\n",
    "    # -----------------------------\n",
    "    if len(data_loader) == 0:\n",
    "        # 如果数据加载器为空，返回 NaN\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        # 如果未指定批次数，则使用 DataLoader 的全部批次\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 如果指定了批次数，但超过了 DataLoader 的总批次，则取最小值\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3️⃣ 遍历每个批次计算损失\n",
    "    # -----------------------------\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            # 计算当前批次的损失\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            # 累加标量损失\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            # 超过指定批次数，提前结束循环\n",
    "            break\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4️⃣ 返回平均损失\n",
    "    # -----------------------------\n",
    "    avg_loss = total_loss / num_batches  # 计算每个批次的平均损失\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56826ecd-6e74-40e6-b772-d3541e585067",
   "metadata": {},
   "source": [
    "- 使用 `calc_loss_loader`，我们可以在开始训练之前计算初始的训练集、验证集和测试集损失。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6f00e53-5beb-4e64-b147-f26fd481c6ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6f00e53-5beb-4e64-b147-f26fd481c6ff",
    "outputId": "49df8648-9e38-4314-854d-9faacd1b2e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 在不计算梯度的情况下计算训练/验证/测试集的平均损失\n",
    "# ==========================================================\n",
    "with torch.no_grad():  # 关闭梯度计算，提高推理效率，因为当前不是训练阶段\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1️⃣ 计算训练集损失\n",
    "    # -----------------------------\n",
    "    train_loss = calc_loss_loader(\n",
    "        data_loader=train_loader,  # 提供训练数据的 DataLoader\n",
    "        model=model,               # 参与计算的模型\n",
    "        device=device,             # 计算设备（CPU 或 GPU）\n",
    "        num_batches=5              # 仅计算前 5 个批次，快速估计\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2️⃣ 计算验证集损失\n",
    "    # -----------------------------\n",
    "    val_loss = calc_loss_loader(\n",
    "        data_loader=val_loader,    # 提供验证数据的 DataLoader\n",
    "        model=model,\n",
    "        device=device,\n",
    "        num_batches=5              # 仅计算前 5 个批次\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3️⃣ 计算测试集损失\n",
    "    # -----------------------------\n",
    "    test_loss = calc_loss_loader(\n",
    "        data_loader=test_loader,   # 提供测试数据的 DataLoader\n",
    "        model=model,\n",
    "        device=device,\n",
    "        num_batches=5              # 仅计算前 5 个批次\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ 打印损失\n",
    "# -----------------------------\n",
    "print(f\"Training loss: {train_loss:.3f}\")  # 显示训练集平均损失，保留 3 位小数\n",
    "print(f\"Validation loss: {val_loss:.3f}\") # 显示验证集平均损失\n",
    "print(f\"Test loss: {test_loss:.3f}\")      # 显示测试集平均损失\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b980b-e583-4f62-84a0-4edafaf99d5d",
   "metadata": {},
   "source": [
    "- 在下一节中，我们将训练模型，以提高损失值表现，从而提升分类准确率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ae0fd-6261-42b4-ab6a-d24289953083",
   "metadata": {
    "id": "456ae0fd-6261-42b4-ab6a-d24289953083"
   },
   "source": [
    "## 6.7 在有监督数据上微调模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b099b-0829-4f72-8a2b-4363e3497026",
   "metadata": {},
   "source": [
    "- 在本节中，我们定义并使用训练函数来提高模型的分类准确率。  \n",
    "- 下面的 `train_classifier_simple` 函数实际上与第 5 章中用于预训练模型的 `train_model_simple` 函数几乎相同。  \n",
    "- 唯一的两个区别是：  \n",
    "  1. 我们现在跟踪的是已见训练样本数量（`examples_seen`），而不是已见 token 数量。  \n",
    "  2. 我们在每个 epoch 后计算准确率，而不是在每个 epoch 后打印示例文本。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b6222-1dc2-4530-9d01-b6b04fe3de12",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/15.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "Csbr60to50FL",
   "metadata": {
    "id": "Csbr60to50FL"
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 简化版分类器训练函数，类似第5章的 train_model_simple\n",
    "# ==========================================================\n",
    "def train_classifier_simple(\n",
    "        model,         # nn.Module 类，待训练的分类模型\n",
    "        train_loader,  # 训练数据的 DataLoader\n",
    "        val_loader,    # 验证数据的 DataLoader\n",
    "        optimizer,     # 优化器，例如 torch.optim.AdamW\n",
    "        device,        # 计算设备（CPU 或 GPU）\n",
    "        num_epochs,    # 总训练轮数\n",
    "        eval_freq,     # 每隔多少步（batch）进行一次评估\n",
    "        eval_iter      # 评估时使用的 batch 数量\n",
    "    ):\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1️⃣ 初始化记录变量\n",
    "    # -----------------------------\n",
    "    train_losses, val_losses = [], []  # 保存每次评估的训练/验证损失\n",
    "    train_accs, val_accs = [], []      # 保存每次评估的训练/验证准确率\n",
    "    examples_seen, global_step = 0, -1 # 总样本数计数 & 全局训练步数\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2️⃣ 主训练循环：遍历每一轮 epoch\n",
    "    # -----------------------------\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 切换模型为训练模式，启用 dropout 和 batch norm 等\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2a️⃣ 遍历训练数据 loader 中的每一个批次\n",
    "        # -----------------------------\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()  # 清零上一批次梯度，防止梯度累加\n",
    "\n",
    "            # 计算当前批次的损失\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch,   # 当前批次输入张量 (batch_size, seq_len)\n",
    "                target_batch,  # 当前批次标签张量 (batch_size)\n",
    "                model,         # 模型\n",
    "                device         # 设备\n",
    "            )\n",
    "\n",
    "            loss.backward()  # 反向传播计算梯度\n",
    "\n",
    "            optimizer.step()  # 根据梯度更新模型权重\n",
    "\n",
    "            # -----------------------------\n",
    "            # 2b️⃣ 更新计数器\n",
    "            # -----------------------------\n",
    "            examples_seen += input_batch.shape[0]  # 累计训练样本数\n",
    "            global_step += 1                        # 全局训练步数 +1\n",
    "\n",
    "            # -----------------------------\n",
    "            # 2c️⃣ 可选评估：每 eval_freq 步进行一次损失评估\n",
    "            # -----------------------------\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model,       # 当前训练模型\n",
    "                    train_loader, # 训练数据 loader\n",
    "                    val_loader,   # 验证数据 loader\n",
    "                    device,       # 计算设备\n",
    "                    eval_iter     # 评估时使用的 batch 数量\n",
    "                )\n",
    "                train_losses.append(train_loss)  # 保存训练损失\n",
    "                val_losses.append(val_loss)      # 保存验证损失\n",
    "\n",
    "                # 输出当前步的训练/验证损失信息\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3️⃣ 每轮结束后计算训练集 & 验证集准确率\n",
    "        # -----------------------------\n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader,  # 训练数据 loader\n",
    "            model,         # 模型\n",
    "            device,        # 设备\n",
    "            num_batches=eval_iter # 只用前 eval_iter 个 batch 快速估计准确率\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader,    # 验证数据 loader\n",
    "            model,\n",
    "            device,\n",
    "            num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        # 打印训练集与验证集准确率\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "        # 保存本轮准确率\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4️⃣ 返回训练记录\n",
    "    # -----------------------------\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624cb30-3e3a-45be-b006-c00475b58ae8",
   "metadata": {},
   "source": [
    "- 在 `train_classifier_simple` 中使用的 `evaluate_model` 函数与第5章中使用的函数相同\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcc7bc04-6aa6-4516-a147-460e2f466eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 评估模型在训练集和验证集上的平均损失\n",
    "# ==========================================================\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    \"\"\"\n",
    "    在训练和验证数据上计算平均交叉熵损失，用于训练过程中监控模型性能。\n",
    "    \n",
    "    参数:\n",
    "        model (nn.Module): 待评估的模型\n",
    "        train_loader (DataLoader): 训练数据加载器\n",
    "        val_loader (DataLoader): 验证数据加载器\n",
    "        device (torch.device): 计算设备（CPU 或 GPU）\n",
    "        eval_iter (int): 用于快速评估的 batch 数量\n",
    "        \n",
    "    返回:\n",
    "        train_loss (float): 前 eval_iter 个训练 batch 的平均损失\n",
    "        val_loss (float): 前 eval_iter 个验证 batch 的平均损失\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 1️⃣ 切换模型为评估模式\n",
    "    # -----------------------------\n",
    "    # 评估模式下，模型禁用 Dropout、BatchNorm 等训练专用层行为\n",
    "    model.eval()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 2️⃣ 禁用梯度计算\n",
    "    # -----------------------------\n",
    "    # 使用 torch.no_grad() 可以节省显存和计算量，因为评估阶段不需要反向传播\n",
    "    with torch.no_grad():\n",
    "        # -----------------------------\n",
    "        # 2a️⃣ 计算训练集前 eval_iter 个 batch 的平均损失\n",
    "        # -----------------------------\n",
    "        train_loss = calc_loss_loader(\n",
    "            data_loader=train_loader,  # 训练数据\n",
    "            model=model,               # 当前模型\n",
    "            device=device,             # 计算设备\n",
    "            num_batches=eval_iter      # 仅取前 eval_iter 个 batch\n",
    "        )\n",
    "        \n",
    "        # -----------------------------\n",
    "        # 2b️⃣ 计算验证集前 eval_iter 个 batch 的平均损失\n",
    "        # -----------------------------\n",
    "        val_loss = calc_loss_loader(\n",
    "            data_loader=val_loader,    # 验证数据\n",
    "            model=model,               # 当前模型\n",
    "            device=device,             # 计算设备\n",
    "            num_batches=eval_iter      # 仅取前 eval_iter 个 batch\n",
    "        )\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 3️⃣ 恢复模型训练模式\n",
    "    # -----------------------------\n",
    "    # 在训练循环中继续训练时，需要恢复 Dropout 等训练行为\n",
    "    model.train()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 4️⃣ 返回训练集和验证集的平均损失\n",
    "    # -----------------------------\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807bfe9-364d-46b2-9e25-3b000c3ef6f9",
   "metadata": {},
   "source": [
    "- 在 M3 MacBook Air 笔记本电脑上训练大约需要 5 分钟，在 V100 或 A100 GPU 上训练不到半分钟\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X7kU3aAj7vTJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7kU3aAj7vTJ",
    "outputId": "504a033e-2bf8-41b5-a037-468309845513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 1.977, Val loss 2.196\n",
      "Ep 1 (Step 000050): Train loss 0.615, Val loss 0.636\n",
      "Ep 1 (Step 000100): Train loss 0.520, Val loss 0.555\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Ep 2 (Step 000150): Train loss 0.563, Val loss 0.488\n",
      "Ep 2 (Step 000200): Train loss 0.418, Val loss 0.395\n",
      "Ep 2 (Step 000250): Train loss 0.408, Val loss 0.352\n",
      "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
      "Ep 3 (Step 000300): Train loss 0.330, Val loss 0.314\n",
      "Ep 3 (Step 000350): Train loss 0.278, Val loss 0.181\n",
      "Training accuracy: 90.00% | Validation accuracy: 92.50%\n",
      "Ep 4 (Step 000400): Train loss 0.076, Val loss 0.131\n",
      "Ep 4 (Step 000450): Train loss 0.125, Val loss 0.099\n",
      "Ep 4 (Step 000500): Train loss 0.202, Val loss 0.102\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.206, Val loss 0.110\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 训练分类器并记录时间\n",
    "# ==========================================================\n",
    "\n",
    "import time  # 导入时间模块，用于计算训练耗时\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ 记录训练开始时间\n",
    "# -----------------------------\n",
    "start_time = time.time()  # 返回当前时间戳（秒）\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ 设置随机种子，保证训练可复现\n",
    "# -----------------------------\n",
    "torch.manual_seed(123)  # 对 CPU & GPU 的随机操作都生效\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ 初始化优化器\n",
    "# -----------------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),  # 待优化的模型参数\n",
    "    lr=5e-5,             # 学习率，控制每次梯度更新幅度\n",
    "    weight_decay=0.1     # 权重衰减（L2 正则化），防止过拟合\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ 设置训练轮数\n",
    "# -----------------------------\n",
    "num_epochs = 5  # 总训练轮数\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ 调用训练函数，开始训练\n",
    "# -----------------------------\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model,         # 待训练模型\n",
    "    train_loader,  # 训练数据 DataLoader\n",
    "    val_loader,    # 验证数据 DataLoader\n",
    "    optimizer,     # 优化器\n",
    "    device,        # 计算设备（CPU 或 GPU）\n",
    "    num_epochs=num_epochs,  # 总训练轮数\n",
    "    eval_freq=50,           # 每 50 个 batch 进行一次训练/验证损失评估\n",
    "    eval_iter=5,            # 每次评估只使用前 5 个 batch（快速估计）\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ 记录训练结束时间\n",
    "# -----------------------------\n",
    "end_time = time.time()  # 返回训练结束时的时间戳（秒）\n",
    "\n",
    "# -----------------------------\n",
    "# 7️⃣ 计算训练耗时（分钟）\n",
    "# -----------------------------\n",
    "execution_time_minutes = (end_time - start_time) / 60  # 秒转分钟\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261bf90-3ce7-4591-895a-044a05538f30",
   "metadata": {},
   "source": [
    "- 类似于第5章，我们使用 matplotlib 绘制训练集和验证集的损失函数曲线\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cURgnDqdCeka",
   "metadata": {
    "id": "cURgnDqdCeka"
   },
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 绘制训练与验证的损失或其他指标随训练过程变化的曲线\n",
    "# ==========================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    \"\"\"\n",
    "    绘制训练与验证曲线，并在同一图中显示 x 轴为 Epochs 和示例数。\n",
    "    \n",
    "    参数:\n",
    "        epochs_seen (list 或 Tensor): 每次评估时对应的训练轮数 (epoch)\n",
    "        examples_seen (list 或 Tensor): 每次评估时对应的累计训练样本数\n",
    "        train_values (list 或 Tensor): 训练指标值，例如 loss 或 accuracy\n",
    "        val_values (list 或 Tensor): 验证指标值，例如 loss 或 accuracy\n",
    "        label (str, optional): y 轴标签，默认为 \"loss\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 1️⃣ 创建画布和子图\n",
    "    # -----------------------------\n",
    "    # figsize=(5, 3) 指定图像宽 5 英寸，高 3 英寸\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 2️⃣ 绘制训练集和验证集曲线\n",
    "    # -----------------------------\n",
    "    # ax1.plot: 绘制训练指标随 epoch 的变化\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")  # 训练集曲线\n",
    "    # ax1.plot: 绘制验证指标随 epoch 的变化，linestyle=\"-.\" 为点划线\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")  # 验证集曲线\n",
    "    \n",
    "    # 设置 x 轴和 y 轴标签\n",
    "    ax1.set_xlabel(\"Epochs\")            # x 轴为训练轮数\n",
    "    ax1.set_ylabel(label.capitalize())  # y 轴为指标值，并首字母大写\n",
    "    \n",
    "    # 显示图例\n",
    "    ax1.legend()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 3️⃣ 创建第二个 x 轴用于显示累计样本数\n",
    "    # -----------------------------\n",
    "    # twiny() 创建共享 y 轴的第二个 x 轴\n",
    "    ax2 = ax1.twiny()\n",
    "    \n",
    "    # 为了让刻度对齐，绘制一条不可见曲线\n",
    "    # alpha=0 表示透明\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)\n",
    "    \n",
    "    # 设置第二个 x 轴的标签\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 4️⃣ 调整布局并保存显示\n",
    "    # -----------------------------\n",
    "    fig.tight_layout()                # 自动调整子图参数，使图像不重叠\n",
    "    plt.savefig(f\"{label}-plot.pdf\")  # 保存为 PDF 文件，文件名根据 label 动态生成\n",
    "    plt.show()                        # 在 Jupyter Notebook 中显示图像\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "OIqRt466DiGk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "OIqRt466DiGk",
    "outputId": "b16987cf-0001-4652-ddaf-02f7cffc34db"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAToxJREFUeJzt3Qd0VGXaB/B/Jr2ShPQQCJAQeu9FEJCiotgXXUHWsiK6KLquWEDkU+yggiC6ih0QBVwFFOm9SDGU0AkJkAYhpNf7needzGQmJCEhZWaS/++ce2bmzp2Zdy5hnvvWx07TNA1ERERklXSWLgARERGVj4GaiIjIijFQExERWTEGaiIiIivGQE1ERGTFGKiJiIisGAM1ERGRFWOgJiIismIM1ERERFaMgZqIKmXQoEF4+umnLV0MogaHgZqojjz00EOws7O7ahsxYoSli0ZEVszB0gUgakgkKH/xxRdm+5ydnS1WHiKyfqxRE9UhCcpBQUFmm4+Pj3puw4YNcHJywubNm43Hv/322wgICEBiYqJ6vHr1avTv3x/e3t5o3Lgxbr31Vpw8edJ4/JkzZ1QtfcmSJRgwYABcXV3Ro0cPHDt2DLt370b37t3h4eGBkSNHIjk52ay2P3r0aEyfPh3+/v7w8vLC448/jry8vHK/S25uLp577jmEhobC3d0dvXr1Ut/BIDY2FqNGjVLfT55v164dVq5cWe77ffzxx4iMjISLiwsCAwNx9913G58rKirCzJkz0bx5c/WdOnXqhKVLl5q9/uDBg+p7yfeT1z/44INISUkxa7r/17/+heeffx6+vr7q3L/66quV+ncjsiQGaiIr6wOWAJOWloZ9+/bhlVdewWeffaYCj8jMzMTkyZOxZ88erF27FjqdDnfccYcKZKamTZuGl19+GXv37oWDgwPuv/9+FaA++OADdSFw4sQJTJ061ew18n5HjhxRwfb777/HTz/9pAJ3eZ588kls374dixYtwl9//YV77rlHtRgcP35cPT9x4kQVzDdt2oTo6Gi89dZbKoiWRb6PBNHXXnsNR48eVRckN9xwg/F5CdJfffUV5s+fj0OHDuGZZ57B3//+d2zcuFE9f/nyZQwePBhdunRR7yWvl4ube++91+xzvvzyS3XRsHPnTnURJJ+3Zs2aKv9bEdUpSXNJRLVv3Lhxmr29vebu7m62vf7668ZjcnNztc6dO2v33nuv1rZtW+3RRx+t8D2Tk5MlTa0WHR2tHp8+fVo9/uyzz4zHfP/992rf2rVrjftmzpypRUVFmZXN19dXy8zMNO6bN2+e5uHhoRUWFqrHAwcO1CZNmqTux8bGqu9y7tw5s/IMGTJEmzJlirrfoUMH7dVXX63Uufnxxx81Ly8v7cqVK1c9l5OTo7m5uWnbtm0z2//www9rY8aMUfdnzJihDRs2zOz5uLg49b2PHj1qLH///v3NjunRo4f2n//8p1JlJLIU9lET1aEbb7wR8+bNM9snzbAG0vT97bffomPHjmjWrBlmzZpldqzUVqUmLDVCadY11KTPnj2L9u3bG4+T1xsYauMdOnQw25eUlGT23tKc7ObmZnzcp08fZGRkIC4uTpXFlNSQCwsL0apVK7P9UoOWJnkhNeQJEybg999/x9ChQ3HXXXeZlcvUTTfdpD6jRYsWqlYum7QUSHmk9p+VlaWOMSXN8lKDFgcOHMD69evLrLFL14ChnKU/Pzg4+KrzQGRtGKiJ6pA0u0ZERFR4zLZt29TtpUuX1CavMZA+Xwlon376KUJCQlSglgBdui/Z0dHReF/6rMvaV7q5vCokgNvb2+PPP/9Ut6YMwfKRRx7B8OHD8euvv6pgLc3X7733Hp566qmr3s/T01M100uzuxwrFyPSfyz96vJZQt5H+sPLGognx8i5keb10iQYl3VeauI8ENUFBmoiKyK1P+l/lUC8ePFijBs3Dn/88Yfqi7548aLqv5XnZKCY2LJlS419ttRKs7Oz1WAtsWPHDhV0w8LCrjpWarJSo5baqKEsZZHXyqA02aZMmaLKXlagFtKXLjVv2aSPXQbMrVu3TtWkJSBLq8HAgQPLfG3Xrl3x448/Ijw8XL0PUX3Cv2iiOiRNwwkJCWb7JLD4+fmpwCcDpKQWOn78eNX8K83VUgv997//rUZPS7PyggULVC1RAtcLL7xQY2WTWvnDDz+sBqHJ6HEJljJgTC4SSpOm5AceeABjx45V5ZPALaPIZUCaNC/fcsstamCcjMKWY1NTU1XTdJs2bcr87F9++QWnTp1SA8jke8rocKnpRkVFqdq2jC6XCxjZJ6PeZbDd1q1b1eh0uZiRgWtyETBmzBjjqG5pMpeBbjIYr3Stn8iWMFAT1SEZjWzaFCskGMXExOD1119XU5okaAk5ToKyBJ9hw4apPmQJPNL3K83d8roPP/xQjRavCUOGDFHToyRYygWFfG5F05dkPvj//d//4dlnn8W5c+fUxUbv3r3VlDEhFx4SQOPj41VAlQuP0n3uBlJ7llHm8nk5OTmqHDLyXKZ0iRkzZqhpY9J8LgFdjpda9Isvvqiel24ACdz/+c9/1LmS8ksXgXxmWRcaRLbETkaUWboQRGRZMo9apjgtX77c0kUholJ4qUlERGTFGKiJiIisGJu+iYiIrBhr1ERERFaMgZqIiMiKMVATERFZMQbqapg7d65aCUnS8kmKv127dqG+kgxIskSjzFeVZRdLT+ORoQ6y7KPM/ZWVrWR1KUMWJQNZDlMWyZA5tTIPVhbXMCwPaSBZmGSlKzmnsqqVZDiyBTK/V9JJyuIckpZSUkbKKmKmZH6wzCuWRUtkxS9Z+9qQvtJAFjGRxUJkjWt5H1nopKCgwOwYWWZT5hDLal2yHOnChQthC2SNc1kMRf79ZZO1xFetWmV8vqGfn7K8+eab6v+bLB5jwPMENd9ezovp1rp16/p7jiyWDsTGLVq0SHNyctI+//xz7dChQyrLkbe3t5aYmKjVRytXrtReeukl7aefflIZiZYtW2b2/Jtvvqk1atRIW758uXbgwAHttttu05o3b65lZ2cbjxkxYoTWqVMnbceOHdrmzZu1iIgIY/YjkZaWpgUGBmoPPPCAdvDgQZX1ydXVVfvkk080azd8+HDtiy++UOXev3+/dvPNN2tNmzbVMjIyjMc8/vjjWlhYmMpitWfPHq13795a3759jc8XFBRo7du314YOHart27dPnXM/Pz9jNipx6tQplUlq8uTJ2uHDh7WPPvpIZbFavXq1Zu1+/vln7ddff9WOHTumMlq9+OKLmqOjozpnoqGfn9J27dqlhYeHax07djRmLRM8T5o2bdo0rV27dtqFCxeMm2SSq6/niIH6OvXs2VObOHGi8bGkAgwJCVHpA+u70oG6qKhICwoK0t555x3jvsuXL2vOzs4q2Ar5Q5fX7d6923jMqlWrNDs7O2OqxI8//ljz8fFRqR4NJAWhaTpGW5GUlKS+78aNG43nQ4LSDz/8YDzmyJEj6pjt27erx/JjodPptISEBLNUk5L+0XBOnn/+efUDZeq+++5TFwq2SP69JSUnz4+59PR0LTIyUluzZo1ZelGep5JALRf9ZamP54hN39e5JrJkDZLmXQNZplAeb9++HQ3N6dOn1frVpuejUaNGqjvAcD7kVpq7u3fvbjxGjpfzJikbDcfI8pWS6tFA1r2WJmRZK9qWyFrUpiks5e8lPz/f7BxJU13Tpk3NzpGs7W1IS2n4/leuXMGhQ4eMx5i+h+EYW/u7k+VFZTnUzMxM1QTO82NOmm2lWbb0d+F5KiFda9IVJ6lRpUtNmrLr6zlioL4OkgdYfmhM/5GFPC6dcKEhMHznis6H3Eo/UOlkFBLITI8p6z1MP8MWSOII6VPs16+fMUe0lF8uQORipaJzdK3vX94x8gMjma+sneSxlj5D6fOTjFrLli1D27ZteX5MyAWMpPyUcQ+l8TzpSSVA+otl7XwZ+yCVBRnbkp6eXi/PEZNyENVCbejgwYM1moKyvpBEIvv371ctDkuXLlWZrzZu3GjpYlmNuLg4TJo0CWvWrFEDKqlskpXNQAYoSuCWJCxLliwxpmmtT1ijvg6SJUjS5pUeRSiPg4KC0NAYvnNF50NuJXexKRlhKSPBTY8p6z1MP8PaSVpIyX4lKR2bNGli3C/lly4TSXxR0Tm61vcv7xgZRW0LP1BS05HRs926dVM1RskI9sEHH/D8FJNmW/l/IiONpcVJNrmQkSxpcl9qdDxPV5Pas6RTldSm9fFviYH6On9s5IdGcu+aNnfKY+lva2iaN2+u/qhNz4c0D0nfs+F8yK38x5EfIoN169ap8yZXw4ZjZBqY9C8ZSM1CamGSo9iayRg7CdLSlCvfS86JKfl7cXR0NDtH0vcu/Wqm50iahk0vaOT7yw+DNA8bjjF9D8Mxtvp3J//+kpKS56ck1ah8R2l1MGwyrkP6YA33eZ6uJtM8T548qaaH1su/pTofvlaPpmfJqOaFCxeqEc2PPfaYmp5lOoqwPpFRqDKNQTb5s3n//ffV/djYWOP0LPn+K1as0P766y/t9ttvL3N6VpcuXbSdO3dqW7ZsUaNaTadnyWhNmZ714IMPqik7co5leoQtTM+aMGGCmp62YcMGsykjWVlZZlNGZMrWunXr1JSRPn36qK30lJFhw4apKV4yDcTf37/MKSP//ve/1UjWuXPn2sy0mhdeeEGNgj99+rT6G5HHMur/999/V8839PNTHtNR34LnSdOeffZZ9X9N/pa2bt2qplnJ9CqZbVEfzxEDdTXIvDr5Y5D51DJdS+YH11fr169XAbr0Nm7cOOMUrVdeeUUFWrmAGTJkiJora+rixYsqMHt4eKhpEOPHj1cXAKZkDnb//v3Ve4SGhqoLAFtQ1rmRTeZWG8hFyxNPPKGmJMkPwB133KGCuakzZ85oI0eOVPPH5YdHfpDy8/Ov+rfo3Lmz+rtr0aKF2WdYs3/84x9as2bNVLnlR1H+RgxBWjT081PZQM3zpKlpUsHBwars8jshj0+cOFFvzxGzZxEREVkx9lETERFZMQZqIiIiK8ZATUREZMUYqImIiKwYAzUREZEVY6AmIiKyYgzU1SArKkkCc7ml8vE8XRvP0bXxHF0bz1H9PEcWnUcta/3+9NNPiImJUWun9u3bF2+99ZZaMrI8kjFl/PjxZvskE09OTg7qmiyTKekcJcGALD1HZeN5ujaeo2vjObo2nqP6eY4sWqOWxeYl09COHTvUGqqyxvOwYcNUjtqKyMm9cOGCcYuNja2zMhMRETWYNJeSS7R0bVlyFkvihhtuuKHc19nZ2dlMNiUiIqJ6k49amiKEr6/vNTOlSO5Rybwj6eDeeOMNtGvXrlKfIakV9+3bp9LF6XTVa1CQJOXi3LlzqjmFysbzdG08R9fGc3RtPEe2c44kfknazC5duqgUphWxmrW+pdC33XabSoW4ZcuWco/bvn07jh8/rpKFS2B/9913VWrEQ4cOmeX/NZABA6aDBqS2Pnjw4Fr7HkRERJW1a9cu9OjRwzYC9YQJE7Bq1SoVpMsKuOWRfu02bdpgzJgxmDFjxlXPy+i+6dOnl3lyJHcpERFRXZPxVT179lRjrJo2bWr9gfrJJ5/EihUrVM24efPmVX79Pffco5oOvv/++2vWqKW5QxKDx8XFVemCgIiIqKbEx8cjLCysUrHIoqO+5RpBgvSyZcuwbt266wrShYWFiI6OLrd2LFO3ZJS4YfP09KyBkhMRETWAwWQyNeu7775TtWkJoAkJCWq/zHGTedVi7NixCA0NVXOuxWuvvYbevXsjIiJC9We/8847qungkUceseRXISIiqn+Bet68eep20KBBZvu/+OILPPTQQ+r+2bNnzUZnp6am4tFHH1VB3cfHB926dcO2bdtUczYREVF9YxV91NbaL0BEDY90p8kgVaLqcHR0hL29fY3EIquaR01EZClSZ5GWOulSI6oJ3t7eanEuWaSrOhioqyP7MnB2B9CoCRDU3tKlIaJqMARpWR3Rzc2t2j+u1LAv+rKyspCUlKQeV3cqMAN1daz7P2D3p0Cvx4GRb1m6NERUjeZuQ5Bu3LixpYtD9YBr8YBoCdbyd1VRM/i1MM1ldYT309+e2WrpkhBRNRj6pKUmTVRTDH9P1R3zwEBdHc2KA3XiQSDrkqVLQ0TVxOZussa/Jwbq6vAIAPxaSY8EcHa7pUtDRET1EAN1dYX319+y+ZuI6onw8HDMnj270sdv2LBB1R5re8T8woUL1UjqhoaBuqaav89stnRJiKiBkeBY0SZJia7H7t278dhjj1X6+L59+6okE7KqJNU8jvquqRp1QrR+upZrw7vaIyLLkOBosHjxYkydOhVHjx417vPw8DCbMiSj26+V+1j4+/tXqRxOTk5qvjDVDtaoq8szCGgcUdxPvcPSpSGiBkSCo2GT2qzUog2PY2JiVA4FSR8sSy1LgiJJI3zy5EncfvvtCAwMVIFcciH/8ccfFTZ9y/t+9tlnuOOOO9RI5sjISPz888/lNn0bmqh/++03lYZYPmfEiBFmFxYFBQX417/+pY6TKXH/+c9/MG7cOIwePbrKS1G3bNlSXSxERUXh66+/Nrs4kVYFSSMp3z8kJER9psHHH3+svouLi4s6H3fffTesEQN1TWDzN1H9XLQir8AiW02u7PzCCy/gzTffxJEjR9CxY0dkZGTg5ptvxtq1a7Fv3z4VQEeNGqXyKlRk+vTpuPfee/HXX3+p1z/wwAO4dKn82S6y4Me7776rAqekMJb3f+6554zPv/XWW/j2229VboetW7fiypUrWL58eZW+27JlyzBp0iQ8++yzOHjwIP75z39i/PjxWL9+vXr+xx9/xKxZs/DJJ5/g+PHj6v07dOigntuzZ48K2pLoSVohVq9ejRtuuAHWiE3fNdX8vfdLIJYDyojqi+z8QrSd+ptFPvvwa8Ph5lQzP88SiG666SbjY19fX3Tq1Mn4eMaMGSrgSQ1Z0g6XRxIljRkzRt1/44038OGHH2LXrl0q0JdF5g7Pnz9f1XaFvLeUxeCjjz7ClClTVC1dzJkzBytXrqzSd3v33XdVuZ544gn1ePLkydixY4faf+ONN6qLA2ldGDp0qFp7W2rWPXv2VMfKc+7u7rj11ltVy0OzZs3QpUsXWCPWqGuyRn3hAJCTZunSEBEZde/e3eyx1KilZitN0tLsLM3SUtu+Vo1aauMGEuC8vLyMS2SWRZrIDUHasIym4fi0tDQkJiYag6aQlbukib4qjhw5gn79in9/i8lj2S/uueceZGdno0WLFirrolyQSJO7kIsXCc7y3IMPPqhq99IKYI1Yo64JjUIBn+ZA6mng7E6g1TBLl4iIqsnV0V7VbC312TVFgqopCdJr1qxRtc6IiAi11KX0zebl5VX4PlIjNSV90kVFRVU6vq6TNYaFhalmbemDl+8sNe933nkHGzduVLXovXv3qv7133//XQ3Ek/5sGfFubVPAWKOuKVE3A61GAE7m/ymIyDZJYJHmZ0tstblCmvQHS3OxNDlLf600DZ85cwZ1SQa+yeAtCYoGMiJdAmdVtGnTRn0fU/K4bdu2xsdyISJ98NJUL0F5+/btiI6OVs/JCHhpFn/77bdV37uch3Xr1sHasEZdU0a8YekSEBFdk4xy/umnn1TwkguCV155pcKacW156qmnMHPmTFWrb926teqzTk1NrdJFyr///W81wE36liXg/u9//1PfzTCKXUafywVAr169VFP8N998owK3NHn/8ssvOHXqlBpA5uPjo/rH5TzIyHFrw0BNRNSAvP/++/jHP/6hFinx8/NT06JkxHVdk8+V1KJjx45V/dOywMrw4cOrlGVq9OjR+OCDD1Qzvoz+bt68uRpFPmjQIPW8NGHLiHcZZCYBW1oQJJjLdDB5ToK6NHfn5OSoC5jvv/8e7dq1g7Wx0+q608DC4uPjVb9FXFwcmjRpUu33Kygsgr1OvwqQcjkO0DkAXtXLP0pEdUd+qE+fPq1+6GVOLdU9qc1KU7bUkGUken3/u4qvQixiH3U1PL/0ALrOWIOD54qvRle/CMxuD+xaYOmiERFZtdjYWHz66ac4duyY6jOeMGGCCmr333+/pYtmdRioqyE1Kx9Xcgqw8VjxFIXAdoCdPZB10dJFIyKyajqdTvUhy8poMqVKgrX0LUutmsyxj7oaBrbyx5rDidh4LBlPDo4E2o0G2t4GOHtaumhERFZNmn1Lj9imsjFQVzNQi71nLyMtOx+NXDk1i4iIahabvqshzNcNLf3dUVikYeuJFPMnLTDdgYiI6h8G6moa2CpA3W48mqzfce5P4NPBwFe3WbZgRERULzBQV9PAKH3zt/RTq5luLt76YB23E8jPtnTxiIjIxjFQV1Ov5r5wdtAh4UoOjiamA74tAM9goDAPiC9ZHo+IiMjmArUsHydD82Vx9ICAALXKjCygfi0//PCDWnJOJpDLSjNVTY1Wk1wc7dGnZeOS5m9Z+ETSXoozHNFIREQ2HKglg8nEiRNV/lDJbCL5S4cNG4bMzMxyX7Nt2zaVE/Xhhx9WSc8luMsmScMtPfpbmr/N0l6e2WKxMhERVZYsufn0008bH4eHh2P27NkVvkZWY1y+fHm1P7um3qciskxo586dYassGqhXr16tsrjI2qqSyFwmv0tO1D///LPc18i6rpKoXBZjl4nxstRc165dVdJxSwfq3WcuITO3oKRGLU3f+TkWKxcR1W+SWEN+D8uyefNmFQQlK1RVSVYrWXu7LoLlhQsXMHLkyBr9rPrGqvqoJZm48PX1LfcYSVEmWVJMyULusr8subm5asF5w5aenl7DpQaa+7mjqa8b8gs1bDt5EWgcAXgEAoW5+oFlRES1QFoWpTVS1o0uTZJTdO/eHR07dqzy+/r7+6tsU3VB0mw6OzvXyWfZKp01LcguTS+ylFz79u3LPU6yrUgeU1PyWPaX1w8uuU8Nm2me0poiV60lzd9J+n5qNn8TUS279dZbVVCV1khTGRkZaiyPBPKLFy+q7sLQ0FAVfGVcj2SJqkjppu/jx4+rdJAyLkh+Q+XioKxsWK1atVKf0aJFC5U+U7ozhZRv+vTpOHDggPq9lM1Q5tJN37KU6ODBg1U6Ssly9dhjj6nvYyCtsNLdKRmzgoOD1THShWr4rMrGm9dee00lw5CLBKnpSwuvQV5eHp588kn1/vKdJS2mxBIhs3ukdaBp06bqtSEhIfjXv/6FBhGo5URLP/OiRYtq9H2nTJmiauqG7fDhw6gNhkC94WjxNK3w4kAdy0BNZNPyMqu+FRaUvF7uy77S0zXLe20VODg4qDSREvRMEyFKkJa0jhKgJYNTt27d8Ouvv6rfWAl8Dz74IHbt2lXpoHbnnXfCyckJO3fuxPz581VQLk0GBUs55DdWuigl4casWbPUc/fddx+effZZ1c0pTd2yyb7SZHyStJBKfmhpfpfv8ccff6igaWr9+vU4efKkuv3yyy/V55a+WKmIlO+9995TwV66BuQzb7vtNnVBIj788EP8/PPPWLJkiRrg/O2336qLF/Hjjz+q7/XJJ5+o4+UiQy5+6v0SovKPIEm8N23adM10X9JMkpiYaLZPHsv+ssgVj2mzSm3lXZWR3072OsSnZuNUSiZaNivup47bDRTkAg5s2iGySW+EVP019ywE2t2hvx/zP+CHhwD5TRj/a8kxszuUncDnVX0XYGVJbul33nlHDc415GGWZu+77rrL2JL43HPPGY9/6qmn8Ntvv6kg1LNnz2u+vwTKmJgY9RqpPYo33njjqn7ll19+2Xhfgpp8plS8nn/+eVU79vDwUBcW5f1Wi++++05dWHz11Vdwd9cvyTxnzhzVF//WW28ZW1MlkMt+yV0tM4BuueUWrF27Fo8++milzpkEaLnY+Nvf/qYey3tL0JdWhLlz56qxUpKfun///qrGLzVqA3lOvoN0wTo6OqqadWXOo83WqOUKUIL0smXLsG7dOpWz81r69Omj/kFMSTOM7Lckd2cH9GjuUzJNyz8KcPMDCrKBc3stWjYiqr8kUPXt2xeff/65enzixAk1kEyavYXUrGXQrdT6ZPyPBEwJuhJwKuPIkSMqgYYhSIuyfm8XL16sui4liMlnSOCu7GeYfpYMLDYEadGvXz9Vqzeduis1cwnSBtJEnZRUnMXwGqSydv78efW+puSxfL6heX3//v2IiopSzdq///678bh77rkH2dnZqnlfLgwkfhUUmLSg1LcatTR3yxXUihUrVLOJoZ9ZrgDlCkxIs470rRj6ByZNmoSBAweqZgu5ipIrtj179mDBAsvngJbm760nLqppWv/o31zf/H14hb75u5llLySI6Dq9eL7qr7E3aUFrPUr/Hnal6kVPR6OmSFCWmrLUBqU23bJlS/U7KaS2LU29UluUYC1BUMYDST9sTZHBvA888IDqh5ZmZPkNl99m+Z2uDY6OjmaPpdYrwbymyEwiyY29atUq1aJw7733qhr00qVL1UWLXDTIfqkkPvHEE8YWjdLlqhc16nnz5ql+Y2mukSsiwyZXZgZyRSb9GQZy5SjBXQKzXHnJiZM+gooGoNWVQVH6db93nLqInPxCfVOXofmbiGyTk3vVN3uTOpDcl32OrpV73+sggUTyO8tvozQbS3O4BC8hqSRvv/12/P3vf1e/mVITPHbsWKXfW6bBxsXFmf0Oy9oXpde3kObhl156SY00l2bj2NhY86/r5KRq99f6LBlwZrqWxtatW9V3k9ptTfDy8lKtA6VTbMpj08HGcpz0o0tfu8Qk6Zu+dOmSek4qktIcL33ZGzZsUBcqMgiuXtaoTQc/lEdOQmnS9CCbtYkM8EBwIxdcSMtRwXpQ29uB0G5AcCdLF42I6jFpapagIoNnpWlXmm4NJGhKhUaCqfTtvv/++2pcT2VnwEhNUkZzjxs3TtUc5f0lIJuSz5BKldSiZbVJGbgmTcKmpN9aaqnSpCxjkaQVtfS0LKmVT5s2TX2WjKxOTk5WLQUy+K30bJ/qkHU45HOk5UFGfEsrhJRLBo0JOUdSaezSpYu6SJBBbdKk7+3trQatyQVHr1691Aj3b775RgVu037sejvquz4wn6aVDHgGAk26mV9dExHVAmn+Tk1NVU3Ppv3J0lcsTbmyX1ovJeDI9KbKkkAlQVf6ZWXQ1COPPILXX3/d7BgZMf3MM8+oMUcS+OSiQKZnmZLBbbI4y4033qimlJU1RUwCn/SfS81VAv7dd9+NIUOG1PiCVtLvPHnyZDUSXboDZGqWjPKWCw4hFxFvv/22ah2Qcpw5c0YtVS3nQoK11LKlT1vmqEsT+P/+9z81Tay22GmVqdbWI7IwgPQxSFPOtUaYX49V0Rcw4du9aOHvjnXP6kdgEpF1k5HGUtuTAa0yb5aotv+uqhKLWNWrYf0i/WCvs8Op5EzEXcpCWGE8sP0jwM4eGFXx2rlERESlsem7hnm5OKJbU/00rQ3S/C3LiO79Coj+wXwRBCIiokpgoK4FA6P8S+ZTB7QD+k8G7pY5jg2ql4GIiGoAA3UtMAwo23YyBblFGjB0GtBqOGBfO3PsiIio/mKgrgVtg73g5+GMrLxC/Hkm1dLFISIiG8ZAXQt0Ojvc0MqvZJpWUSFwYi2w7nX9fSKySjW5uhVRUQ39PXHUdy2uUvbT3nMqUE8Z0Qr4YTyQmwa0vhkI6WLp4hFRqVWzZI6srAEtc3zlsWFlL6KqklnPskSrLNgif1fy91QdDNS1ZECEn0pLHZOQjgvpeQiWtb6PrQbObGWgJrIy8mMqc11lmUwJ1kQ1QRZwkexa8vdVHQzUtcTH3Qmdmnhjf9xlbDqWjPua9SsO1FuAvua5VYnI8qTWIz+qkgnpWmtSE12LZPeStJ410TLDQF3Lo78lUEvz932DilOqnd2m76fWlaRoIyLrID+qkgGptrIgEV0PDiarRYOK51NvPp6CgoAOgJMnkJMGJB6ydNGIiMhGMFDXoo5NvOHt5oj0nALsO5cBNO2tf0Kav4mIiCqBgboWyZrfAyJNVikLL27+jjXPg0pERFQeBupaNqh4lbINx5KAZv1LAjXnaxIRUSUwUNeyAcULnxw8dwXJnm0AR3cgOxVIOmzpohERkQ1goK5lAZ4uaBfipe5vPnUZaNpL/wSbv4mIqBIYqOtw9LdaTlTmUwsOKCMiokpgoK4DA1sFqFtZ+KTQtJ9aY9pLIiKqGBc8qQNdmnrD09kBqVn5OKi1QKfI4fom8IJcwNHF0sUjIiIrxkBdBxztdegf6YdVBxOw4UQaOj2wxNJFIiIiG8Gm7zpcTtQ4TYuIiKiSGKjryA3FgfpA3GWkZuYB6YnAoeXspyYiogoxUNeREG9XtAr0QJEGbD12HvigI/DDOODiCUsXjYiIrJhFA/WmTZswatQohISEqKw1y5cvr/D4DRs2qONKbwkJCbAFg6L0o7+lnxphvYCgjkDWJUsXi4iIrJhFA3VmZiY6deqEuXPnVul1R48eVQneDVtAgD4A2ko/tcynLnrgR+DxzSULoBAREVnbqO+RI0eqraokMHt7e8PWdA/3gZuTPZLTc3EkKQvtQhpZukhERGTlbLKPunPnzggODsZNN92ErVttZylOZwd79G3ZuGSVMpGfDeRlWbZgRERktWwqUEtwnj9/Pn788Ue1hYWFYdCgQdi7d2+5r8nNzcWVK1eMW3p6OqximpakvVz5PPBmUyD6B4uWiYiIrJdNLXgSFRWlNoO+ffvi5MmTmDVrFr7++usyXzNz5kxMnz4d1rWc6CHsjU1FbnMPOBfm6ZcT7TbO0kUjIiIrZFM16rL07NkTJ06UP8VpypQpSEtLM26HD1s2vWTTxm5o4eeOgiINB+w7lCTo4HxqIiKqj4F6//79qkm8PM7OzvDy8jJunp6esJbFT35JbQLoHIEr54DUM5YuFhERWSGLBuqMjAwVaGUTp0+fVvfPnj1rrA2PHTvWePzs2bOxYsUKVYM+ePAgnn76aaxbtw4TJ06ELRlYnPbyj+NXoIV21e9k2ksiIrK2Puo9e/bgxhtvND6ePHmyuh03bhwWLlyo5kgbgrbIy8vDs88+i3PnzsHNzQ0dO3bEH3/8YfYetqBPi8ZwdtDhfFoOUtv1hG/cTn0/ddcHLV00IiKyMnaa1rA6R+Pj49Vo8bi4ODRp0sRi5Rj7+S6Vn3pe78sYuf8JoFFT4Jloi5WHiIisMxbZfB+1rTJM01qaFArY2QNpZ4HUWEsXi4iIrAwDtYUD9ebYbBSGdNHvlOZvIiKi6gZqqapLtd1g165damDXggULruftGqSW/u5o4uOKvMIixHsVB+ozDNRERFQDgfr+++/H+vXr1X3JXCVLeUqwfumll/Daa69dz1s2OJL1y1Cr3pRbvIjLmc2WLRQREdWPQC1To2ShEbFkyRK0b98e27Ztw7fffqtGa1PlGAL1dwkh+n7qy7FAWklLBRER0XUF6vz8fLWQiJDpUbfddpu637p1azWliiqnb4QfHO3tcOQSkOvfAXBwAZKPWrpYRERk64G6Xbt2KjnG5s2bsWbNGowYMULtP3/+PBo31meHomvzcHZA92a+6v7/omYCL5wFIoZYulhERGTrgfqtt97CJ598ojJXjRkzBp06dVL7f/75Z2OTOFVtlbJfzzoADvpWCiIiomqtTCYBOiUlRaWN9PHxMe5/7LHH1IphVHmDovzx5qoYbD91ETn5hXBxtNcn6LCzs3TRiIjIVmvU2dnZKs+zIUjHxsaqdbiPHj2KgABJ40iVFRXoiUAvZ+TkF+H8yreBub2Bgz9aulhERGTLgfr222/HV199pe5fvnwZvXr1wnvvvYfRo0dj3rx5NV3GBjNNK/F8LJB8hAk6iIioeoF67969GDBggLq/dOlSBAYGqlq1BO8PP/zwet6yQRsUpW+F+DyjN3Dv18DgVyxdJCIisuVAnZWVZczr/Pvvv+POO++ETqdD7969VcCmqukX4Qd7nR3WXPRHfPBQwJ0j54mIqBqBOiIiAsuXL1dLif72228YNmyY2p+UlAQvL6/recsGrZGrI7qEeav7m46lWLo4RERk64F66tSpeO655xAeHq6mY/Xp08dYu+7SpXjdaqoSQz/14eg/gQ1vAjs/sXSRiIjIVgP13XffjbNnz2LPnj2qRm0wZMgQzJo1qybL1+D6qTPiooENM4E9n1u6SEREZKvzqEVQUJDaDFm0JPE1Fzu5fu1CvNDY3QkbMyMBFwDJMUBmCuDuZ+miERGRrdWoi4qKVJasRo0aoVmzZmrz9vbGjBkz1HNUdTqdHW5o5Y9UeCHJtaV+J/NTExE1eNcVqCWd5Zw5c/Dmm29i3759anvjjTfw0Ucf4ZVXOLWoOquUiR1FbfQ7OJ+aiKjBu66m7y+//BKfffaZMWuW6NixI0JDQ/HEE0/g9ddfr8kyNhj9I/zUyqGr0lviNicJ1KxRExE1dNdVo7506ZJKaVma7JPn6Po09nBGx9BG2FVUfG6TDgFZPJ9ERA3ZdQVqyZYlTd+lyT6pWdP1GxgVgItohAtOzfQ7YrdZukhERGRrTd9vv/02brnlFvzxxx/GOdTbt29XC6CsXLmypsvY4OZTf7j2ODblReE+xOr7qdvcauliERGRLdWoBw4ciGPHjuGOO+5QSTlkk2VEDx06hK+//rrmS9mAdGrSSK1UtjkvSr8jlgPKiIgasuueRx0SEnLVoLEDBw7gv//9LxYsWFATZWuQHOx16B/ph51/FY/8TjgIZKcCriV5v4mIqOG4rho11a5BrfyRDG/E2zcBoAGx2y1dJCIiaoiBetOmTRg1apSqnUteZkn0cS0bNmxA165d4ezsrJKDLFy4EPV13e9Nea30O7jwCRFRg2XRQJ2ZmalGkM+dO7dSx58+fVoNYrvxxhuxf/9+PP3003jkkUfM1huvDwK8XNAm2Av/K+yDI1ETgfZ3WbpIRERkC33UMmCsIjKorCpGjhyptsqaP38+mjdvjvfee089btOmDbZs2aISgQwfPhz1bZWyeRfaYYEuFLNCO1u6OEREZAs1alnbu6JN1vweO3ZsrRVWpoANHTrUbJ8EaNlfb5u/jyWjqEizdHGIiMgWatRffPEFLCkhIQGBgYFm++TxlStXkJ2dDVdX16tek5ubqzaD9PR02IJuzXzg4eyA/MxLiNu2BM38PIHWN1u6WEREVMfq/ajvmTNnmtX627ZtC1vgaK9Dv4jGGKzbj2Z/PAZsftfSRSIiIguwqUAt+a8TExPN9sljLy+vMmvTYsqUKUhLSzNuhw8fhq0Y2CoAO4vaIM4+DAjtDmhsAiciamhsKlDLcqVr164127dmzRrjMqZlkWlcEsgNm6enJ2zFwCh/XEBjDMx6C2mDXodKrUVERA2KRQN1RkaGmmYlm2H6ldw/e/assTZsOjjt8ccfx6lTp/D8888jJiYGH3/8MZYsWYJnnnkG9VGotysiAzwgY8m2nEixdHGIiKihBeo9e/agS5cuahOTJ09W96dOnaoeX7hwwRi0hUzN+vXXX1UtWuZfyzQtyYtd36ZmlTX6e0vMOSAh2tLFISKiOmanaQ2r4zM+Ph5hYWEq01eTJrJEp3XbfDwZT/93Dba6TIKzrgh2L5wFnNwtXSwiIqqGqsQim+qjboh6hPsiy9EXKZoX7IoKgLidli4SERHVIQZqK+fiaI8+LRtjZ1Fr/Q7JT01ERA0GA7WN9FPvKCqe/32GCTqIiBoSBmobCdQyn1po5/4E8rIsXSQiIqojDNQ2INzPHTqfcFzQfGFXlA/E77Z0kYiIqI4wUNuIgVEB2FFcq2Y/NRFRw8FAbUOrlBmbv2MZqImIGgoGahvRu0Vj7LXTDyjT4v8E8nMsXSQiIqoDDNQ2ws3JAYHh7ZCoeUNXmAuc22PpIhERUR1goLaxfmpD8zf7qYmIGgYGahsyyKSfuvA0AzURUUPAQG1DWvp74JR7F+Rp9kjLLWJ+aiKiBoCB2obY2dkhPKozOuZ+hg9D3mF+aiKiBoCB2gb7qXPgjE3Hki1dFCIiqgMM1DamX0RjOOjscColE3EJKZYuDhER1TIGahvj6eKIQU3s8IvTiwj6tANQkGfpIhERUS1ioLZBXdtEINjuIhwLs4DEg5YuDhER1SIGahs0KCoQ/8x7BgOL5iE3sJOli0NERLWIgdoGtQn2RKxHJ8TmNcKeM6mWLg4REdUih9p8c6q9aVqSo3rpn/HI3fAesCUaCGwPBMnWAfBvDTg4W7qYRERUAxiobXiVMgnUrhd2AoV/Amc2lzypcwD8WpUEb3XbAfAIsGSRiYjoOjBQ26j+EX5qmta0rHvRUdcdbXVn0c35HCK1M3ArvAIkHdZv0UtKXuQeoA/cHf8GdLrPksUnIqJKYqC2Ud5uTvhwTBcs3xeAjXERWJqeC+TLMxqCcAltdbHo4hSPnm7nEVl0Bj45cbDLTAJOrgOa9i15o9RYYPHfgdBuwKjZFvxGRERUFgZqG3Zzh2C1aZqG82k52H/2MvadTcX+OF9sPeePdTldgeK01a7IQZRdPPp7XkDRmRYIdDyDLk290SbtLzgm/KUCvJnvimvcxubzDoBvc0BnX/dflIioAWOgrieDy0K9XdV2S8dgtS+/sAgxF9KxPy4V++IuqyC+P8UF+69EAFcAHDmkjgt0yMKdjV9Gc3d3uB44r4J3qKcD7KTmXZgHHFtd8kGObkBAW/N+bxm45upd699RLkbScwtwMSMPFzNykZKRh+z8AvQI90UTH7da/3wiIkux0+QXsAGJj49HWFgY4uLi0KRJEzQkl7PysF+CdvG27+xlpGWr9nIzge4OuDPwPHq7XUBrnIZf5nHYJ8cABdllv7FHoH7w2tDpQJNu+n2F+fpBbRUkDsktKMSlTAm8eUjJyNUH4Uz9bUrxfeP+jDzkFRaV+T6dmjTCiPbBGNk+COF+7td5doiIrDMWWUWgnjt3Lt555x0kJCSgU6dO+Oijj9CzZ88yj124cCHGjx9vts/Z2Rk5OcVtvNfQkAN1afJPf+ZiVnFzuT54Hz5/BQVF5n8SEmtb+7thaGAGertfQGu7WPimH4OdrIqWft54XNEj65Hm014FWPvdnyJs3zs4GnoXfmvyL1ULvpieC6e0kzic0xiJmYVIzymocpk9nB3Q2MMJjd2dVGO9lNn0L7hNsBdubh+EkR2CEBHgWb0TRERUS6oSiyze9L148WJMnjwZ8+fPR69evTB79mwMHz4cR48eRUBA2dOJvLy81POmTb9UdXLemvu5q+3Orvo/lJz8Qhw6n6Zq24Ym83OXs3EkKQtHknT4CKEAQuHuNAAdmjSCp2c2XK+cgk/2Gfz48RlkFF1Q7/OawzaMdcjCppOX8eHR42qfP1Kx22Ui8jV7xGqBOOkYglMIRaJTU6S6NUeWVwt4ePmoINzYw1kFZD8VlJ3h5+ms9rs4mveRJ6fn4vfDCVgVnYDtpy7iyIUrantvzTFEBnioWvbIDsFoHeTJvxMiskkWr1FLcO7RowfmzJmjHhcVFamrjKeeegovvPBCmTXqp59+GpcvX76uz2ONuuqS0osHqhUH7r/iLyMzr7Dc4xu5OiLQ3Q7tXC7B1d0T9j5NVdBtVXgcw3Y9AgdZo7w8niGAfyt9U7phC+sFOLpcs5ypmXlYczgRKw9ewNYTKcgvLPnTDm/spgK2BO4OoY0YtInIomym6TsvLw9ubm5YunQpRo8ebdw/btw4FYhXrFhRZqB+5JFHEBoaqoJ6165d8cYbb6Bdu3ZlfkZubq7aDM6dO4e2bdsyUFdDYZGG40npiI5Pg4O9narx6mu/zvBxc4KTQwUr0xYV6ZvLk48CKceBlOJbeSzTx8ry3PGSxVoO/gRcPgtE3gQElv1vLqTvfe2RRKw6mICNx5KRV1DSvy2D7gw17S5h3tDpGLSJqG7ZTNN3SkoKCgsLERgYaLZfHsfExJT5mqioKHz++efo2LEj0tLS8O6776Jv3744dOhQmV925syZmD59eq19h4bIXmeH1kFeaqsynQ5o1ES/RQwxfy47tTh4HysO5MeA9ATA3b/kmL8W60eiO7mXBOpLp4F93+jngsvmGahq9dKcL1tGbgHWxyRh1cELWB+TrJryP9tyWm1BXi4Y0T5IbTKCXL4bEZE1sWiN+vz586pmvG3bNvTp08e4//nnn8fGjRuxc+fOa75Hfn4+2rRpgzFjxmDGjBlXPc8adT2z61Pg7A6gzxP6oCz2fg38/GTJMY3CgNCuJYE7uDPg7KGeys4rxMZjErQTsPZIkgriBtIfPqxdEG5uH4xeLXzhaM+cNUTUwGvUfn5+sLe3R2Jiotl+eRwUFFSp93B0dESXLl1w4sSJMp+XEeGyGVy5IpOIyWb1fFS/mWrcEujyd+DcXiDpCJAWp98OF3ed2OkA/zYqeLuGdsMI2e7pgJwiO9WXvTI6AWsOJ6gpYd/tPKs2bzdHDGsbiJHtg9Evwq/i5nwiolpk0UDt5OSEbt26Ye3atcY+aul3lsdPPmlSQ6qANJ1HR0fj5ptvruXSktVq1le/idx04MIBIH4PcO5PffC+Eg8kHdJv+77WHxfSBS6PbcCQNoFqy7scgO2J9lh9KAG/HUpU87uX7IlXm6eLA4a2kaAdhBta+V818pyIqDZZfHqWTM2SwWPdu3dXc6dlelZmZqZxrvTYsWNV87j0NYvXXnsNvXv3RkREhBpwJvOvY2Nj1QAzIjh7AuH99ZuB9HOroG3Y9poPRCvMh9Oczhjo5IGBj2/BjNvbY9fpS/gtOh4rD6eoKWDL9p1Tm5uTPQa3DlA1bUmM4uZsr5KjcBQ5EdXbQH3fffchOTkZU6dOVQuedO7cGatXrzYOMDt79ix0MgCpWGpqKh599FF1rI+Pj6qRSx+39DsTlckzCGh9i34zjDzPzyx5XgajFRUCRflqlTUHnQ59I/zQd//zeNVzPy6Ftceu/Ob4MSEQm9OD8ctfF9RmysleB0d7Ozg6yK2u5LG61an9TqaP5RgHO/VZhvtmzxmONb7f1e/l7+mMVoGe8HRxrOMTSkQNah51XeM8aipTfo5+2pfM4TaY3RG4HGt2WJHOEYmuEdie2wy7spsgQfNBkuaDRM0Hl+AJDXXfly3TzaKCPPVboP62hb87nB3YRE9krWxmHrUlMFBTpWVdAs7v0zeVn9uj7/fOSin3cE3ngOT+M5DS+u8qKYrd5bPwPvETMjzCcT50pNon65XnFxQhv0hTj2VRlnzDPvW8YX/x44JSj+X5Av37nEvNRsKVspfOleZ4WXGuVZAnWgd66m+DPBHm48Z540RWwGZGfRNZNTdf/Vxvw3xvuaaV0eTSzy1BW+Z6ZyQA6YlAZjLsigoQ4B+AgJDi+eWZ24ADs4CQrmh700Ml7zunB5CXpW+SN918gwEPw+Ng/edfo+9bEq0cS8zA0YQrOJqYjqMJ6YhJSFfrqB9PylDbryhppnd1tEerQA9V65Zmc5kL3yrIA/4ezuxnJ7JSDNRElSWBzLupfmt3h/lzki0sIwlwMVkExjMQ6PKg/ngDCfaX4/SZyGQ0ekV0jiVBfMCzQNRI/f7MFOD8fsA7DN7+UejZ3FdtJR+hqZq2BG3jlpiugnZ2fiEOxKepzZSvu5MK4CpwFzefyyZJUIjIsvi/kKgm2DsCjSRhiQnDgiulPbVHPxJdbReAjET9rXpcfF+a2GVwm2FOeL7J+uiy4MviB4AmPYFH1pTs/3QwUJALOzdfBLv6ItjNF4PcGgNNfYHWvih08cGFfA+cSHfCocuOiE4uwrGkDJy5mKmmo+04dUltZl/B21U1mRuaziWIt/T3uO555XIRIUvQSoY289si/W1h2fsNm2G/TJHj8q/UUDBQE9V1rdywhGpFCvL0a58bArqstGagswcC2gGNI8xfk3i4/Jzhci0BoEnxNki9jwNw62zkdLgfJ5IycP74Pvgf+i+O5Afjw6zhqlYuy602SjuC00edsEjzQBo8oNPZo1ljNxUsywqixqArjwvN95fKoFotLf3d8c+BLTG6cygXpKF6jYPJiOoD+W8sA9+yLwFZqUDWxeL7l0rdv6S/b6ih3/050P4u/f3DPwNLHtRnK3v4d9X/Lc3m7Rf3hnuuPmFKEexwRXNDquaBbLggF47I0Zz0t9Df5mqOWFbUH9uL9HPVg3ERt9pvR5LmjRVFJfPbe9jFwMGuUB2fCycU6AybCwrsnFCoc1aj7O3tdWoNdhkgp7/V4fzlbKQXL/8a3MgFjwxogb/1CIM7m+rJRnAwGVFDrKmb1rqvJT9bH7RdGpXsk5SiN75szFTm7eaEXi0aA16+wJVcIDcNOmjwtstUW0VuuGEkMtoPVMHVLX4zApZ/hwK/Npj60Ksq0Nrb28FtwTToLupzlZdJEp4VSdO2C2DnDOhcgX6TgN4TkJ6Tj0XbTyJmy0/4I60lZvySg4/WHce4PuF4qG84fNydKn8uiKwcAzVRQ+ToenWfekBr/VbaxJ0lA+Ykw5mxVp4NFOQUb7nFj3PV46CIfkCAPhEKCpoAHe+Dg2cwGnuUrLsP3xb6ZnyT1xk3I03fnC9bzmXjc7LIy6ORGcDGt5Dr6Y3hjl/gzKVsfLD2OL7edBi394zEowNaIMTbtebPHVEdY6AmosoPmJPatiE3eGUFtQfuXHD1/geWlN+MX5hXKoDLbbZaOc4oJw3wi4KzXyTW3nsjVh9MwMfrj+OTS+ORs9sJG3e1QVHTvug7ZBSat4iq4pclsh7soyYi2yY1fbmIkBifdg52s65eTjjZIRi65v3QuO1gILwf4N3smnPUiWoT+6iJqOEoDtLCTprznz+tprAlRa9F1onNCMs5Bv+CC8DxpfpNArpXKOya9dNnXZMELjKCnoGbrBQDNRHVL7KiW+ubEdBan/r2ZPx5rP/9fyg4vQU97I6go90pOF45B0Qv0W/imUMlU+bUIDtvwCQZEJElMVATUb3WskkIWv7jnzh/eSw+23waj+w6jjaFMeili8FAp6No7poNF/dgGIe5LXsciN8F3DYHaHMrGrqM3AKcTMpQc+1PJGcg7lKWGs0v8+hLNp1antZw3/Q5V5N9ct/Z5L5kg6NrY6AmogZBRoBPHdUWTw2OwJfb2+CLbWcwKysfdllF8H9rPR7u3xz39wyDZ0K0fnS76aj4gz8BB77XN5VLk3lwZ8Ch/kwBk6FKKRl5xmBsCMwnkzNwIa3sxC81QebFuzjo4Opkr7K9qYDvZA8Xdd888LsW3/d1d0a/iMZoH9KowaxMx8FkRNQgZeYWYNHuOHy2+ZQxGHm6OOChXqF4uGUavFv2AuyL6zIrJgL7vil5sazqJtPLZO652RZhPjfdyhQVaYhPzcaJ5HR9IE7KVIFZ7qdl55f7Oj8PZ0QEuCMiwAPhjd3Vvuy8QuQUFCInv0itIZ+TX4hck/uyZecXIdd4X3+svKYmok5jdyfc0Mofg6L8MSDSX61Xb0uY5rICDNREZCqvoAgr9p/D/I0ncTJZv5CLs4MO93YPw2M3tECYrxuQdAQ4uR6I3arfpMZdHsmA5hcJtL5FLc5iJD+1dTRgLbegEKdTMvWBuLiWLLenkjOQWyAryVxNiiZpUCUYy/Kscqs2f080cisZsFddEnKkDLnFQdss4BffzzUN7Pkl92W/fK9tJy+qJnnTsndq4q2C9sBW/ujYxFvV1q0ZA3UFGKiJqLza5pojifh4w0kciLus9smP/aiOwXh8UEuVWaz4QCD9vD7NacpxIOVY8XZcn/bUoPs/gFtn6e/nZQLvttLXwv/xG+Dkpt8vSVikBu7ocl1lvpKTb9Z/bLh/9lJWueuqO9nr0MLfXSVXaWkMxh5qnzQx28rF1Z+xqdhwLAkbjyar1K6mfNwcjbXtGyL9zRfasRIM1BVgoCaiishP4vZTFzFvw0lsPp5i3D+4dQAmDGqJHuElKUWvIouwpJzQB27f5kDT3vr9Fw4An9wAuPkBz59EfqG+idh50X1wOrMO+V5hyPZqiUzPFrji0RypbuFIcW6GNDsv5BToa5pyvNryClUgloCclJ5bblE8nR1KAnFxMJZbaSGw9tpmVSWk5WDjsSRsOJqMLcdTjOvAG2rbHUIbYVArfwyMCkDnMOuobTNQV4CBmogq6+C5NMzbeBIroy8Y+1W7N/PBrR2DVVaw3FJBNKdUQDU02+bn5cM3/zw88i9ha34r9Vrxq9MUtNPFlvv5kvzkpBaCk0UhOCG3Wgiii5ojGT7qeWfkIcojG2F+XmgcHG4MyFGOifB1LoKdVgTIJq0A6n4hUFRYct/0ucYt9ZuQpv2T6wB7Z/OR70dX6dOwynuorcBkK+exDMBrN7pk6tvK5yT0AHf/t+R9170OnN1ewXvmlzy2d9LPe4+8Cej1z6vOmVwE7Y1NxcZjySpwH75wxez5Rq6OGBDph0FRAaqZ3N/TMrVtBuoKMFATUVVJv+iCTSfx45/nkFdYdh/v9bCz09DEMQOtHRIQqbuAlrrzCNfOIawoHn6FSSoJSmnbwifiXPsJKii3ytwD98V3A4HtgQlbSw76sCtw6WTVCiMJWQb+W39fRr7P769fsvW5YyXH/HcYEFe89ntl9fwncPPb+vuSsvW9KMDOHphmkvt80QNAzC9Ve9/ODwCjPy5JC/tBJ/2Fxt++A1yKuynyMpGUrcOG4ykqcG8+lowrOSW1bdE+1AuDWgVgYJS/ynHuUEdTxrgyGRFRDWru546Zd3bE00Nb4cttZ3A8KUNNF1KbTCcy3i+ZQ1z28yX7ZT6xDFqzK2+AWV6WPtga+r+L+8L79rkBiArTH3PaBXBwMVudTXH3A/IyADudPijKrSzgYvZYbmWz0983XcPdyQMIHwC46mvuRlI7luZ7OV5Gvsvnyq3hsXEzedykR8nrnb2AEW/q95vqMxFof2c57+Fovk++l+paaFHy+kun9OMGctMBZ8+S/cv+iYBTG3GvXyvc6x+FwiGROIVQbLzoi5/POuCv85k4eO6K2uasPwEvFwc1glyCtjSVB3hd39iBmsYaNRER2baCXCDxIJCRDESNKNk/tzeQfKTs19g7o8C3JS44NkN0biA2XPLBgZxAnNaCkQf9hU+bYC81IE2CdtdmPjW6QAubvivAQE1E1IAC+EVplTgKJB8rvi0erV9Y9kC8HU3+gZk5d+Gvc2lopKVjsG4fjmphOOsUif6Rfqpf+9ZOIfBwrl6DNJu+iYiIHJyBwLb6zZQMSrscaxK8jwHJMapJvXevfljRoT8uZuQiZstP6LdjvmouH5zzDlYdTMBvhxIwvF2QjOSru69Rdx9FRERkBXT2+j5u2UybyqWBWUbAy8pnHs7o1yoESBiAcN+WWN6lHzYcTULilRz41PEqaFaxIvrcuXMRHh4OFxcX9OrVC7t27arw+B9++AGtW7dWx3fo0AErV66ss7ISEVE9ZVc8sM6gxUDgoV+gu+0DNf9aBhPKoMK6ZvFAvXjxYkyePBnTpk3D3r170alTJwwfPhxJSUllHr9t2zaMGTMGDz/8MPbt24fRo0er7eDBg3VediIiotpm8cFkUoPu0aMH5syZox4XFRWpDvannnoKL7zwwlXH33fffcjMzMQvv5TMuevduzc6d+6M+fPnX/PzOJiMiIgsrSqxyKI16ry8PPz5558YOnRoSYF0OvV4+/btZb5G9pseL6QGXt7xREREtsyig8lSUlJQWFiIwMBAs/3yOCYmpszXJCQklHm87C9Lbm6u2gzS080XbyciIrJmFu+jrm0zZ85Eo0aNjFvbtqWG6RMREVkxiwZqPz8/2NvbIzEx0Wy/PA4KCirzNbK/KsdPmTIFaWlpxu3w4cM1+A2IiIjqcdO3k5MTunXrhrVr16qR24bBZPL4ySefLPM1ffr0Uc8//fTTxn1r1qxR+8vi7OysNoPLl/V5Zi9cuFDD34aIiKhyDDFIYt41aRa2aNEizdnZWVu4cKF2+PBh7bHHHtO8vb21hIQE9fyDDz6ovfDCC8bjt27dqjk4OGjvvvuuduTIEW3atGmao6OjFh0dXanP27Vrl4xy58aNGzdu3DRLbxKTrsXiK5PJdKvk5GRMnTpVDQiTaVarV682Dhg7e/asGglu0LdvX3z33Xd4+eWX8eKLLyIyMhLLly9H+/btK/V5Xbp0UQuqyPubvu/1kIFp0uctzemeniYZW6hMPF9Vx3NWNTxfVcPzZbnzJTVp6baVmGT186ht2ZUrV9QANen79vIqzn9K5eL5qjqes6rh+aoani/bOF/1ftQ3ERGRLWOgJiIismIM1NUgo8lljXLTUeVUPp6vquM5qxqer6rh+bKN88U+aiIiIivGGjUREZEVY6AmIiKyYgzUREREVoyBuhrmzp2L8PBwuLi4qLzaspAKlW3Tpk0YNWoUQkJCYGdnpxapofITyUiOdllQISAgQC2ve/ToUUsXy2rNmzcPHTt2VPNaZZPlhFetWmXpYtmMN998U/2fNF2Wmcy9+uqr6hyZbq1bt0ZdYaC+TosXL8bkyZPVCMC9e/eiU6dOKi92UlKSpYtmlTIzM9U5kosbqtjGjRsxceJE7NixQ61jn5+fj2HDhqlzSFdr0qSJCjaS237Pnj0YPHgwbr/9dhw6dMjSRbN6u3fvxieffKIudKhi7dq1U+tzG7YtW7agzlz3It0NXM+ePbWJEycaHxcWFmohISHazJkzLVouWyB/dsuWLbN0MWxGUlKSOmcbN260dFFsho+Pj/bZZ59ZuhhWLT09XYuMjNTWrFmjDRw4UJs0aZKli2S1pk2bpnXq1Mlin88a9XXIy8tTV+9Dhw417pN1w+Xx9u3bLVo2qn9kuULh6+tr6aJYvcLCQixatEi1PpSXUY/0pNXmlltuMfsdo/IdP35cdd21aNECDzzwgMpDUVcsnpTDFqWkpKgfBEPiEAN5HBMTY7FyUf0jC/dL32G/fv0qnXimIYqOjlaBOScnBx4eHli2bJlKnkBlk4sZ6bKTpm+6NhmDtHDhQkRFRalm7+nTp2PAgAE4ePBgnSQzYaAmsvJaj/wY1Gl/mA2SH9D9+/er1oelS5di3Lhxqq+fwfpqcXFxmDRpkhr/IANh6dpGjhxpvC/9+RK4mzVrhiVLluDhhx9GbWOgvg5+fn6wt7dXKcpMyeOgoCCLlYvqlyeffBK//PKLGjEvA6aofE5OToiIiFD3u3XrpmqKH3zwgRooReak204GvXbt2tW4T1oI5e9szpw5yM3NVb9vVD5vb2+0atUKJ06cQF1gH/V1/ijIj8HatWvNmijlMfvFqLpkvJ0EaWm+XbduHZo3b27pItkc+f8oAYeuNmTIENVVIC0Qhq179+6q31XuM0hfW0ZGBk6ePIng4GDUBdaor5NMzZLmNfkD79mzJ2bPnq0GsIwfP97SRbPaP2zTq8/Tp0+rHwUZINW0aVOLls0am7u/++47rFixQvV/JSQkqP2SB9fV1dXSxbM6U6ZMUU2T8neUnp6uzt2GDRvw22+/WbpoVkn+pkqPd3B3d0fjxo05DqIczz33nFoHQpq7z58/r6blygXNmDFjUBcYqK/Tfffdh+TkZEydOlX9kHbu3BmrV6++aoAZ6cn81htvvNHsQkfIxY4M0iDzBTzEoEGDzPZ/8cUXeOihhyxUKuslzbhjx45Vg3zkYkb6ECVI33TTTZYuGtUT8fHxKihfvHgR/v7+6N+/v1rnQO7XBWbPIiIismLsoyYiIrJiDNRERERWjIGaiIjIijFQExERWTEGaiIiIivGQE1ERGTFGKiJiIisGAM1ERGRFWOgJqJaY2dnh+XLl1u6GEQ2jYGaqJ6S5UYlUJbeRowYYemiEVEVcK1vonpMgrKsEW7K2dnZYuUhoqpjjZqoHpOgLDnSTTcfHx/1nNSuJQGIZJ6SrFwtWrTA0qVLzV4v6RAHDx6snpfsSo899pjKhGbq888/R7t27dRnSdo/SdFpKiUlBXfccQfc3NwQGRmJn3/+2fhcamqqSq8oyQ3kM+T50hcWRA0dAzVRA/bKK6/grrvuwoEDB1TA/Nvf/oYjR46o5yRt6/Dhw1Vg3717N3744Qf88ccfZoFYAr2k5ZQALkFdgnBERITZZ0yfPh333nsv/vrrL9x8883qcy5dumT8/MOHD2PVqlXqc+X9/Pz86vgsEFk5yZ5FRPXPuHHjNHt7e83d3d1se/3119Xz8t//8ccfN3tNr169tAkTJqj7CxYs0Hx8fLSMjAzj87/++qum0+m0hIQE9TgkJER76aWXyi2DfMbLL79sfCzvJftWrVqlHo8aNUobP358DX9zovqFfdRE9ZjkADfktzbw9fU13u/Tp4/Zc/J4//796r7UcDt16gR3d3fj8/369UNRURGOHj2qms7Pnz+PIUOGVFgGyQ9tIO/l5eWlckiLCRMmqBr93r17MWzYMIwePRp9+/at5rcmql8YqInqMQmMpZuia4r0KVeGo6Oj2WMJ8BLshfSPx8bGYuXKlVizZo0K+tKU/u6779ZKmYlsEfuoiRqwHTt2XPW4TZs26r7cSt+19FUbbN26FTqdDlFRUfD09ER4eDjWrl1brTLIQLJx48bhm2++wezZs7FgwYJqvR9RfcMaNVE9lpubi4SEBLN9Dg4OxgFbMkCse/fu6N+/P7799lvs2rUL//3vf9VzMuhr2rRpKoi++uqrSE5OxlNPPYUHH3wQgYGB6hjZ//jjjyMgIEDVjtPT01Uwl+MqY+rUqejWrZsaNS5l/eWXX4wXCkSkx0BNVI+tXr1aTZkyJbXhmJgY44jsRYsW4YknnlDHff/992jbtq16TqZT/fbbb5g0aRJ69OihHkt/8vvvv298LwniOTk5mDVrFp577jl1AXD33XdXunxOTk6YMmUKzpw5o5rSBwwYoMpDRCXsZESZyWMiaiCkr3jZsmVqABcRWS/2URMREVkxBmoiIiIrxj5qogaKvV5EtoE1aiIiIivGQE1ERGTFGKiJiIisGAM1ERGRFWOgJiIismIM1ERERFaMgZqIiMiKMVATERFZMQZqIiIiWK//B2WD9xNrcWhBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 使用 plot_values 绘制训练损失曲线\n",
    "# ==========================================================\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1️⃣ 创建 x 轴数据：训练轮数对应的 tensor\n",
    "# ---------------------------------------\n",
    "# torch.linspace(start, end, steps)\n",
    "# 生成一个从 start 到 end 的等间距 tensor，共 steps 个点\n",
    "# 这里 len(train_losses) 个点对应每次记录的训练损失\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "# 说明：epochs_tensor 中每个值表示训练过程中每次记录 loss 时对应的训练轮数\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2️⃣ 创建第二个 x 轴数据：累计训练样本数\n",
    "# ---------------------------------------\n",
    "# 同样使用 torch.linspace 生成等间距 tensor\n",
    "# 从 0 到 examples_seen，共 len(train_losses) 个点\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "# 说明：每个点对应训练过程中累计处理的样本数，方便在双 x 轴上显示\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3️⃣ 调用自定义的 plot_values 函数绘图\n",
    "# ---------------------------------------\n",
    "# 参数解释：\n",
    "# - epochs_tensor: 第一个 x 轴，表示训练轮数\n",
    "# - examples_seen_tensor: 第二个 x 轴，表示累计样本数\n",
    "# - train_losses: y 轴，训练损失值\n",
    "# - val_losses: y 轴，验证损失值\n",
    "plot_values(\n",
    "    epochs_seen=epochs_tensor,        # 第一 x 轴：训练轮数\n",
    "    examples_seen=examples_seen_tensor, # 第二 x 轴：累计样本数\n",
    "    train_values=train_losses,        # 训练集指标值\n",
    "    val_values=val_losses             # 验证集指标值\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd28174-1836-44ba-b6c0-7e0be774fadc",
   "metadata": {},
   "source": [
    "- 从上图的下降趋势可以看出，模型学习效果良好。  \n",
    "- 此外，训练损失和验证损失非常接近，表明模型不易过拟合训练数据。  \n",
    "- 同样，我们可以绘制下面的准确率曲线。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "yz8BIsaF0TUo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "yz8BIsaF0TUo",
    "outputId": "3a7ed967-1f2a-4c6d-f4a3-0cc8cc9d6c5f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAEiCAYAAADONmoUAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAU8JJREFUeJztnQdYFFcXhj/poliwY0XFrth77yVGjT0aiRr9NbYkmhiNLSZG0zTGGk3UNHtPbCEae+8du2BBsYJIZ//n3HWXBQFZXNhl93sf52Hv7OzMnSvsN+fcc8/JpNFoNCCEEEJIumOX/pckhBBCiEARJoQQQswERZgQQggxExRhQgghxExQhAkhhBAzQREmhBBCzARFmBBCCDETFGFCCCHETFCECSGEEDNBESaEJErjxo3xwQcfmLsbhFg1FGFC0oh3330XmTJlemlr3bq1ubtGCLEQHMzdAUKsGRHcxYsXx9vn7Oxstv4QQiwLWsKEpCEiuPnz54+35cyZU723c+dOODk5Yc+ePfrjv/nmG+TNmxf37t1T7a1bt6J+/frIkSMHcuXKhTfeeANXr17VH3/jxg1lXa9cuRINGjRA5syZUaNGDVy6dAlHjhxB9erVkTVrVrRp0wZBQUHxrPSOHTvi888/R548eZAtWzYMGjQIkZGRSd5LREQERo0ahYIFCyJLliyoVauWugcdN2/eRPv27dX9yfvly5fH5s2bkzzf3Llz4eXlBRcXF+TLlw9dunTRvxcbG4upU6fC09NT3ZO3tzdWr14d7/Nnz55V9yX3J59/55138ODBg3ju9OHDh+OTTz6Bu7u7GvtJkyal6P+NkPSCIkyImedcRTyePn2KEydOYPz48fj555+VqAihoaH46KOPcPToUWzfvh12dnbo1KmTEilDJk6ciHHjxuH48eNwcHDA22+/rcRn5syZSuSvXLmCCRMmxPuMnO/ChQtKSJctW4a1a9cqUU6KoUOH4sCBA1i+fDlOnz6Nrl27Kkv/8uXL6v0hQ4Yood69ezfOnDmDr7/+WglkYsj9iEBOnjwZfn5+6mGjYcOG+vdFgH/77TfMnz8f586dw4cffojevXtj165d6v0nT56gadOmqFKlijqXfF4eXLp16xbvOr/++qt6IDh06JB6wJHr+fr6Gv1/RUiaIaUMCSGmx8fHR2Nvb6/JkiVLvG3KlCn6YyIiIjSVK1fWdOvWTVOuXDnNgAEDkj1nUFCQlB7VnDlzRrWvX7+u2j///LP+mGXLlql927dv1++bOnWqpnTp0vH65u7urgkNDdXvmzdvniZr1qyamJgY1W7UqJFmxIgR6vXNmzfVvdy+fTtef5o1a6YZM2aMel2xYkXNpEmTUjQ2a9as0WTLlk0THBz80nvh4eEaV1dXzf79++Pt79+/v6Znz57q9RdffKFp2bJlvPcDAgLUffv5+en7X79+/XjH1KhRQzN69OgU9ZGQ9IBzwoSkIU2aNMG8efPi7RPXqA5xR//555+oVKkSihYtihkzZsQ7VqxMsWDFkhNXq84C9vf3R4UKFfTHyed16KzoihUrxtt3//79eOcWF6+rq6u+XadOHTx79gwBAQGqL4aIZRsTE4NSpUrF2y+Wr7jJBbFsBw8ejH/++QfNmzdH586d4/XLkBYtWqhrFC9eXFnTsomFL/0Rq/358+fqGEPEVS6Wr3Dq1Cn8999/iVra4q7X9TPh9QsUKPDSOBBiTijChKQh4gotWbJkssfs379f/Xz06JHa5DM6ZI5VxGrhwoXw8PBQIizim3Du1tHRUf9a5ogT25fQhW0MIs729vY4duyY+mmITgjfe+89tGrVCps2bVJCLC7l77//HsOGDXvpfG5ubsp1Lq5wOVYeNGS+Vuax5VqCnEfmnxMLapNjZGzE5Z0QEdrExsUU40CIqaEIE2JGxGqT+U4R2RUrVsDHxwf//vuvmvt9+PChmi+V9yToSti7d6/Jri3WZFhYmAp8Eg4ePKgEtXDhwi8dKxaoWMJiRer6khjyWQnwkm3MmDGq74mJsCBz12IxyyZz2hJ8tmPHDmUBi9iKtd+oUaNEP1u1alWsWbMGxYoVU+chJKPC315C0hBx1wYGBsbbJ6KRO3duJWoSbCTWY9++fZVLVlzIYj1+/PHHKspYXL0LFixQ1p2I0qeffmqyvok13b9/fxXQJVHWIoQSfCUPAAkR926vXr3Qp08f1T8RZYm2luAucfm2a9dOBZlJtLIc+/jxY+UuLlu2bKLX/vvvv3Ht2jUVjCX3KVHUYqGWLl1aWckShS0PJ7JPosMlcG3fvn0qilseVCQITAS+Z8+e+uhncWNL0JgEtiW01gmxVCjChKQhErVr6B4VRGguXryIKVOmqGU9IkiCHCeCK8LSsmVLNWcroiJzreKCls/9+OOPKqraFDRr1kwtERIhlIcFuW5yS3hkvfOXX36JkSNH4vbt2+pBonbt2mrZlCAPFSKOt27dUmIpDxUJ57h1iNUr0dhyvfDwcNUPidCWZU3CF198oZZOiUtbxFqOF+t37Nix6n1xzYsojx49Wo2V9F/c9nLNxB4iCLFUMkl0lrk7QQhJX2SdsCzzWb9+vbm7QohNw0dGQgghxExQhAkhhBAzQXc0IYQQYiZoCRNCCCFmgiJMCCGEmAmKMCGEEGImKMKpZM6cOSpbj5Rhk5Juhw8fhjUiFXEkPaCsy5SUfwmXtEhIgaQclDWuknlJsh/pqurokFSMkuhB1o7Kek9JEKFLTahDqvJIJiYZT8m6JBVvLB1ZwyplAyW5hJQflNKAkuHKEFkDK2tnJemGZKOSfMq6MoU6JAmHJLuQvMlyHknUER0dHe8YSe8o62Qlk5SkwVyyZAksGcmXLUk85P9cNslLvWXLFtj6uCTFtGnT1N+XJDzRYctjNGnSJDUehluZMmWsc2zSpUyElbF8+XKNk5OTZtGiRZpz586pyjc5cuTQ3Lt3T2NtbN68WfPZZ59p1q5dqyrUrFu3Lt7706ZN02TPnl2zfv16zalTpzRvvvmmxtPTUxMWFqY/pnXr1hpvb2/NwYMHNXv27NGULFlSXw1HePr0qSZfvnyaXr16ac6ePauqAGXOnFnz008/aSyZVq1aaRYvXqz6fPLkSU3btm01RYoU0Tx79kx/zKBBgzSFCxdWFY2OHj2qqV27tqZu3br696OjozUVKlTQNG/eXHPixAk13rlz59ZXJhKuXbumqgp99NFHmvPnz2tmzZqlKhpt3bpVY6ls3LhRs2nTJs2lS5dUVaOxY8dqHB0d1VjZ8rgkxuHDhzXFihXTVKpUSV+1ytbHaOLEiZry5ctr7t69q9+kgpg1jg1FOBXUrFlTM2TIEH1bSr95eHiocnHWTEIRjo2N1eTPn1/z7bff6vc9efJE4+zsrIRUkF9u+dyRI0f0x2zZskWTKVMmfVm8uXPnanLmzKnK+umQcnOGpfcyAvfv31f3umvXLv1YiPCsWrVKf8yFCxfUMQcOHFBt+XKws7PTBAYGxispKGX+dOPxySefqC8kQ7p3764eAjIS8n8sJRc5LnGEhIRovLy8NL6+vvFKR9r6GE2cOFE9uCeGtY0N3dGpyLcrlWTE7apD0uRJWwqe2xLXr19XeZENxyJ79uzKPa8bC/kpLujq1avrj5HjZcykPJ/uGEmdKGX9dEg+ZXHtSg7ijILkNzYsVSi/J1FRUfHGR1xqRYoUiTc+ki9aV35Qd+/BwcGqmL3uGMNz6I7JKL9vks5S0m+GhoYqtzTHJQ5xqYrLNOF9cIygprVkGkzKXcp0lriXrXFsKMJGIjVd5UvF8D9XkHbCRP3Wju5+kxsL+SnzMQkLGIhQGR6T2DkMr2HpSKEBmc+rV6+evs6v9F0eLOQhJLnxedW9J3WMfKFIFSRLRWoQy3ydzLdJVaV169ahXLlyNj8uOuTBRMo5SmxBQmx9jGrVqqXmZyX3usQXyAO/xIyEhIRY3diwgAMhJrJozp49a9JSgxkdKThx8uRJ5SFYvXq1qn60a9cuc3fLIggICMCIESPg6+urghFJfKQalw4J8BNRlgIdK1eu1JfetBZoCRuJVI6RMmkJI/GknT9/ftgSuvtNbizkp9SgNUQiFCVi2vCYxM5heA1LRsr/SSUkKd1XqFAh/X7pu0xfSKGE5MbnVfee1DESdWzJX0hirUjEabVq1ZS1J1WhZs6cafPjonOpyt+FROaKZ0g2eUCRKlnyWiwyWx8jQ8TqlRKZUq7S2n5/KMKp+GKRLxWpo2roipS2zHfZEp6enuoX2XAsxJUjc726sZCf8sciXzo6pHC7jJk83eqOkaVQMs+jQywEsaSk1qylIrFqIsDiZpV7kvEwRH5PHB0d442PzHPL3Jbh+Ijb1vBBRe5dvgjEdas7xvAcumMy2u+b/J9LyUGOi7aMpNyfeAp0m8RNyNyn7rWtj5EhsqTx6tWraimk1f3+pGsYmBUtUZII4CVLlqjo34EDB6olSoaReNaCRG9KiL9s8usyffp09frmzZv6JUpy7xs2bNCcPn1a06FDh0SXKFWpUkVz6NAhzd69e1U0qOESJYl2lCVK77zzjlrCIuMrSwcsfYnS4MGD1fKsnTt3xltK8fz583hLKWTZ0o4dO9RSijp16qgt4VKKli1bqmVOsjwiT548iS6l+Pjjj1UU6Jw5cyx+mcmnn36qosSvX7+ufi+kLRHx//zzj02PS3IYRkfb+hiNHDlS/V3J78++ffvUUiNZYiQrEKxtbCjCqUTWlMkvgawXliVLsgbWGvnvv/+U+CbcfHx89MuUxo8fr0RUHkyaNWum1oUa8vDhQyW6WbNmVUsE+vbtq8TdEFljXL9+fXWOggULKnG3dBIbF9lk7bAOeRh5//331fIc+YPv1KmTEmpDbty4oWnTpo1aGy1fNPIFFBUV9dL/Q+XKldXvW/HixeNdwxLp16+fpmjRoqq/8uUnvxc6AbblcTFGhG15jLp3764pUKCA6rN8H0j7ypUrVjk2rKJECCGEmAnOCRNCCCFmgiJMCCGEmAmKMCGEEGImKMKEEEKImaAIE0IIIWaCIkwIIYSYCYrwayDZf6T4tPwkL8PxSRqOTfJwfJKH42M9Y8N1wq+BpGiU0n2SoF7SoZH4cHyShmOTPByf5OH4WM/Y0BImhBBCzARFmBBCCDETNldPWMronThxQpUKs7N7vWcQKTAt3L59W7lASHw4PknDsUkejk/ycHwse2ykYpiURaxSpYoqTZkcNjcnfOTIEdSsWdPc3SCEEGLlHD58GDVq1Ej2GJuzhMUC1g2O1KYkhBBCTMndu3eVsafTm+SwORHWuaBFgAsVKmTu7hBCCLFSUjLlycAsQgghxEyYVYR3796N9u3bw8PDA5kyZcL69etf+ZmdO3eiatWqcHZ2RsmSJbFkyZJ06SshhBBiVSIcGhoKb29vzJkzJ0XHX79+He3atUOTJk1w8uRJfPDBB3jvvfewbdu2NO8rIYQQYmrMOifcpk0btaWU+fPnw9PTE99//71qly1bFnv37sWMGTPQqlUrk/YtJiYGUVFRJj0nIZaAk5PTay/PI4SYhgwVmHXgwAE0b9483j4RX7GITYWs2AoMDMSTJ09Mdk5CLAkRYHmYFTEmlkl4VAyO3niMqJhYc3fF5sjj5owKBbOn2/UylAiLOCYM+Za2LMgOCwtD5syZX/qMJPE2TOStW8id3DVEgPPmzQtXV1c1V02ItSBJBO7cuaOWUBQpUoS/3xbIjov3MHHjOQQ8CjN3V2ySNyoVwOy3q6bb9TKUCKeGqVOn4vPPP0+xC1onwLly5UrzvhFiDvLkyaOEWLLHOTo6mrs75AW3Hj/H53+dh+/5e6qdO6sTPHK8bFiQtKWIuyvSkwwlwvnz51epwAyRtlTKSMwKFsaMGYOPPvpI35ZUZuXKlUv0WN0csFjAhFgrOje0PHRShM1PRHQMft5zHbN2XEZ4VCwc7DKhf31PDG/mhSzOGeormqSCDPU/XKdOHWzevDnePl9fX7U/KWQpk2w6UpJLlC46Ys3w99ty2HflAcZvOItrQaGqXcvTHV90rIBS+dzM3TViCyL87NkzXLlyJd4SJFl65O7uruarxIoVy/W3335T7w8aNAizZ8/GJ598gn79+mHHjh1YuXIlNm3aZMa7IIQQ47gXHI4v/j6Pv0/fVe3cWZ0xrl1ZdKiszZlAbAezrlM4evSoqjIhmyBuY3k9YcIE1ZbgEX9/f/3xEtEpgivWr6wvlqVKP//8s8mXJxEtxYoVww8//JDi4yWRinyBMLKckMSJjonFz3uuodn3u5QA22UC3q1bDNtHNkLHKgUpwDaIWS3hxo0bqyVBSZFYNiz5jJQiJHG86g934sSJmDRpUqoqTmXJkiXFx9etW1c9OGXPnn7h/YRkFI7ceITx68/iYqB2hUaVIjnwRYcK6bochlgeGWpOmCSOCJ+OFStWKE+Cn5+ffl/WrFn1r+WhRwJyXlXjUhdFa2zAjwTP2SKRkZFcd0sS5cGzCEzdfBFrjt9S7Zyujvi0TRl0rVYYdmIKE5uGaXOsABE+3SZWqFjGuvbFixfh5uaGLVu2oFq1aipITbKMXb16FR06dFDrrEWkpeblv//+m6w7Ws4r7v9OnTqpCHIvLy9s3LgxSXe0eDJy5Mih0opKdjO5TuvWreM9NMgymeHDh6vjZFnY6NGj4ePjg44dOyZ5vw8fPkTPnj1RsGBB1Y+KFSti2bJlL62H/eabb1R+cblniTGYMmWK/v1bt26pc0j8gVj71atXx6FDh9R777777kvXl4Qw4oXRIa+HDh2q9ufOnVs/JTJ9+nTVHzln4cKF8f7776vYB0P27dunPi99z5kzp/rs48ePVeyDjIHhunZB+vLOO+8kOR7EMomJ1eD3gzfR9LudegHuWbMwdoxsjO41ilCAiYIi/ArEcnweGW2WLTlXvbF8+umnmDZtGi5cuIBKlSopYWjbti22b9+u3PsijlJMw3AOPjFkzXW3bt1w+vRp9flevXrh0aNHSR7//PlzfPfdd/j9999VwQ45/6hRo/Tvf/311/jzzz+xePFiJU4Svf6qQh7h4eHqgULiA86ePYuBAwcqkZIa0TokqE/ud/z48Th//jyWLl2qT/Qi996oUSMV9CcPEadOnVLBfiLcxvDrr78q61f6LSlVddmofvzxR5w7d069L8GDcm4dEnjYrFkztUxOMsDJA5GMu3gnunbtqn4aPtjcv39f3acEIpKMw6mAJ+g0d59yPweHR6O8Rzasfb8upr5VCTmz0GNC4qA7+hWERcWg3ATzFIg4P7kVXJ1M8180efJktGjRQt8WC1CC23R88cUXWLdunRIAsfCSQqxEsSCFr776SgmOiJ+IeFJrr0WgSpQoodpybumLjlmzZinBFOtakOj3hMvQEiIWsKGQDxs2TFnbEikvhbQlK9rMmTPVucSqFuT69evXV69FkIOCgtSct4yDIBazsYgnQKxtQwxTqIon4csvv1RR/XPnzlX75HixunVtoXz58vrXb7/9tnogEUEW/vjjD2XFG1rhxHJ58jwS327zw9LD/pBnaDcXB4xqWRq9axeFPS1fkggUYRtBvvgNEWtQgrXEyhL3sLiFJfXnqyxhsaJ1iMtVEqWItZYU4nLVCbBQoEAB/fFPnz5VyVZEOHXY29srKzc5q1SsRXkAENEVa1bmY8WFq0uyIta+tMXiTAyxRiUKXyfAqUX6mRBx6UuWNpkGEKtexlUsd/EISP/k2jqBTYwBAwaoqQG5L3nYEJe+PPgwatayiY3VYPXxW5i25SIehUaqfW9VKYgxbcuqXMSEJAVF+BVkdrRXFqm5rm0qEkY5iyUpS73EVSxWoGQc69KlixK05EiYYUnEITnBTOz413Wzf/vtt8rSlflq3fyrWKC6vieVPU3Hq94Xl3LCPiZWUSvhmN64cQNvvPEGBg8erOafReTF3dy/f3/VNxHhV11bHg7EQyHzwy1btlRuba6Dt2zO3wlWCTeO3Xys2qXyZVVRz7WKM/UteTUU4VcgomEql7AlIfOYYmHp3MBiGYuIpCcSRCbztOIWbtiwod7KPX78OCpXrpxs3yWorHfv3qotDwGXLl3SpyMVN7GIncx3S73pxKx5CTCTuezErGGJCpe5ZkPEgn1Visdjx46pvsj6dV2pQLHWE15b+pVcPnPpszxgiDUsVcMkwItYHiHhUZjhexm/HrihgrBcnezxQXMv9K3nCUf71wy3kQfbx9eBmETKqWYvCDi/yKgV9gQICQScXIEcReKOCboEaIyswOSWD8icU/s64hnw9Bbg4Ay4e8Yd8/Bq4n1Kjix5gCwvHkiiwoDHNwE7ByC3wRTQ4xtAVLhx55W+Sp8F6ZP0TTxGeUrHHfMkAIjUZiNLES7ZgWwFkJ5Yn7qQFCFCtXbtWhUUJA8aEsBkbGCSKZD5XHHfijVepkwZNUcskcLJuV+l76tXr8b+/ftVdLFEJItbWyfCLi4uKspaAqIkcKpevXpqDlisSrFKZU5b3NkSdSzXFhe5BKd5eHioFKhNmzZV1rZYo9KWeVkRZV1SmaSQexCLWe5BxtUwYEuHzH+L9S5R0zJXLP3777//lItaoqx188LiqVi4cKE+WxyxHMRLsvHUHUzZdAH3Q7SR7O0qFsC4N8qiQPbXLLggQnRmJbB/NvAgbplhPHouB0q/qMN+aSuw7n9AiWbAO2vjjlnYBIiMH5X/St6cBVTto33tfxD4szNQwBv43+64Y/54SyuYxtBsItDgRf7+oIvAgsZAtoLAR+fjjlndH7h91Ljz1hkKtHqx4uHZPWBuLcDeGRhvMD22eZR2jFJKld5AhzlITyjCNooIl0TcSoIN+fIX0UpJXm1TI9eV8pF9+vRR88ES6SxLduR1UowbNw7Xrl1Tx4mLVz4jgipzzDrkoULWQsuaaakYJEIroieI8P3zzz8YOXKkivCWeVsR8DlztH98cl75vIi4zOfKOEn/zpw5k+y9iBtZxlUivkVsxboXkZfP6ihVqpS69tixY9VcuFjstWrV0ge76TwEnTt3Vm7o5JZqkfTnyv0QTNhwDvuvPlRtz9xZ8Pmb5dGwlHFr6l/i+SPg6C/AoQVA6AsREUFxjlvjr8fewCNj7wS45gJcssU/JrO71oo1BgcXg/M6vDhv9petz4jky8G+hKPBg4ndi/PqLG4dch3ZbwyOBoV2MtlpPy9jZoh4DIw5r1Mi453GZNKYch1MBkDWh4p7LyAgAIUKFYr3nnzhSv5qSY8p1hRJf8QalzXFsgxKIrZtFQkqk6hpiT43Nfw9Nx5ZMjhrxxWVcjIqRgNnBzsMbVISAxsVh7PDa8RuiFV5YC5w4ncg6rl2X7ZCQO3BWqs0obiSDK8zCaElTMzKzZs3lWUo63YlolmWFYlAiEvWFhFXvCQ9kc1wGRMxD2KjbDt3TxVbuP0kTO1rXjYvJrYvj8KmqDsrbucjC7Wv81UE6g0HyneKb+0Sq4YiTMyKBDDJMhyZA5UvvAoVKqhlPmIN2yIy7yxCLC7t0qUNAkxIunPzYSgmbjyHnX5Bql0wR2ZMerM8WpR7EQxkLBJzccVXOx+av4J2X533tQFYMr9ZvLE2sIjYFBRhYlbEZSMBTERLekeok5cJj4rB/F1XMXfnVURGx8LRPhP+17AEhjQpicxOr+F63vEFsHc6UPZNoPvv2n3uxYHea0zWd5LxoAgTQsgL/vO7j0kbz+HmQ+38bP2SufF5h/IokSdr6oKtoiPilrxU6gYc+UUrvBKKQ6uXUIQJIQS48yQMk/86j63nAlU7XzZnjH+jnFp6ZHS2Mgm2OjgPOP47ULY98NZP2v15ywKj/OJHCxObhyJMCLFZxN38y97r+HH7ZZUnXvI796tXDCOal0JWZyO/Hm8fB/bPAs6vj0uUIWt9Y6K1S34ECjBJAEWYEGKT7L/6QK35vXJfm9SiZjF3TO5YHmXyZzM+2ErE98aeuP0lmgJ1hzPYirwSijAhxKa4HxyOKZsvYMPJO6qdO6sTxrQpi7eqFky561nmek+vBA7M1maB0iWiqNAFqDssLvqZkFdAESaE2ATRMbH47cBNzPC9hJCIaGWgvlO7KEa2LI3smVO4LjfsMXB0EXDoJ22qRME5G1DtXaDWIG1eZ0KM4DWzjBNrQmrWJqyHK4UEkkMsh/Xr17/2tU11HkISQyoctZ+9D5P/Pq8E2LtwDmwcUh+TO1RIuQALawcC2ydrBVjW+7b8EvjwLNDyCwowSRW0hK0AKRYghQO2bn05UfmePXtUDuNTp07FqwWcEqS6UcJyfa+L1DAWsZWqRIZITWMpxkCIKXn4LAJfb72IlUdvqbYI7ujWZdCjRmHY2aXA9XznBJC9MJBFW1wDNQYAwXe1LucKbzGzFXltKMJWgFQGkoT/kq80YZ7SxYsXo3r16kYLsK6kX3qRP39+2CJSZ1gKShDTEhurwbIj/vhmqx+ehmlL73WvXhij25SBe5YUjvfmT4DDPwGNRgNNxmr3ebXQbgy2IiaC7mgrQArJi2BK+kdDpEbwqlWrlEg/fPhQVeopWLCgqjwk5fSWLVuW7HkTuqMvX76srGpJ+i9Vh3x9fROtiiSVguQaxYsXV9WIxEoXpH9SR1escnE/y6brc0J3tFQskpKCUmUoV65cqlKS3I8OqYUsFYa+++47VSFJjhkyZIj+Wolx9epVVYdYahhnzZoVNWrUUCkyDZH81XIPksnL2dlZlSf85Zdf9O9LOUQZ72zZssHNzQ0NGjRQ503MnS9IH6WvhmMqhSmkspKcQ+7rVeOm46+//lJ9lvGXyle6WtCTJ09W6T4TIjWZ5Ty2xplbT9Fp3n58tu6sEuCyBbJhzeA6+LpLpeQFWIKtIl8UURCK1tEGW4XHVedS4ksBJiaElnBKMaYwtA4pq6VbHyhrBWMitCW3DNcKJnVep5S7gaVkn3ypi6B99tln+ghPEeCYmBglviJg1apVU1/28uUvZfLeeecdlChRQpXUS0l1o7feeksJ2KFDh1TZwISCI4gwST+kNq8I6YABA9Q+KQvYvXt3VZdX3OY68ZOyfQkJDQ1V5QSllq+4xO/fv68K3Q8dOjTeg4bU4RUBlp9XrlxR5xfhkWsmhoyBlC6cMmWKElip1SuufD8/PxQpoi2ILuN44MABVb1IShNKMYkHDx6o927fvq0eQkRsd+zYocZRUm5KKURjkAcHKbE4ceLEFI2bIP9fIrry/yv9Fgt68+bN6j0ptSgPNzJWItKC1Ec+ffq0qhltKzx9HoXv/vHDH4duqoRUss53ZMtSKvjKwd4uZcFWUr2o/ofa/ZJecsRpzvWStEVjYwQEBEjpRvUzIWFhYZrz58+rny8xMZvx29m1cZ+X17JvUdv45/3aM/HPGsmFCxfUff3333/6fQ0aNND07t07yc+0a9dOM3LkSH27UaNGmhEjRujbRYsW1cyYMUO93rZtm8bBwUFz+/Zt/ftbtmxR11y3bl2S1/j222811apV07cnTpyo8fb2fuk4w/MsWLBAkzNnTs2zZ8/072/atEljZ2enCQwMVG0fHx/Vv+joaP0xXbt21XTv3l1jDOXLl9fMmjVLvfbz81P98PX1TfTYMWPGaDw9PTWRkZGJvp9w/IQOHTqovuqQPnfs2PGV/Uo4bnXq1NH06tUryePbtGmjGTx4sL49bNgwTePGjRM9Ntnf8wxIbGysZvXRAE3Vyf9oio7+W23Dlx3X3Hv6ivt7dEOj2Txao/myQNzf3U+N5YTp1XVigzqTEFrCVkKZMmVQt25dLFq0SFlqYhlKUJa4KgWxiL/66iusXLlSWXRiSYnrVdyfKeHChQvKRSuWmg6xVBOyYsUKZUWKi1YsT7ESxWI0BrmWWKGGQWH16tVT1rhYrWKNC1Jv194+LqG+WMViRSaF9EcCw8SqlEAw6VtYWBj8/f3V+xIsJueTsoqJIe+L+9nR8fWCcWSO3thxk2snZeEL8p5YxNOnT1eVqZYuXYoZM2bA2rkYGIwJ68/h8I1Hql0yb1ZM7lAedUu8CKRKKthKkmuck8xWMdp9ecu/KCP4Ft3NJF2hCKeUsdqF/Ua7o3WUaa89h7ijDfkgadEwFpn7HTZsGObMmaMCssTVrBOUb7/9FjNnzlRzvDIfLAIn7mQRY1MhbtxevXop16i4k8XVvHz5cnz//fdICxKKobjhRaiTQsolyjy2uINlrlfmm7t06aIfA2knx6veF/HTGvVxJDZHnTDiPCXj9qpri1tdXOzr1q1TgV5yXbk3a+VZRDR+8L2ExftvICZWg8yO9hjR3Av96nnCycEuicxW/wL7f4yf2UoyWklmK8lwRfElZoAinFKMmKNNFJkb1s0Pm/K8BnTr1g0jRoxQVpDMGw4ePFg/PyxzlxKU1Lt3b9UWsbp06ZIKsEoJUt83ICBAWZBicQoHDx6Md8z+/ftRtGhRNW+p4+bNm/GOEYEQq/xV15L5UZkb1gmW9F9E7nVq7Mo5JEhKF9AkFqdh6UB5OJFx2bVrF5o3b/7S5yXC/Ndff1UCl5g1LMFxMj465D5lDrxJkybJ9isl4ybX3r59O/r27ZtkXICPj496+JIx7tGjxyuFOyMiDzmbztzFF3+fx73gCLWvdfn8GN++nKr3m2iw1ZlVWstXl9kqkz1QobN2mVEB41cNEGJKGB1tRUjErwQnjRkzRomBYVSul5eXsgLlC1/cvf/73/9w796LjD8pQERJonfli16im8XVbSgaumuIa1esOHGrintVLDNDJDpYgp3EvSoBT+IST4hYhRIBLNcSEZPAK7HwJZBM54pODdI/CVSSa8s9vP322/EsZ+mbXFPcuhKpLf3cuXOncuELEhgWHBysBO7o0aMqWvz3339XLnJBornF1S3bxYsX1UPQkydPUtSvV42bBHFJNLv8lP8/cbt//fXX8Y6R4DUJGJPAN7kHa+Nq0DO888thDF16Qglw0VyuWNK3Bua/Uy1xARYWtQY2DNEKsFNWoM5QYMQpoPNCCjCxCCjCVoa4pB8/fqzcmobzt+PGjUPVqlXVfpkzlnW5snwmpYgVKsIgc6gSTS1f+BJlbMibb76JDz/8UImVRCmL4CdcIiPrmVu3bq2sQ7EcE1smJfPU27Ztw6NHj1S0r7hVmzVrhtmzZ+N1kPlSSQgic+fivpWxkDExZN68eep677//vppnl7lWscgFWQYlIicWtLj5Jdp84cKFeqtYhE9EXCKs5X1ZavQqKzil4yb/ZxLtvnHjRnWMCP7hw4dfEnO5N+l3rVq1YC2ERcbgu21+aP3Dbuy98kC5mz9o7oVtHzRE49J54x/8xF+7EkFH+Y6AWwGgxWTgw3NAqylAjsLpfg+EJEUmic6CDSEJLSTASFyrCRNbhIeHK+vH09NTWWKEZCTkT1mEWB4gPvrooySPy0i/577n72HSxnO4/SRMtZuUzoNJb5ZH0VyJTONs/hg48ovWyhV3sxAVpnU/OzAhCrEMnUkI54QJsQKCgoKUOzswMDDJeeOMRMCj50p8t1+8r9ribp7QvhxalssXV+lIZz/o2q65tNHOAYfjRJj1e4mFQxEmxArImzevyqK1YMGCDJ2DOyI6Bj/tuoY5/11BRHQsHO0z4b0GxTGsaUm4Or34uoqO1AZbSRnBZhOB0q21+2sOBEq3AQp4m/UeCDEGijAhVoA1zCrtvhSEiRvP4foD7Rx83RK5VJUjWfurCHsCHFuszWwV8iIK/cjCOBF2ddduhGQgKMKEELNy92mYWnK0+Uygaud1c8a4N8qhfaUCWtezBFsdnA8c/xWIfJE/XIKtpH6v1PElJANDESaEmIWomFgs3ncdP/x7Gc8jY2Bvlwk+dYrhwxZecHNxBO6e0q7vPbs2fmYrVUawM4OtiFVAEU6E5LIuEZLRsQTX9cFrDzFhw1lcuqe1bKsXzalcz+UKuAFXtmszW13flSCz1TCgRDNmtiJWBUXYAMk0JOth79y5o9awSlsfiUmIlQiwRFLL7/Xr5sBODfdDwjF180WsO3FbtaW04Jg2ZdC5aiHYibW7oDFw92SCzFZDGWxFrBaKsAEiwLJ2UrJNiRATYo2IAMvaRcPiF2mN5Hf+4+BNlXQjJCJaGbNv1yyCj5sUQo4cumhuByBvOeDhFe1cr8z5MrEGsXIowgkQ61dqy0oVm1flOCYkIyIWcHoK8HH/xxi//izO3QlW7YoFs+PLDuXhffF7YO4SoN9WIH8F7cHNJwKtpwKZc6Rb/wgxJxThRNC56szhriPEWngcGolvtl3EssMBqp3NxQEfty6jLGAJwsJBfyAyBDi7Ok6E3fKbt9OEpDMUYUKISYmN1WDl0QB8vfUiHj+XUo4ajC19F+/iLzh5zQBEgIVGnwJV+gAlm5m7y4SYDYowIcRknL39FOM3nMUJ/ydwRDSGuh/H+05b4HpTW2kKB+YAb0zXvs5XTrsRYsNQhAkhr01weBSm/3MJvx24gayaUAxz+g+DMvsiy/Mg4LkEW2QFqvoAtQebu6uEWBQUYULIay15Wn/yNqZsuginZ7cxxmErejvtRObY50BEgsxWDLYi5CUowoSQVHHpXoiKen524zg+c9iEN10OwB6xkH9qqZHKbNWFma0ISQY7mJk5c+agWLFiqq6pFCJPWKjckKioKEyePBklSpRQx3t7e2Pr1q3p2l9CbJ3QiGhM3XwB3WZuw7BbI7HJeSw62e/TCrBnI6DXGmDwfqDy2xRgQizZEl6xYoUqPj5//nwlwD/88ANatWoFPz8/VZotIePGjcMff/yBhQsXokyZMti2bRs6deqE/fv3o0qVKma5B0JsyfW85cxdfLHpAu4+DQfggkJu0dBE2iNThbeAOkMBj8rm7iYhGYpMGjMmkhXhrVGjBmbPnq3P2Vy4cGEMGzYMn3766UvHe3h44LPPPsOQIUP0+zp37ozMmTMrcU4Jt27dUtcICAhQWYMIIa/m+r3HOLj0S1R/vAWdIychu3tufP5meTTNdldbPjBHEXN3kRCLwRidMdoSFtdxv3798O6776rMUqklMjISx44dw5gxY+KljWzevDkOHDiQ6GciIiKUG9oQEeC9e/cmeR35jGw6QkJCUt1nQmyK6EjceRaDRXuvq6jnv+y3wsvuNmaWPY86b4+Hi6Nk3cpn7l4SYltzwh988AHWrl2L4sWLo0WLFli+fHk8kUspDx48UGkh8+WL/0cs7cBAbV3RhIirevr06bh8+bKymn19fVVfJNdzUkydOhXZs2fXb+XKcV0iIUkSfBc4uhghi95C+FdF8cY3f+HnvdcRGaPBprwDEdRsBpr0GvNCgAkhZhHhkydPqgCqsmXLKtdxgQIFMHToUBw/fhxpycyZM+Hl5aXmgyXHs1yzb9++yoJOCrG0nz59qt/Onz+fpn0kJEMhs1GBZ4Bd30CzoAkwvQzw9wdw898Ol9jnqIlzqFM8Fxb3rYEPhwxHngb9AAdnc/eaEKsh1YFZVatWVdv333+PuXPnYvTo0Zg3bx4qVqyI4cOHK3FMrgxg7ty5VRL5e/fuxdsv7fz5E88fK+UF169fj/DwcDx8+FDNEcvcsVjlSeHs7Kw2HcHB2iTyhNgs0RHAjb2A3xbtFnxL7db9tZ6MLYHtsdUQUbI1hjRrhoqFub6XEIsTYVkutG7dOixevFi5hWvXro3+/furCemxY8fi33//xdKlS5P8vFiy1apVw/bt29GxY0e1T1zM0hYLNzlkXrhgwYKqD2vWrEG3bt1SexuE2A7n1mm3K9uByGf63eFwwp6Yivg3tioO2FdD8xre6FuvGAq7u5q1u4TYAkaLsLicRXiXLVum3MB9+vTBjBkzlItYhywbkqjnVyHLk3x8fFC9enXUrFlTLVEKDQ1VVrQg5xaxlXld4dChQ7h9+zYqV66sfk6aNEkJ9yeffGLsbRBi/Ty6BrgbeInOrAYu/q1ehjjmxtZIb2yJqoL9seXh5pZNCe/YmkWR3ZXVwwixWBEWcZWALHE9iwWbWLk/T09P9OjR45Xn6t69O4KCgjBhwgQVjCXiKsk3dMFa/v7+8eZ7xQ0ta4WvXbuGrFmzom3btvj999+RIwfdZYToiYkG5tcHgi4AQ48BuUuq3f5FO+NikDvmBZbCyfBi0MAOXnmzYnLD4uhQ2QPODgy2IsTi1wnfvHkTRYsWRUaF64SJVREeDFz5F7h/AWj6Wdz+X98Ebu6H5q2F2ONUHwv3XMOeyw/0b0uw1cCGxdGoVB7Y6UoLEkIsf53w/fv3ldUqiTYMEVexBFqJa5kQkoY88Qf8tgJ+m7UBVrFRL9xU/QE3bVBjZJsZ2Ho9CnP/vY+LgdpUsPZ2mdC2YgEMaOCJSoXoPSLEEjBahCVblczBJhRhmaP9+uuvlRgTQkxIbCxw5wRw6UU0872z8d/PVRIo3UYtN5KSgssP+2PR3hsIDJbUkoCrkz261yiMfvU8GWxFSEYXYVlnK0uTEiK5m7kGlxATEfkcuL5LK7qXtgLPDJbyZbIDitQBSrXWim9uL9x5EoYle29g6aHTeBYRrQ7L4+aMd+sWQ+9aDLYixGpEWNbcylrehGtzJWuVgwMrIxLy2kiYxuwa+vW7Cic3oGQzoHRbwKuFNl+zPBTfCcbCFSfx16k7iI7VhndIsNUABlsRkiEwWjVbtmypslBt2LBBpYEUnjx5otYGS9Q0IcTIwKrDPwG3jgE9lwGS4Ea2YvWBm/u0lq5sRevrywJKLOXey0FYsDt+sFXt4u74X8MSDLYixJpF+LvvvkPDhg1VhLSufKCksZRlRbJciBCSDNGRWgtXt37X3gnYMx2Ieg4EngYKeGv3t/secMqiFeQXRMXEKotXxPdioLYQiWhtu0oeDLYixFZEWJJnnD59Gn/++SdOnTqlqhhJco2ePXsmumaYEJvn+SPgsq82sEqyVWXzAIa8CGB0dAEajgJccwHZC8d9xjmr/mVIeBSWHfbH4n03XtTxZbAVIdZCqiZxs2TJgoEDB5q+N4RYCw+uxEUz+x8ENDFx7z130Qrzi3ldNBiZ6CnuPg1TwrvskD9CEgRb9apVBDlcte5pQkjGJdWRVBIJLRmtpC6wIW+++aYp+kVIxstSdetwXFGEh5fjv5+3/Iv53baARxUpnp3kqSTY6uc917DRINiqZN6sGNigODpUYbAVITYtwpIyUnJDnzlzRlVJ0iXc0lVMkhrBhNgMESHAplHA5X+AsEdx++0ctcFVIryylChn8lnmVLDVlQcvBVvV8nTH/xoVR+NSeRlsRYgVYrQIjxgxQuWGlmpH8lPqCktZwZEjR6qgLUKsmicBwMMrQIkm2rZTVuDGHq0Au+QASrXSCm+JZoBLtleeToKt/j4twVbXceGutsymaK02s1VxeLOMICFWjdEifODAAezYsUPVA5biCrLVr19fVTqSOsInTpxIm54SYm5kGdHPTYHM7sDHVwA7e230cutp2sCqwrUA+5T9SUmw1fLDAVi077o+2CqzozbYqn99BlsRYisYLcLibnZzc1OvRYjv3LmD0qVLqyVLfn5+adFHQtKXqDDg2i5tYJVbAaDxp9r9snzINbfKUIXQIH2eZpRLeRyEBFst2SeZreKCrXJndVZlBBlsRYjtYbQIV6hQQS1NEle05I/+5ptv4OTkhAULFryURYuQDMOz+9r0kBJUdfU/IDpMu1+WDTUarbV4xcr94AzgZLyVKq5mqWS08WRcsFWJPFlUJaMOlQvCxZHBVoTYIkaLsNTzDQ0NVa8nT56MN954Aw0aNECuXLmwYsWKtOgjIaZHAgrvn4+LZr59THbGvZ+tUFy2KjlWlzTDCAGWYKt9Vx5iwZ5r2H0pKF6wlYhvk9IMtiLE1jFahFu1aqV/XbJkSVy8eBGPHj1Czpw59RHShFhstipJBaks3s3akoCGyNIhWUIkwpuvQrxsVcYgwVabTt9Vkc7nDYKt2lQsoJYZMdiKEJIqEY6KilIZsiRNpbildbi7v0g6QIgllgHUrcl9GgD83jHuPQcXwLNR3DKibAVe61ISbLXiSAAW7b2OOwy2IoSYWoQlLWWRIkW4FphYPgGHge2TgSy5ga5LtPtyldAWQnAvprV4izfW5md+TQKfhmPxvusMtiKEpL07+rPPPlMVk6RYAy1gYhHExgC3jmjX7OZ/4aGxd9Su33XMonVDv6hAhL6bTHbZi4HByuXMYCtCSLqJ8OzZs3HlyhV4eHioZUmSR9qQ48ePp7ozhBiVqerqDsBvK3B5G/D8IeD9NtBpnvb9ApWBdtO1NXh1AmwCGGxFCDGrCHfsaDCnRkh6c34DcPw34PpuIMYgb7lLdsBZu35dIUFVNfqb7LLJBVtJZqvKDLYihKSHCE+cODE11yHk9RNobP4YOGFQszqnZ1w0c5HaWhe0iUku2ErKCBbJxWArQogZqigRkq5lAVf5APfOiokL1B0GVOkN5C6V6mVEKQq22v8i2Co8Ltjq3bpF0atWUeTMwmArQogZRFhyRSe3HpiR08SknF0LbBwORIYAWfIAnX/WRjWnERJstXD3dWw8dRtRMXHBVuJy7liFwVaEEDOL8Lp1615aOyxFG3799Vd8/vnnpuwbsXUOzge2jta+LloP6PzLa6/lTSrYav/Vh2q+d5dBsFVNCbZqUBxNyzDYihBiISLcoUOHl/Z16dIF5cuXV2kr+/c3XTAMsXHKvgHs/gao2gdoMi7FFYqMCbbafEYbbHXujkGwVYUCeK+BJ6oUyWnS6xFCSEJM9q1Wu3ZtDBw40FSnI7ZKkB+Qp7T2dfZCwNCjgKtp16M/i4jG8sP+WLzvBm4/CdMHW3WrXgj96nuiaK7XT+BBCCHpJsJhYWH48ccfUbBgQVOcjtgiUiTBdwKwfxbQYylQpq12vwkF+F5wuKrfGz/Yygk+dYqhd20GWxFCMoAIJyzUIPNpISEhcHV1xR9//GHq/hFbQX6nYkUYNcCdE3EibAL8AkNUGcENJ+OCrYq/CLbqxGArQkhGEuEZM2bEE2GJls6TJ4+qLSwCTYhRxETHzfU2/xzwagGUaPrap5WHwwNXH+KnhMFWxbSZrRhsRQjJkCL87rvvpk1PiO3le945Fbh5AOizQSvEkl7yNQVYF2wllu/Z23HBVq0r5FeWL4OtCCEZWoQXL16MrFmzomvXrvH2r1q1Cs+fP4ePj48p+0eskZB7wJr+2gILwqUtQNn2Jg+2cnG0Q/fqhRlsRQixHhGeOnUqfvrpp5f2582bV0VHU4RJskjO59X9gdD72gpH7We+lgBLsJUI75+HbjLYihBi/SLs7+8PT0/Pl/ZLRSV5j5BEiY0F9n4P/PcVoIkF8pQFuv0G5CmVqtMx2IoQYpMiLBbv6dOnUaxYsXj7T506hVy5cpmyb8RaCH0IrB0AXN2ubVfuBbT9DnAyvvjBsZuPMWvHZez0ix9sNaBhcTRjsBUhxNpFuGfPnhg+fDjc3NzQsGFDtW/Xrl0YMWIEevTokRZ9JBkZ/0PA6r5A8G3AITPQ7jtt8QUjiY3VYO7OK5juewmxGgZbEUJsVIS/+OIL3LhxA82aNYODg/bjsbGx6NOnD7766qu06CPJqMk3DswG/p2kXf+bywvo9iuQr7zRp3oUGokPVpzE7hdLjTpW9sCHLUox2IoQYnsi7OTkpHJEf/nllzh58iQyZ86MihUrqjlhQhRhT4D17wN+m7TtCp21AVjObkaf6uiNRxi69AQCg8NVtPPkDhXQrXph0/eZEEIyUtpKLy8vtRHyEnb2wAM/wN4JaD0NqN7P6Lq/kmzj5z3XMW3rRcTEalTQ1dxeVVEmf7Y06zYhhFi8CHfu3Bk1a9bE6NEvSsy94JtvvsGRI0fUemFio+5nQcRWLN5uvwMxkYBHZaNP9fR5FEatPgXf8/dU+01vD3z1VkVkdTZtFSVCCDE3dsZ+YPfu3Wjb9uW8vm3atFHvERskPFgbfHVwXty+fOVSJcCnbz1Bu1l7lAA72dvhy44VMLNHZQowIcQqMfqb7dmzZ2peOCGOjo4IDtamCSQ2xoW/gHPrAL+tQKVuQJbcRp9C3M+/H7yJL/++gMiYWBRxd1Xu5woFs6dJlwkhJENawhKEJYFZCVm+fDnKlStnqn6RjETlt4FagwGfjakS4JDwKAxddgITNpxTAtyqfD78Naw+BZgQYvUYbQmPHz8eb731Fq5evYqmTbXJ9rdv346lS5di9erVadFHYmlEhgI7pwENRwEu2bXzwG2mpepU5+8EY8jS47j+IBQOdpkwpm1Z9KtXLF6lLkIIsVaMFuH27dtj/fr1ak2wiK4sUfL29saOHTvg7m66AuzEQgnyA1b6AEEXgCf+2rW/qUDczyuPBijrNyI6Fh7ZXTC7V1VUZeINQogNYbQ7WmjXrh327duH0NBQXLt2Dd26dcOoUaOUGBvLnDlzVApMFxcXVZP48OHDyR7/ww8/oHTp0kr8CxcujA8//BDh4eGpuQ1iLKdXAguaaAU4az6g5oBUneZ5ZDRGrjqF0WvOKAFuUjoPNg1vQAEmhNgcqQ45lUjoX375BWvWrIGHh4dyUYugGoPMLX/00UeYP3++EmAR2FatWsHPz0/lqE6IuLw//fRTLFq0CHXr1sWlS5dUfWNxXU6fPj21t0JeRVQ4sHU0cGyJtu3ZCOj8M5D15f+jV3HlfggG/3Ecl+8/U6knR7UqjUENSzDnMyHEJjFKhAMDA7FkyRIlvhIJLRZwRESEck+nJihLhHPAgAHo27evaosYb9q0SYmsiG1C9u/fj3r16uHtt99WbbGgJZf1oUOHjL42SSEPrwKrfIDAM7IIGGj0CdBotDYhh5GsP3EbY9edwfPIGOR1c8aPPaugdnEW/SCE2C52xswFixtYKiiJxXrnzh3MmjUr1ReOjIzEsWPH0Lx587jO2Nmp9oEDBxL9jFi/8hmdy1pc4Zs3b0503TIxAec3AAsaawXYNRfQew3QZKzRAhweFYMxa8+o/M8iwPVK5lLuZwowIcTWSbElvGXLFlU9afDgwSZJV/ngwQPExMQgX7588fZL++LFi4l+Rixg+Vz9+vVVYE90dDQGDRqEsWPHJnkdsdRl0xESEvLafbd6oiMB3wnAoRfJN4rUAbosArJ5GH2qGw9C8f6fx3H+brAKoh7e1AvDm3nBnu5nQghJuSW8d+9eJWDVqlVT87ezZ89Wgpie7Ny5U0Vlz507F8ePH8fatWuV+1oqOyXF1KlTkT17dv3GtcyvQCKeF7eOE+B6IwCfv1IlwFvO3MUbs/YqAc6VxQm/9aupqh9RgAkhREsmjZiURiAR0RJQJfO24hYWa1bmdvv166dqDBvjjnZ1dVXLnDp27Kjf7+PjgydPnmDDhg0vfaZBgwaoXbs2vv32W/2+P/74AwMHDlSZvMSd/SpL+Pbt20qIAwICUKhQIWNu3TZY8Q5wYSPgkgPoNB8o3cboU0RGx2LqlgtYvO+GatcolhOzelZF/uwuadBhQgixLG7duqVW76REZ4xeopQlSxYluGIZnzlzBiNHjsS0adNUNPObb76Z4vNI6kuxqiXRhw6pSyztOnXqJPqZ58+fvyS09vba+cmkniWcnZ2RLVs2/WbMg4JN0vY7oHRb4H+7UyXAtx4/R9efDugFeFCjElg2oDYFmBBCTLVOWIcEakn1JFH9ZcuWGf15WZ60cOFC/Prrr7hw4YKabxZLWxct3adPH4wZMyZecNi8efNUiszr16/D19dXZfCS/ToxJkYSfBc49FNc2y0f0HMZkNP4+tDbL9xDux/34lTAE2TP7IhffKrj0zZl4GD/Wr9mhBBitZikNI0IoLiUDd3KKaF79+4ICgrChAkT1PKnypUrY+vWrfpgLX9//3iW77hx49SaYPkpbuU8efIoAZ4yZYopbsP2CH8K/NQQCL2vjX6u2CVVp4mOicW3//jhp13XVNu7cA7MebsKCuV0NXGHCSHExueEbclXbxNs/wK4tE2bfjJXCaM/Hvg0HMOXncDhG49Uu2+9YhjTpiycHGj9EkJsk1tG6AyLtNoaz4KA6HAgR2Ftu/EYbSEGx8xGn2rP5SB8sPwkHoZGqnq/33SphLYVC5i+z4QQYqVQhG2JG/uA1f0At/xA/38AB2fA3kG7GUFMrAYzt1/GrB2XIX6UcgWyqdq/xXJnSbOuE0KINUIRtgViY4H9M7WuZ02MtvxgaBCQ3Xh3fFBIBD5YcQL7rjxU7Z41i2Bi+3JwcWRgHCGEGAtF2Np5/ghY9z/g8j/adqUewBvTASfjrdZD1x5i2LITuB8SAVcne3zVqSI6Vilo+j4TQoiNQBG2ZgKOAKveBYJvAQ4uQJtvgKp9oPJHGkFsrAbzd1/Fd9v8EKsBvPJmxbzeVVEyL9dcE0LI60ARtkZkovbgPMB3PBAbDbgXB7r9BuSvaPSpHodG4qOVJ/GfX5Bqv1WlIL7sVAGuTvzVIYSQ14XfpNZG2BNgwxDg4t/adrmOwJuzAJdsRp/quP9jDP3zOO48DYezgx0mdyiPbtULq7XahBBCXh+KsDVx56S29u/jG4CdI9DqK6DmAKPdz7J0fNG+G5i6+QKiYzXwzJ0Fc96uinIexgs5IYSQpKEIWwvX9wB/dAZiIoDsRYBuS4CC1Yw+zdOwKHyy+hS2nbun2u0qFsC0zhXh5uKYBp0mhBDbhiJsLRSqDuT2ArIXBjrOBVzdjT7F2dtPVe1f/0fP4WifCePfKId3ahel+5kQQtIIinBG5tE1IEcxQPJrS8YrqfubOWeq3M9/HvLH5L/OIzImFoVyZlbuZ8kBTQghJO1ggt+MyqkVwNy6wJ7v4/aJ9WukAD+LiMaI5Scxbv1ZJcDNy+bDpmENKMCEEJIO0BLOqMjSo+gwIOCQNiNWgjrLKeFiYLByP18LCoW9XSZ82roM3mvgSfczIYSkExThjERsDGD3Ij1klV5a13OpVqkS4FVHAzB+w1mER8UifzYXzH67CqoXM34emRBCSOqhOzqjcHYNMLcOEKrN2awo0zZOlFNIWGQMPl51Ch+vPq0EuGGpPNg0vD4FmBBCzAAtYUsnOgLYNhY48rO2fXAO0GxCqk51NegZhvx5HBcDQ2CXCfioRSm837gk7KRBCCEk3aEIWzKPrmtzP989qW03GKWt/5sKNp66gzFrTiM0Mga5szrjx56VUbdEbtP2lxBCiFFQhC2VC38D698HIp4Cmd2BtxYAXi2MPk14VAy+3HQefxz0V+3axd3xY88qyOvmkgadJoQQYgwUYUsjJgr4dxJwYLa2Xagm0HVxqmr/+j98jveXHsPZ28GqPaxpSYxo5gUHe4YCEEKIJUARtiSe3gJW9QVuHda26wwFmk8C7I1PGbntXCBGrTqFkPBo5HR1xIzuldG4dF7T95kQQkiqoQhbCpd9gbUDgbBHgHN2berJsm8YfZqomFh8veUift57XbWrFc2JWT2rwCNH5jToNCGEkNeBImwJa3//mxKX+apAZaDrEsDd0+hT3X4ShqFLj+OE/xPVHtDAE5+0LgNHup8JIcQioQibnUzAvXPalzXe05YfdHA2+iz/+d3HhytO4snzKGRzccB3Xb3Rsnx+03eXEEKIyaAImwuNRpvnWbJddZwH3NgDlOtg9GmiY2Ix499LmPPfVdWuVCi7Kr5Q2N01DTpNCCHElFCE0xvJ8yyu58c3gA6ztUIshRdSIcD3g8MxbNkJHLr+SLX71CmKz9qVhbODcVm0CCGEmAeKcHpz7wyw8ytAEwtU7gkUq5+q0+y/8gDDl5/Ag2eRyOJkj2mdK6G9t4fJu0sIISTtoAinNwW8gRZfaIsvpEKAY2M1mP3fFeWCFo92mfxumNurKornyZom3SWEEJJ2UITTGlHKA3MAr5ZAnlLafXWHpupUD59F4IMVJ7Hn8gPV7l69MD7vUB4ujnQ/E0JIRoQinJaEPQbWDQYubQFO/AEM3Ak4pi5d5JEbjzBs6QkEBofDxdEOX3asiC7VjM+iRQghxHKgCKcVt49piy888QfsnYBaA1O19Ejczwv3XMM32/wQE6tBiTxZMLdXNZTO75Ym3SaEEJJ+UITTwv18eKG2/GBsFJCzGND1V8CjstGnevI8UqWe/PfCfdXuUNkDX3WqiCzO/G8jhBBrgN/mpiQ8GNg4DDi/Xtsu2x7oMAdwyW70qU4GPFG1fyULlpODHSa1L4+eNQsjkyxpIoQQYhVQhE1F4BlgpQ/w6Cpg5wC0/BKoNUi7DtgINBoNft1/A1M2X0BUjAZFc7mq5BsVChov5IQQQiwbirAp3M/HfwO2fAJEhwPZCmlzPxeuYfSpgsOj8Oma09h8JlC121TIj6+7VEI2F+OrKBFCCLF8KMKvQ2Qo8PdHwOnl2rYsQ+r0kzYDlpGcu/NUuZ9vPHwOR/tMGNu2LN6tW4zuZ0IIsWIowq/DoflaAc5kDzQbD9Qdoc0FbaT7efmRAEzceA6R0bEomCMzZr9dBVWK5EyzbhNCCLEMKMKvQ51hwO3jQO33gWL1jP54aEQ0xq0/i3Unbqt20zJ5Mb2bN3K4OqVBZwkhhFgaFOHXwcEJ6PFnqj56+V4IBv95HFfuP4O9XSZ83Ko0BjYoDjs7up8JIcRWoAibgbXHb+GzdWcRFhWDfNmcMatnVdT0NH4emRBCSMaGIpyOhEfF4PO/zmHZ4QDVrl8yN37oURm5sxqfSYsQQkjGhyKcTlx/EIr3/zyOC3eD1dLhEc28MKypl3JFE0IIsU0owunAptN3MXrNaTyLiEauLE6Y2aMK6nvlNne3CCGEmBmKcBoSER2DrzZdwK8Hbqq2zPvO6lkF+bKlrpISIYQQ64IinEYEPHqOoUuP49Stp6o9uHEJjGxRCg72xq0jJoQQYr1QhNMA3/P3MHLlSQSHRyN7ZkfM6O6NpmXymbtbhBBCLAyKsAmJionFd9v88NPua6pduXAOlf2qUE5Xc3eNEEKIBWIRvtE5c+agWLFicHFxQa1atXD48OEkj23cuLHKp5xwa9euHczJ3adh6LngoF6A+9XzxMr/1aEAE0IIsVxLeMWKFfjoo48wf/58JcA//PADWrVqBT8/P+TNm/el49euXYvIyEh9++HDh/D29kbXrl1hLnZfCsIHK07iUWgk3Jwd8G3XSmhdoYDZ+kMIISRjYHZLePr06RgwYAD69u2LcuXKKTF2dXXFokWLEj3e3d0d+fPn12++vr7qeHOIcEysBtP/8YPP4sNKgMt7ZMPfw+tTgAkhhFi+JSwW7bFjxzBmzBj9Pjs7OzRv3hwHDhxI0Tl++eUX9OjRA1myZEn0/YiICLXpCAkJMUHPgfsh4Rix7CQOXHuo2r1qFcH4N8rBxdHeJOcnhBBi/ZjVEn7w4AFiYmKQL1/8yGFpBwZqC9snh8wdnz17Fu+9916Sx0ydOhXZs2fXb2Jtm4KAR2E4cuMRXJ3sMbNHZUzpVJECTAghJGO5o18HsYIrVqyImjVrJnmMWNlPnz7Vb+fPnzfJtasVzYlvulTCxqH10aFyQZOckxBCiG1hVnd07ty5YW9vj3v37sXbL22Z702O0NBQLF++HJMnT072OGdnZ7XpCA4Ohql4q2ohk52LEEKI7WFWS9jJyQnVqlXD9u3b9ftiY2NVu06dOsl+dtWqVWqut3fv3unQU0IIIcQKlyjJ8iQfHx9Ur15duZVliZJYuRItLfTp0wcFCxZUc7sJXdEdO3ZErly5zNRzQgghJIOLcPfu3REUFIQJEyaoYKzKlStj69at+mAtf39/FTFtiKwh3rt3L/755x8z9ZoQQgh5fTJpNBoNbIhbt26hcOHCCAgIQKFCnNMlhBBiPp3J0NHRhBBCSEbG7O7o9EYCv4S7d++auyuEEEKsEJ2+6PQmOWxOhHXLoZJbW0wIIYSYQm+KFCmS7DE2NyccHR2NEydOqMCvhAFfxiIpMCUDlyQAcXNzM1kfrQ2OU8rhWKUcjlXK4Dil/1iJBSwCXKVKFTg4JG/r2pwImxJJ/CGpMCUTV7Zs2czdHYuF45RyOFYph2OVMjhOlj1WDMwihBBCzARFmBBCCDETFOHXQHJST5w4MV5uavIyHKeUw7FKORyrlMFxsuyx4pwwIYQQYiZoCRNCCCFmgiJMCCGEmAmKMCGEEGImKMKpZM6cOShWrBhcXFxQq1YtHD582Nxdskh2796N9u3bw8PDA5kyZcL69evN3SWLREp11qhRQyUIyJs3ryrTKdXCSHzmzZuHSpUqqTWcsknd8S1btpi7WxbPtGnT1N/fBx98YO6uWByTJk1SY2O4lSlTJt2uTxFOBStWrFB1kCWK7vjx4/D29karVq1w//59c3fN4pDa0DI+8tBCkmbXrl0YMmQIDh48CF9fX0RFRaFly5Zq/EgcUpFGBOXYsWM4evQomjZtig4dOuDcuXPm7prFcuTIEfz000/q4YUkTvny5VW+Z90mpXLTDYmOJsZRs2ZNzZAhQ/TtmJgYjYeHh2bq1Klm7ZelI79u69atM3c3MgT3799X47Vr1y5zd8XiyZkzp+bnn382dzcskpCQEI2Xl5fG19dX06hRI82IESPM3SWLY+LEiRpvb2+zXZ+WsJFERkaqp/DmzZvr90kOamkfOHDArH0j1oOkzRPc3d3N3RWLJSYmBsuXL1feAnFLk5cR70q7du3ifV+Rl7l8+bKaMitevDh69eoFf39/pBc2V0XpdXnw4IH645cCEIZI++LFi2brF7EeJPm7zN3Vq1cPFSpUMHd3LI4zZ84o0Q0PD0fWrFmxbt06lXSfxEceUGS6TNzRJGkkpmfJkiUoXbq0ckV//vnnaNCgAc6ePZsuBS8owoRYoPUiXwDpOi+VgZAvy5MnTypvwerVq+Hj46Pm1CnEcQQEBGDEiBEqvkCCR0nStGnTRv9a5s1FlIsWLYqVK1eif//+SGsowkaSO3du2Nvb6+sS65B2/vz5zdYvYh0MHToUf//9t4oqlyAk8jJOTk4oWbKkel2tWjVl6c2cOVMFHxEtMmUmgaJVq1bV7xMPnvxezZ49GxEREep7jLxMjhw5UKpUKVy5cgXpAeeEU/EFIH/427dvj+c+lDbnpUhqkbg1EWBxre7YsQOenp7m7lKGQf7+RFRIHM2aNVNue/EY6Lbq1aur+U55TQFOmmfPnuHq1asoUKAA0gNawqlAlieJC0x+qWvWrIkffvhBBYf07dvX3F2zyF9owyfK69evqy8BCTgqUqSIWftmaS7opUuXYsOGDWoeKjAwUO2X2qaZM2c2d/cshjFjxij3ofzuSAF2GbOdO3di27Zt5u6aRSG/QwnjCbJkyYJcuXIxziABo0aNUrkMxAV9584dtfRUHlJ69uyJ9IAinAq6d++OoKAgTJgwQX1ZVq5cGVu3bn0pWItAreVs0qRJvAcYQR5iJBiCxCWhEBo3bhxv/+LFi/Huu++aqVeWh7hY+/TpowJo5AFF5vBEgFu0aGHurpEMyq1bt5TgPnz4EHny5EH9+vXVen15nR6wihIhhBBiJjgnTAghhJgJijAhhBBiJijChBBCiJmgCBNCCCFmgiJMCCGEmAmKMCGEEGImKMKEEEKImaAIE0IIIWaCIkwIMRmZMmXC+vXrzd0NQjIMFGFCrARJbykimHBr3bq1ubtGCEkC5o4mxIoQwZV804Y4OzubrT+EkOShJUyIFSGCK3WtDbecOXOq98QqlkIRUoVIKjMVL14cq1evjvd5KX/XtGlT9b5U3Bk4cKCqhGXIokWLUL58eXUtKfcmJRgNefDgATp16gRXV1d4eXlh48aN+vceP36syulJcny5hryf8KGBEFuCIkyIDTF+/Hh07twZp06dUmLYo0cPXLhwQb0n5ThbtWqlRPvIkSNYtWoV/v3333giKyIuZRdFnEWwRWBLliwZ7xqff/45unXrhtOnT6Nt27bqOo8ePdJf//z589iyZYu6rpwvd+7c6TwKhFgQUkWJEJLx8fHx0djb22uyZMkSb5syZYp6X/7cBw0aFO8ztWrV0gwePFi9XrBggSZnzpyaZ8+e6d/ftGmTxs7OThMYGKjaHh4ems8++yzJPsg1xo0bp2/LuWTfli1bVLt9+/aavn37mvjOCcm4cE6YECtCajfrahPrcHd317+uU6dOvPekffLkSfVaLFNvb29V/F1HvXr1EBsbCz8/P+XOlqLnzZo1S7YPUuNXh5wrW7Zsqg6wMHjwYGWJHz9+HC1btkTHjh1Rt27d17xrQjIuFGFCrAgRvYTuYVMhc7gpwdHRMV5bxFuEXJD56Js3b2Lz5s3w9fVVgi7u7e+++y5N+kyIpcM5YUJsiIMHD77ULlu2rHotP2WuWOaGdezbtw92dnYoXbo03NzcUKxYMWzfvv21+iBBWT4+Pvjjjz/www8/YMGCBa91PkIyMrSECbEiIiIiEBgYGG+fg4ODPvhJgq2qV6+O+vXr488//8Thw4fxyy+/qPckgGrixIlKICdNmoSgoCAMGzYM77zzDvLly6eOkf2DBg1C3rx5lVUbEhKihFqOSwkTJkxAtWrVVHS19PXvv//WPwQQYotQhAmxIrZu3aqWDRkiVuzFixf1kcvLly/H+++/r45btmwZypUrp96TJUXbtm3DiBEjUKNGDdWW+dvp06frzyUCHR4ejhkzZmDUqFFK3Lt06ZLi/jk5OWHMmDG4ceOGcm83aNBA9YcQWyWTRGeZuxOEkLRH5mbXrVungqEIIZYB54QJIYQQM0ERJoQQQswE54QJsRE480SI5UFLmBBCCDETFGFCCCHETFCECSGEEDNBESaEEELMBEWYEEIIMRMUYUIIIcRMUIQJIYQQM0ERJoQQQswERZgQQgiBefg/prMItlIYrpkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 绘制训练和验证准确率曲线\n",
    "# ==========================================================\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1️⃣ 创建 x 轴数据：训练轮数对应的 tensor\n",
    "# ---------------------------------------\n",
    "# torch.linspace(start, end, steps)\n",
    "# 生成一个从 start 到 end 的等间距 tensor，共 steps 个点\n",
    "# 这里 len(train_accs) 个点对应每次记录的训练准确率\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "# 说明：每个点表示训练过程中记录准确率时的训练轮数\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2️⃣ 创建第二个 x 轴数据：累计训练样本数\n",
    "# ---------------------------------------\n",
    "# 同样使用 torch.linspace 生成等间距 tensor\n",
    "# 从 0 到 examples_seen，共 len(train_accs) 个点\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "# 说明：每个点对应训练过程中累计处理的样本数，方便在双 x 轴上显示\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3️⃣ 调用自定义的 plot_values 函数绘图\n",
    "# ---------------------------------------\n",
    "# 参数解释：\n",
    "# - epochs_tensor: 第一个 x 轴，表示训练轮数\n",
    "# - examples_seen_tensor: 第二个 x 轴，表示累计样本数\n",
    "# - train_accs: y 轴，训练准确率\n",
    "# - val_accs: y 轴，验证准确率\n",
    "# - label=\"accuracy\": 用于修改 y 轴标签和图例\n",
    "plot_values(\n",
    "    epochs_seen=epochs_tensor,          # 第一 x 轴：训练轮数\n",
    "    examples_seen=examples_seen_tensor, # 第二 x 轴：累计样本数\n",
    "    train_values=train_accs,            # 训练集准确率\n",
    "    val_values=val_accs,                # 验证集准确率\n",
    "    label=\"accuracy\"                    # y 轴标签显示为 accuracy\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aba699-21bc-42de-a69c-99f370bb0363",
   "metadata": {},
   "source": [
    "- 根据上图的准确率曲线，我们可以看到模型在第 4 和第 5 个 epoch 后达到了相对较高的训练和验证准确率。  \n",
    "- 但是，我们必须注意，之前在训练函数中指定了 `eval_iter=5`，这意味着我们只是估算了训练集和验证集的表现。  \n",
    "- 我们可以按照下面的方法计算整个数据集上的训练、验证和测试集性能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "UHWaJFrjY0zW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHWaJFrjY0zW",
    "outputId": "e111e6e6-b147-4159-eb9d-19d4e809ed34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.60%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 96.00%\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 计算训练集、验证集和测试集准确率\n",
    "# ==========================================================\n",
    "# 1️⃣ 调用自定义函数 calc_accuracy_loader 计算训练集准确率\n",
    "# 参数说明：\n",
    "# - train_loader: 数据加载器（DataLoader），提供训练批次\n",
    "# - model: 当前训练或微调的模型（nn.Module）\n",
    "# - device: 运行设备（CPU 或 GPU）\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "# 计算结果是一个 0~1 之间的小数，表示预测正确的样本比例\n",
    "\n",
    "# 2️⃣ 调用 calc_accuracy_loader 计算验证集准确率\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "# 用于监控模型在未见过数据上的性能\n",
    "\n",
    "# 3️⃣ 调用 calc_accuracy_loader 计算测试集准确率\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "# 测试集一般是最终评估模型的指标，不能用于训练\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4️⃣ 输出结果\n",
    "# ---------------------------------------\n",
    "# f-string 格式化：\n",
    "# - {train_accuracy*100:.2f}%: 将小数转为百分比形式，保留两位小数\n",
    "# - 输出类似 \"Training accuracy: 95.12%\"\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882649f-dc7b-401f-84d2-024ff79c74a1",
   "metadata": {},
   "source": [
    "- 我们可以看到训练集和验证集的性能几乎完全一致。  \n",
    "- 然而，根据测试集性能略低的情况，可以发现模型对训练数据以及用于调整部分超参数（例如学习率）的验证数据存在轻微过拟合。  \n",
    "- 不过，这是正常现象，这种差距有可能通过增加模型的 dropout 率（`drop_rate`）或优化器设置中的 `weight_decay` 来进一步缩小。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d9ad7-3ec1-450e-8c9f-4fc46d3d5bb0",
   "metadata": {},
   "source": [
    "## 6.8 将大型语言模型（LLM）用作垃圾邮件分类器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebcfa2-479e-408b-9cf0-7421f6144855",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch06_compressed/18.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5408e6-83e4-4e5a-8503-c2fba6073f31",
   "metadata": {},
   "source": [
    "- 最后，让我们实际使用微调后的 GPT 模型。  \n",
    "- 下面的 `classify_review` 函数实现了与之前 `SpamDataset` 类似的数据预处理步骤。  \n",
    "- 接着，该函数会返回模型预测的整数类别标签，并返回对应的类别名称。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aHdn6xvL-IW5",
   "metadata": {
    "id": "aHdn6xvL-IW5"
   },
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    \"\"\"\n",
    "    使用微调后的 GPT 分类器对单条文本进行垃圾邮件分类。\n",
    "\n",
    "    参数：\n",
    "    - text (str): 待分类的文本字符串\n",
    "    - model (nn.Module): 微调后的 GPT 模型\n",
    "    - tokenizer: 分词器，用于将文本编码成 token ids\n",
    "    - device (torch.device): 运行设备（CPU 或 GPU）\n",
    "    - max_length (int, optional): 输入文本的最大长度（超过截断）\n",
    "    - pad_token_id (int, default=50256): 用于补齐序列的 token ID\n",
    "\n",
    "    返回：\n",
    "    - \"spam\" 或 \"not spam\" 字符串\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 1️⃣ 将模型切换到评估模式\n",
    "    #    - 禁用 Dropout 和其他训练特定行为\n",
    "    model.eval()\n",
    "\n",
    "    # -------------------------\n",
    "    # 2️⃣ 将文本编码成 token id 序列\n",
    "    input_ids = tokenizer.encode(text)  # 返回一个整数列表，每个整数对应一个词/子词\n",
    "\n",
    "    # -------------------------\n",
    "    # 3️⃣ 获取模型支持的最大上下文长度\n",
    "    #    - pos_emb.weight.shape[0] 对应 GPT 模型的位置嵌入长度，也就是 context length\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    # 注意：\n",
    "    # - 在书中曾错误使用 shape[1]，结果没报错，但可能导致不必要的截断\n",
    "\n",
    "    # -------------------------\n",
    "    # 4️⃣ 截断文本序列，保证不超过 max_length 和模型支持的 context length\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # -------------------------\n",
    "    # 5️⃣ 参数校验\n",
    "    assert max_length is not None, (\n",
    "        \"必须指定 max_length。如果希望使用模型全上下文长度，\"\n",
    "        \"请传入 max_length=model.pos_emb.weight.shape[0].\"\n",
    "    )\n",
    "    assert max_length <= supported_context_length, (\n",
    "        f\"max_length ({max_length}) 超过模型最大支持的上下文长度 ({supported_context_length})\"\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 6️⃣ 补齐序列到 max_length\n",
    "    #    - 使用 pad_token_id 补齐不足的长度\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "\n",
    "    # -------------------------\n",
    "    # 7️⃣ 转换为张量，并增加 batch 维度\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "    # 结果 shape: (1, max_length)\n",
    "\n",
    "    # -------------------------\n",
    "    # 8️⃣ 模型前向推理\n",
    "    with torch.no_grad():  # 禁用梯度计算，提高推理效率\n",
    "        logits = model(input_tensor)[:, -1, :]  # 取最后一个 token 的 logits\n",
    "        # logits shape: (1, num_classes)\n",
    "\n",
    "    # -------------------------\n",
    "    # 9️⃣ 获取预测类别\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    # predicted_label: 0 或 1\n",
    "    # 0 -> not spam, 1 -> spam\n",
    "\n",
    "    # -------------------------\n",
    "    # 10️⃣ 返回可读的分类结果\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29682d8-a899-4d9b-b973-f8d5ec68172c",
   "metadata": {},
   "source": [
    "- 让我们在下面几个示例上试用该函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "apU_pf51AWSV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apU_pf51AWSV",
    "outputId": "d0fde0a5-e7a3-4dbe-d9c5-0567dbab7e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1️⃣ 定义要分类的文本\n",
    "#    - 这里是一条典型的垃圾短信示例\n",
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ 调用 classify_review 函数进行分类\n",
    "# 参数说明：\n",
    "# - text_1: 待分类文本\n",
    "# - model: 微调后的 GPT 分类器模型\n",
    "# - tokenizer: GPT 使用的分词器，将文本转换为 token id\n",
    "# - device: 运行设备（CPU 或 GPU）\n",
    "# - max_length: 输入文本的最大长度，这里使用训练数据集的最大长度\n",
    "predicted_label = classify_review(\n",
    "    text_1,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_length=train_dataset.max_length\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ 输出分类结果\n",
    "print(predicted_label)\n",
    "# 输出示例：\n",
    "# \"spam\" -> 表示模型认为这是垃圾短信\n",
    "# \"not spam\" -> 表示模型认为这不是垃圾短信\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1g5VTOo_Ajs5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1g5VTOo_Ajs5",
    "outputId": "659b08eb-b6a9-4a8a-9af7-d94c757e93c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1️⃣ 定义要分类的文本\n",
    "#    - 这里是一条普通短信示例，不含垃圾信息\n",
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ 调用 classify_review 函数进行分类\n",
    "# 参数说明：\n",
    "# - text_2: 待分类文本\n",
    "# - model: 微调后的 GPT 分类器模型\n",
    "# - tokenizer: GPT 使用的分词器，用于将文本转为 token id\n",
    "# - device: 运行设备（CPU 或 GPU）\n",
    "# - max_length: 输入文本的最大长度，这里使用训练集的最大长度\n",
    "predicted_label = classify_review(\n",
    "    text_2,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_length=train_dataset.max_length\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ 输出分类结果\n",
    "print(predicted_label)\n",
    "# 输出示例：\n",
    "# \"spam\" -> 表示模型认为这是垃圾短信\n",
    "# \"not spam\" -> 表示模型认为这不是垃圾短信\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf736e39-0d47-40c1-8d18-1f716cf7a81e",
   "metadata": {},
   "source": [
    "- 最后，让我们保存模型，以便以后在不重新训练的情况下重复使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mYnX-gI1CfQY",
   "metadata": {
    "id": "mYnX-gI1CfQY"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 1️⃣ 保存模型权重\n",
    "# torch.save() 是 PyTorch 用于保存对象的函数\n",
    "# model.state_dict() 返回模型的状态字典（state dict），\n",
    "#    包含模型的所有可学习参数（权重和偏置），不包含模型结构\n",
    "# \"review_classifier.pth\" 是保存文件的路径和名称\n",
    "torch.save(model.state_dict(), \"review_classifier.pth\")\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ 注释说明\n",
    "# - state_dict: 一个 Python 字典，key 是参数名称，value 是对应的张量\n",
    "# - 仅保存 state_dict 而不保存整个模型对象有以下优点：\n",
    "#    1. 文件更小\n",
    "#    2. 加载时可以灵活地在相同结构的模型中重用权重\n",
    "#    3. 避免因代码修改导致的模型对象不兼容问题\n",
    "# - 使用方法示例：\n",
    "#   model = GPTModel(BASE_CONFIG)      # 先定义模型结构\n",
    "#   model.load_state_dict(torch.load(\"review_classifier.pth\"))  # 加载权重\n",
    "#   model.eval()                        # 切换到评估模式\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba78cf7c-6b80-4f71-a50e-3ccc73839af6",
   "metadata": {},
   "source": [
    "- 然后，在新的会话中，我们可以按如下方式加载模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc4e68a5-d492-493b-87ef-45c475f353f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 1️⃣ 加载已保存的模型权重\n",
    "# torch.load() 用于加载 PyTorch 保存的对象（这里是 state_dict）\n",
    "# map_location=device 表示将权重映射到指定设备（CPU 或 GPU）\n",
    "# 注意：torch.load 没有 weights_only 参数，因此不要加\n",
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ 将加载的权重赋值给模型\n",
    "# model.load_state_dict() 接收 state_dict 并更新模型参数\n",
    "# 如果 state_dict 与模型结构完全匹配，这一步会成功\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ 切换模型到评估模式\n",
    "# model.eval() 告诉 PyTorch 模型在推理阶段，不要应用 Dropout 或 BatchNorm 的训练行为\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70ac71-234f-4eeb-b33d-c62726d50cd4",
   "metadata": {
    "id": "5b70ac71-234f-4eeb-b33d-c62726d50cd4"
   },
   "source": [
    "## 总结与要点\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdc910-d616-47ab-aa85-f90c6e7ed80e",
   "metadata": {},
   "source": [
    "- 参见 [./gpt_class_finetune.py](./gpt_class_finetune.py) 脚本，这是一个独立的分类微调脚本。  \n",
    "- 练习答案可以在 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 中找到。  \n",
    "- 此外，有兴趣的读者可以在 [附录 E](../../appendix-E) 中找到关于低秩适配（LoRA）参数高效训练的介绍。\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
