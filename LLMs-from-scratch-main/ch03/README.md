# 第3章：编写注意力机制代码

&nbsp;
## 章节主代码

- [01_main-chapter-code](01_main-chapter-code) 包含了本章的主要代码。

&nbsp;
## 附加材料

- [02_bonus_efficient-multihead-attention](02_bonus_efficient-multihead-attention) 实现并比较了多头注意力（multihead-attention）的不同实现变体
- [03_understanding-buffers](03_understanding-buffers) 解释了 PyTorch buffers 的概念，这些 buffers 用于在第3章中实现因果注意力机制（causal attention）

<br>
在下面的视频中，我提供了一个代码实操课程，涵盖了一些章节内容，作为补充材料。

<br>
<br>

[![视频链接](https://img.youtube.com/vi/-Ll8DtpNtvk/0.jpg)](https://www.youtube.com/watch?v=-Ll8DtpNtvk)
