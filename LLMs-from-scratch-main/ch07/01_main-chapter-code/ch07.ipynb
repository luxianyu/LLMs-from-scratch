{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e91914-5f51-43fa-b65b-625e73b4d17b",
   "metadata": {
    "id": "12e91914-5f51-43fa-b65b-625e73b4d17b"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "以下代码为 <a href=\"http://mng.bz/orYv\">《从零开始构建大型语言模型》</a> 一书的补充代码，作者为 <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>中文翻译和代码详细注释由Lux整理，Github下载地址：<a href=\"https://github.com/luxianyu\">https://github.com/luxianyu</a>\n",
    "    \n",
    "<br>Lux的Github上还有吴恩达深度学习Pytorch版学习笔记及中文详细注释的代码下载\n",
    "    \n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf",
   "metadata": {
    "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf"
   },
   "source": [
    "# 第7章：微调以遵循指令\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
    "outputId": "bcdfe2cb-d084-4920-d703-503131aabec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.1.3\n",
      "matplotlib version: 3.10.0\n",
      "tiktoken version: 0.12.0\n",
      "torch version: 2.9.0+cpu\n",
      "tqdm version: 4.67.1\n",
      "tensorflow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",       # PyTorch & TensorFlow dependency\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "    \"tqdm\",        # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fca98-2f9a-4193-b435-2abfa3b4142f",
   "metadata": {
    "id": "264fca98-2f9a-4193-b435-2abfa3b4142f"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/01.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813",
   "metadata": {
    "id": "8bbc68e9-75b3-41f1-ac2c-e071c3cd0813"
   },
   "source": [
    "## 7.1 指令微调简介\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab",
   "metadata": {
    "id": "53dba24a-6805-496c-9a7f-c75e2d3527ab"
   },
   "source": [
    "- 在第 5 章中，我们看到预训练 LLM 的过程涉及一种训练方法，使其学会一次生成一个单词。  \n",
    "- 因此，预训练后的 LLM 擅长文本补全，但不擅长遵循指令。  \n",
    "- 在本章中，我们将教 LLM 更好地遵循指令。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc0535-0904-44ed-beaf-9b678292ef35",
   "metadata": {
    "id": "18dc0535-0904-44ed-beaf-9b678292ef35"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/02.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8",
   "metadata": {
    "id": "b4698b23-12e0-4bd7-a140-ccb3dd71d4e8"
   },
   "source": [
    "- 本章涵盖的主题在下图中进行了总结。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/03.webp\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86",
   "metadata": {
    "id": "5384f0cf-ef3c-4436-a5fa-59bd25649f86"
   },
   "source": [
    "## 7.2 为有监督指令微调准备数据集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b34ff8-619f-4e89-bd03-ce513269760d",
   "metadata": {
    "id": "f8b34ff8-619f-4e89-bd03-ce513269760d"
   },
   "source": [
    "- 我们将使用我为本章准备的指令数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0G3axLw6kY1N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G3axLw6kY1N",
    "outputId": "07e1e4f9-026c-48c1-8a06-f2bfb1fb354e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 导入必要的 Python 库\n",
    "# ==============================\n",
    "import json      # 用于读取和解析 JSON 文件\n",
    "import os        # 用于操作文件路径、检查文件是否存在\n",
    "import requests  # 用于发送 HTTP 请求下载文件\n",
    "\n",
    "# ==============================\n",
    "# 定义下载并加载 JSON 文件的函数\n",
    "# ==============================\n",
    "def download_and_load_file(file_path, url):\n",
    "    \"\"\"\n",
    "    如果文件不存在，则从指定 URL 下载文件，并保存到本地。\n",
    "    然后加载该 JSON 文件并返回解析后的 Python 对象（如 list 或 dict）。\n",
    "\n",
    "    参数：\n",
    "    - file_path: str，本地保存文件的路径\n",
    "    - url: str，文件下载 URL\n",
    "\n",
    "    返回：\n",
    "    - data: Python 对象（list/dict），JSON 文件解析后的内容\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------\n",
    "    # 1️⃣ 检查文件是否已经存在\n",
    "    # 如果存在，则跳过下载，直接读取\n",
    "    # --------------------------\n",
    "    if not os.path.exists(file_path):\n",
    "        # 发送 HTTP GET 请求下载文件，timeout=30 表示 30 秒超时\n",
    "        response = requests.get(url, timeout=30)\n",
    "        # 如果返回状态码不是 200，则抛出异常\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # 将下载的文本内容存储到变量\n",
    "        text_data = response.text\n",
    "\n",
    "        # 将下载的文本写入本地文件，encoding=\"utf-8\" 保证中文正常保存\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    # --------------------------\n",
    "    # 2️⃣ 文件已存在或刚下载完毕，读取 JSON 文件\n",
    "    # --------------------------\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)  # json.load 解析 JSON 字符串为 Python 对象\n",
    "\n",
    "    # 返回解析后的数据\n",
    "    return data\n",
    "\n",
    "# ==============================\n",
    "# 原书中的旧版本实现\n",
    "# 使用 urllib 下载文件，但协议较老，某些 VPN 环境可能无法访问\n",
    "# 使用 requests 更加稳健\n",
    "# ==============================\n",
    "\"\"\"\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# 3️⃣ 设置本地文件路径和下载 URL\n",
    "# ==============================\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 4️⃣ 调用函数下载并加载 JSON 文件\n",
    "# ==============================\n",
    "data = download_and_load_file(file_path, url)\n",
    "\n",
    "# 打印数据条目数量\n",
    "print(\"Number of entries:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8176-4255-4e92-8c7d-998771733eb8",
   "metadata": {
    "id": "d7af8176-4255-4e92-8c7d-998771733eb8"
   },
   "source": [
    "- 我们从上面 JSON 文件中加载的 `data` 列表中的每一项都是一个字典，形式如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "-LiuBMsHkzQV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-LiuBMsHkzQV",
    "outputId": "a4ee5c2d-db53-4a80-e5ee-0bbcf6fe0450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "# 打印 JSON 数据中的第 51 条条目（Python 的索引从 0 开始，所以 data[50] 是第 51 条）\n",
    "# 用于查看数据结构和示例内容\n",
    "print(\"Example entry:\\n\", data[50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46",
   "metadata": {
    "id": "c5a32b34-485a-4816-a77a-da14f9fe6e46"
   },
   "source": [
    "- 注意，`'input'` 字段可以为空：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "uFInFxDDk2Je",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFInFxDDk2Je",
    "outputId": "b4f84027-bb9e-4e51-b79e-1329c8bff093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034799a-6575-45fd-98c9-9d1012d0fd58",
   "metadata": {
    "id": "f034799a-6575-45fd-98c9-9d1012d0fd58"
   },
   "source": [
    "- 指令微调通常被称为“监督式指令微调（supervised instruction finetuning）”，因为它涉及在明确提供输入-输出对的数据集上训练模型。  \n",
    "- 将条目格式化为 LLM 输入的方式有多种；下图展示了两种示例格式，分别用于训练 Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html) 和 Phi-3 (https://arxiv.org/abs/2404.14219) LLM。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10",
   "metadata": {
    "id": "dffa4f70-44d4-4be4-89a9-2159f4885b10"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/04.webp?2\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6",
   "metadata": {
    "id": "dd79a74e-befb-491c-be49-f777a6a5b6a6"
   },
   "source": [
    "- 在本章中，我们使用 Alpaca 风格的提示（prompt）格式，这是指令微调的原始提示模板。  \n",
    "- 下面，我们将格式化要传递给 LLM 的输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Jhk37nnJnkBh",
   "metadata": {
    "id": "Jhk37nnJnkBh"
   },
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    \"\"\"\n",
    "    将 JSON 数据中的单条条目（entry）格式化为模型输入文本。\n",
    "    \n",
    "    参数：\n",
    "    - entry: dict, 例如 data[50]，包含 \"instruction\" 和 \"input\" 等字段\n",
    "    \n",
    "    返回：\n",
    "    - 字符串, 经过格式化后的文本，可以直接作为模型的输入\n",
    "    \"\"\"\n",
    "\n",
    "    # 构建 instruction 部分文本\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"   # 引导文本，说明这是一个任务\n",
    "        f\"Write a response that appropriately completes the request.\"  # 告诉模型要生成适当的回应\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"  # 将条目中的 instruction 字段插入到 Instruction 部分\n",
    "    )\n",
    "\n",
    "    # 构建 input 部分文本（如果 entry[\"input\"] 不是空字符串）\n",
    "    # 使用条件表达式，如果 input 有内容，就添加 Input 部分，否则为空字符串\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    # 返回拼接后的完整文本：instruction + input\n",
    "    return instruction_text + input_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6",
   "metadata": {
    "id": "011e78b4-e89a-4653-a2ee-7b2739ca04d6"
   },
   "source": [
    "- 带有输入字段的格式化响应如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "F9UQRfjzo4Js",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9UQRfjzo4Js",
    "outputId": "7b615d35-2a5f-474d-9292-a69bc3850e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "# 取 JSON 数据中的第 50 条条目，并格式化成模型输入文本\n",
    "model_input = format_input(data[50])\n",
    "# 调用之前定义的 format_input 函数，将 instruction 和 input（如果有）拼接成文本\n",
    "# 结果类似：\n",
    "# \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# \n",
    "# ### Instruction:\n",
    "# ...\"\n",
    "# 如果 entry[\"input\"] 不为空，还会加上 ### Input 部分\n",
    "\n",
    "# 构建目标输出文本\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "# 使用 f-string 将条目中的 \"output\" 字段作为 Response 部分\n",
    "# 注意前面加了 \"\\n\\n\" 让输出与输入文本分行，方便模型学习或阅读\n",
    "\n",
    "# 打印完整的输入 + 目标输出\n",
    "print(model_input + desired_response)\n",
    "# 这一步展示了完整的训练或微调示例，包括 Instruction、Input（如果有）、Response\n",
    "# 可以直接用作示例数据，或者检查格式是否符合模型预期\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc93ddf-431c-49c0-96f2-fb3a79c4d94c",
   "metadata": {
    "id": "4dc93ddf-431c-49c0-96f2-fb3a79c4d94c"
   },
   "source": [
    "- 下面是一个没有输入字段的格式化响应：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3891fa9-f738-41cd-946c-80ef9a99c346",
    "outputId": "2142c5a4-b594-49c5-affe-2d963a7bd46b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# 从 JSON 数据中取第 1000 条条目（索引为 999，因为 Python 索引从 0 开始）\n",
    "model_input = format_input(data[999])\n",
    "# 调用之前定义的 format_input 函数\n",
    "# 将 entry[\"instruction\"] 和 entry[\"input\"]（如果有）拼接成输入文本\n",
    "# 生成类似如下的文本：\n",
    "# \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "#\n",
    "# ### Instruction:\n",
    "# ...\"\n",
    "# 如果 entry[\"input\"] 不为空，还会自动加上 \"\\n\\n### Input:\\n{input内容}\"\n",
    "\n",
    "# 构建模型期望输出\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "# 将 JSON 数据中该条目的 output 字段放入 \"### Response:\" 部分\n",
    "# 前面加了两个换行符 \"\\n\\n\" 让输出和输入文本分开，便于模型阅读和训练\n",
    "\n",
    "# 打印完整的训练示例（输入 + 目标输出）\n",
    "print(model_input + desired_response)\n",
    "# 输出结果会包括 Instruction、Input（可选）和 Response 三个部分\n",
    "# 这样可以直接用于模型训练、微调或示例展示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8afd5-2a21-49a5-90c3-6a03865a4771",
   "metadata": {
    "id": "4aa8afd5-2a21-49a5-90c3-6a03865a4771"
   },
   "source": [
    "- 最后，在下一节准备 PyTorch 数据加载器之前，我们将数据集划分为训练集、验证集和测试集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aFZVopbIlNfx",
   "metadata": {
    "id": "aFZVopbIlNfx"
   },
   "outputs": [],
   "source": [
    "# 计算训练集大小：取总数据的 85%\n",
    "train_portion = int(len(data) * 0.85)  \n",
    "\n",
    "# 计算测试集大小：取总数据的 10%\n",
    "test_portion = int(len(data) * 0.1)    \n",
    "\n",
    "# 计算验证集大小：剩余的 5%\n",
    "val_portion = len(data) - train_portion - test_portion  \n",
    "\n",
    "# 划分训练集：从索引 0 到 train_portion（不包含 train_portion）\n",
    "train_data = data[:train_portion]\n",
    "\n",
    "# 划分测试集：从训练集末尾开始，长度为 test_portion\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "\n",
    "# 划分验证集：从测试集末尾到数据结尾\n",
    "val_data = data[train_portion + test_portion:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "-zf6oht6bIUQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zf6oht6bIUQ",
    "outputId": "657ec5c6-4caa-4d1a-ba2e-23acd755ab07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf606-f913-4445-8301-632ae10d387d",
   "metadata": {
    "id": "fcaaf606-f913-4445-8301-632ae10d387d"
   },
   "source": [
    "## 7.3 将数据整理成训练批次\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f63bd-9755-4d07-8884-5e2e5345cf27",
   "metadata": {
    "id": "233f63bd-9755-4d07-8884-5e2e5345cf27"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/05.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c",
   "metadata": {
    "id": "c149fc1a-7757-4ec8-80cb-e2a3fb007a2c"
   },
   "source": [
    "- 我们将通过几个步骤来处理数据集的批处理，如下图所示：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/06.webp?1\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af423f-aad9-4b3c-bea5-153021c04862",
   "metadata": {
    "id": "b9af423f-aad9-4b3c-bea5-153021c04862"
   },
   "source": [
    "- 首先，我们实现一个 `InstructionDataset` 类，对数据集中的所有输入进行预先 token 化，类似于第 6 章中的 `SpamDataset`。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/07.webp?1\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb",
   "metadata": {
    "id": "adc29dc4-f1c7-4c71-937b-95119d6239bb"
   },
   "outputs": [],
   "source": [
    "# 导入 PyTorch 和 Dataset 基类\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 自定义数据集类，用于指令-响应任务\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        \"\"\"\n",
    "        初始化方法\n",
    "        参数：\n",
    "        - data: 一个列表，每个元素是一个字典，包含 'instruction', 'input', 'output' 字段\n",
    "        - tokenizer: 用于将文本转换为 token id 的分词器\n",
    "        \"\"\"\n",
    "        self.data = data  # 保存原始数据\n",
    "\n",
    "        # 预先对所有文本进行编码（tokenize）\n",
    "        self.encoded_texts = []  # 用于存储编码后的 token id 列表\n",
    "        for entry in data:\n",
    "            # 调用 format_input，将 instruction 和 input 拼接成模型输入部分\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            \n",
    "            # 将期望输出（response）拼接到输入文本后\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            \n",
    "            # 得到完整的文本序列：输入 + 输出\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            \n",
    "            # 使用 tokenizer 将完整文本编码为 token id 列表\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        返回指定索引的数据\n",
    "        参数：\n",
    "        - index: 数据索引\n",
    "        返回：\n",
    "        - 编码后的 token id 列表\n",
    "        \"\"\"\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集总长度\n",
    "        \"\"\"\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f0e69-4b22-41c0-a25d-f077527eddd1",
   "metadata": {
    "id": "384f0e69-4b22-41c0-a25d-f077527eddd1"
   },
   "source": [
    "- 与第 6 章类似，我们希望将多个训练示例收集到一个 batch 中以加快训练速度；这需要将所有输入填充（padding）到相似长度。  \n",
    "- 同样类似于上一章，我们使用 `<|endoftext|>` token 作为填充 token。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
    "outputId": "ac44227b-9ec2-4131-9df8-89caa6e879ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# 导入 tiktoken 库，用于 GPT-2 风格的分词和编码\n",
    "import tiktoken\n",
    "\n",
    "# 获取 GPT-2 的编码器对象\n",
    "# 这个对象可以将文本编码为 token id，也可以解码回文本\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 编码特殊 token \"<|endoftext|>\"\n",
    "# allowed_special 参数用于指定哪些特殊 token 可以被识别\n",
    "# \"<|endoftext|>\" 是 GPT-2 模型使用的结束标记\n",
    "encoded_eot = tokenizer.encode(\n",
    "    \"<|endoftext|>\",               # 待编码文本\n",
    "    allowed_special={\"<|endoftext|>\"}  # 指定允许的特殊 token\n",
    ")\n",
    "\n",
    "# 输出编码后的 token id\n",
    "print(encoded_eot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427",
   "metadata": {
    "id": "9e5bd7bc-f347-4cf8-a0c2-94cb8799e427"
   },
   "source": [
    "- 在第 6 章中，我们将数据集中的所有示例填充到相同长度。  \n",
    "  - 在这里，我们采用更高级的方法，开发了一个自定义的 “collate” 函数，可以传递给数据加载器（data loader）。  \n",
    "  - 这个自定义 collate 函数会将每个 batch 中的训练示例填充到相同长度（但不同 batch 可以有不同长度）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3",
   "metadata": {
    "id": "65c4d943-4aa8-4a44-874e-05bc6831fbd3"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/08.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca",
   "metadata": {
    "id": "eb4c77dd-c956-4a1b-897b-b466909f18ca"
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(\n",
    "    batch,               # 一个 batch，通常是 InstructionDataset 返回的一批编码后的 token 列表\n",
    "    pad_token_id=50256,  # 默认使用 GPT-2 的结束标记 <|endoftext|> 作为填充 token\n",
    "    device=\"cpu\"         # 指定将返回的 tensor 放在哪个设备上（CPU 或 GPU）\n",
    "):\n",
    "    # ------------------------------\n",
    "    # 1️⃣ 找到当前 batch 中最长的序列长度\n",
    "    # 并加 +1，这是为了给每个序列增加一个额外的 padding token\n",
    "    # ------------------------------\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 2️⃣ 对 batch 中每个序列进行填充和处理\n",
    "    # ------------------------------\n",
    "    inputs_lst = []  # 用于存储处理后的每条序列\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()        # 拷贝原序列，避免修改原数据\n",
    "        new_item += [pad_token_id]    # 在序列末尾添加 <|endoftext|> token\n",
    "        # 填充序列到 batch_max_length\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        \n",
    "        # 注意：\n",
    "        # padded[:-1] 去掉最后一个多余的 padding token\n",
    "        # 因为 batch_max_length 在上一步加了 +1，这样保证每条序列末尾仍保留一个 <|endoftext|>\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "\n",
    "    # ------------------------------\n",
    "    # 3️⃣ 将 list 转换为 batch tensor\n",
    "    # torch.stack 可以把多个 shape 相同的一维/多维 tensor 堆叠成一个 batch\n",
    "    # 并放到指定的 device（CPU 或 GPU）上\n",
    "    # 输出 shape: (batch_size, batch_max_length-1)\n",
    "    # ------------------------------\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fb02373-59b3-4f3a-b1d1-8181a2432645",
    "outputId": "93d987b9-e3ca-4857-9b28-b67d515a94d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]  # 第 1 条序列，长度 5\n",
    "inputs_2 = [5, 6]           # 第 2 条序列，长度 2\n",
    "inputs_3 = [7, 8, 9]        # 第 3 条序列，长度 3\n",
    "\n",
    "batch = (inputs_1, inputs_2, inputs_3)  # batch 是一个 tuple，包含三条序列\n",
    "\n",
    "# 调用 custom_collate_draft_1 对 batch 进行处理\n",
    "result = custom_collate_draft_1(batch)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673ade5-be4c-4a2c-9a9a-d5c63fb1c424",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/09.webp?1\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17769a19-b961-4213-92ef-34f441b2d1d6",
   "metadata": {
    "id": "17769a19-b961-4213-92ef-34f441b2d1d6"
   },
   "source": [
    "- 上面，我们只返回了 LLM 的输入；然而，对于 LLM 训练，我们还需要目标值（targets）。  \n",
    "- 类似于 LLM 的预训练，targets 是将输入向右移动一个位置的结果，这样 LLM 就能学习预测下一个 token。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef",
   "metadata": {
    "id": "0386b6fe-3455-4e70-becd-a5a4681ba2ef"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/10.webp?1\" width=400px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc",
   "metadata": {
    "id": "74af192e-757c-4c0a-bdf9-b7eb25bf6ebc"
   },
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,                 # batch: 一个包含多条 token 序列（list of lists）的批次\n",
    "    pad_token_id=50256,    # pad_token_id: 用于填充的特殊 token（GPT-2 的 <|endoftext|>）\n",
    "    device=\"cpu\"           # device: 张量放置的设备（CPU 或 GPU）\n",
    "):\n",
    "    # ----------------------------------------------------------\n",
    "    # 1️⃣ 找到 batch 中最长序列长度，并额外 +1\n",
    "    #    +1 的原因：后面要在每个序列尾部添加一个 <|endoftext|> 标志\n",
    "    # ----------------------------------------------------------\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 2️⃣ 创建存放输入与目标的两个列表\n",
    "    # ----------------------------------------------------------\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 3️⃣ 依次处理 batch 中的每个样本\n",
    "    # ----------------------------------------------------------\n",
    "    for item in batch:\n",
    "        new_item = item.copy()           # 复制原序列，避免修改原数据\n",
    "        new_item += [pad_token_id]       # 在末尾添加 <|endoftext|> 结束符\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 填充到 batch_max_length\n",
    "        # 若序列较短，则补上 pad_token_id\n",
    "        # ----------------------------------------------\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 构造输入与目标\n",
    "        # 输入：从第一个 token 到倒数第二个 token\n",
    "        # 目标：从第二个 token 到最后一个 token\n",
    "        # 即向右平移一个位置，用于语言模型预测下一个 token\n",
    "        # ----------------------------------------------\n",
    "        inputs = torch.tensor(padded[:-1])  # 输入序列\n",
    "        targets = torch.tensor(padded[1:])  # 对应目标序列\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # 加入列表以便后续堆叠成张量\n",
    "        # ----------------------------------------------\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 4️⃣ 将所有序列堆叠成批次张量，并放到目标设备上\n",
    "    # ----------------------------------------------------------\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 5️⃣ 返回两个张量\n",
    "    # inputs_tensor 形状: [batch_size, seq_len]\n",
    "    # targets_tensor 形状: [batch_size, seq_len]\n",
    "    # ----------------------------------------------------------\n",
    "    return inputs_tensor, targets_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eb2bce3-28a7-4f39-9d4b-5e972d69066c",
    "outputId": "3d104439-c328-431b-ef7c-2639d86c2135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15",
   "metadata": {
    "id": "3bf85703-a0e0-42aa-8f29-cbc28dbf4e15"
   },
   "source": [
    "- 接下来，我们引入一个 `ignore_index` 值，用来将所有填充 token ID 替换为一个新值；`ignore_index` 的目的是在计算损失函数时忽略填充值（稍后会详细说明）。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/11.webp?1\" width=400px>\n",
    "\n",
    "- 具体来说，这意味着我们将对应 `50256` 的 token ID 替换为 `-100`，如下图所示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bed33-956e-4b3f-a09c-586d8203109a",
   "metadata": {
    "id": "bd4bed33-956e-4b3f-a09c-586d8203109a"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/12.webp?2\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346513e-c3f4-44fe-af22-4ebd36497728",
   "metadata": {
    "id": "5346513e-c3f4-44fe-af22-4ebd36497728"
   },
   "source": [
    "- （此外，我们还引入了 `allowed_max_length`，以便在需要时限制样本长度；如果你计划使用比 GPT-2 模型支持的 1024 token 上下文长度更长的数据集，这将非常有用。）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2",
   "metadata": {
    "id": "41ec6e2d-9eb2-4124-913e-d2af39be4cf2"
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 自定义批处理函数（collate function）示例\n",
    "# ===============================\n",
    "import torch  # 导入 PyTorch 库，用于张量操作和 GPU 加速\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,                    # 一个 batch（列表），每个元素是单条序列（通常为 token id 的列表）\n",
    "    pad_token_id=50256,       # 用于 padding 的 token id，默认使用 GPT 系列模型的 <|endoftext|> token id\n",
    "    ignore_index=-100,        # 用于在计算 loss 时忽略的索引，通常传给 CrossEntropyLoss\n",
    "    allowed_max_length=None,  # 可选：允许的最大序列长度，如果设置则会截断序列\n",
    "    device=\"cpu\"              # 输出张量所在设备，可设置为 \"cpu\" 或 \"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    自定义 collate 函数：\n",
    "    1. 将 batch 中的序列 padding 到统一长度\n",
    "    2. 构建模型输入 inputs 和目标 targets\n",
    "    3. 对 targets 的 padding 部分应用 ignore_index\n",
    "    4. 可选截断到最大序列长度\n",
    "    \"\"\"\n",
    "\n",
    "    # ===============================\n",
    "    # 1. 找出 batch 中序列的最大长度（加 1 是为了给每条序列添加一个 <|endoftext|> token）\n",
    "    # ===============================\n",
    "    batch_max_length = max(len(item)+1 for item in batch)  \n",
    "    # len(item)+1：每条序列长度 +1，用于后续添加结束 token\n",
    "    # max(...)：取整个 batch 中最长的序列长度，保证 padding 后每条序列长度一致\n",
    "\n",
    "    # ===============================\n",
    "    # 2. 初始化输入列表和目标列表\n",
    "    # ===============================\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    # ===============================\n",
    "    # 3. 遍历 batch 中每条序列进行处理\n",
    "    # ===============================\n",
    "    for item in batch:\n",
    "        new_item = item.copy()  # 复制序列，避免修改原始数据\n",
    "        \n",
    "        # -------------------------------\n",
    "        # 3.1 在序列末尾添加 <|endoftext|> token\n",
    "        # -------------------------------\n",
    "        new_item += [pad_token_id]  # 这里使用 pad_token_id 作为结束 token\n",
    "        \n",
    "        # -------------------------------\n",
    "        # 3.2 对序列进行 padding，使长度统一为 batch_max_length\n",
    "        # -------------------------------\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        # padded = 原序列 + pad_token_id 重复填充到最大长度\n",
    "        # batch_max_length - len(new_item)：计算需要补多少个 pad token\n",
    "\n",
    "        # -------------------------------\n",
    "        # 3.3 构建 inputs 和 targets\n",
    "        # -------------------------------\n",
    "        inputs = torch.tensor(padded[:-1])   # inputs：去掉最后一个 token\n",
    "        targets = torch.tensor(padded[1:])   # targets：向右移动一位，用于语言模型预测下一个 token\n",
    "\n",
    "        # -------------------------------\n",
    "        # 3.4 将 targets 中除第一个 padding 外的 pad token 替换为 ignore_index\n",
    "        # -------------------------------\n",
    "        mask = targets == pad_token_id          # 找到 targets 中等于 pad_token_id 的位置\n",
    "        indices = torch.nonzero(mask).squeeze() # 获取索引\n",
    "        if indices.numel() > 1:                 # 如果 pad token 超过 1 个\n",
    "            targets[indices[1:]] = ignore_index # 除第一个 pad 外，其余替换为 ignore_index\n",
    "\n",
    "        # -------------------------------\n",
    "        # 3.5 可选：截断序列到允许的最大长度\n",
    "        # -------------------------------\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        # -------------------------------\n",
    "        # 3.6 将处理好的序列加入列表\n",
    "        # -------------------------------\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # ===============================\n",
    "    # 4. 将列表转换为 batch 张量，并移动到指定设备\n",
    "    # ===============================\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)   # 将列表堆叠成张量，形状为 [batch_size, seq_len]\n",
    "    targets_tensor = torch.stack(targets_lst).to(device) # 同上\n",
    "\n",
    "    # ===============================\n",
    "    # 5. 返回处理好的 inputs 和 targets\n",
    "    # ===============================\n",
    "    return inputs_tensor, targets_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdf5eec4-9ebe-4be0-9fca-9a47bee88fdc",
    "outputId": "e8f709b9-f4c5-428a-a6ac-2a4c1b9358ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7",
   "metadata": {
    "id": "26727c90-0d42-43b3-af21-0a66ad4fbbc7"
   },
   "source": [
    "- 让我们看看将 token ID 替换为 -100 的作用。  \n",
    "- 为了便于说明，假设我们有一个小型分类任务，包含 2 个类别标签，0 和 1，类似于第 6 章。  \n",
    "- 如果我们有如下 logits 值（模型最后一层的输出），则可以计算如下损失：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "W2jvh-OP9MFV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W2jvh-OP9MFV",
    "outputId": "ccb3a703-59a7-4258-8841-57959a016e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 定义模型输出 logits\n",
    "# -------------------------------\n",
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],  # 第 1 个训练样本的输出 logits，长度等于类别数（这里有 2 类）\n",
    "     [-0.5, 1.5]]  # 第 2 个训练样本的输出 logits\n",
    ")\n",
    "# 说明：\n",
    "# - logits 是模型在每个类别上的原始预测分数（未经过 softmax）\n",
    "# - 行表示 batch 中的样本，列表示类别\n",
    "# - cross_entropy 函数内部会自动对 logits 应用 softmax，因此这里不需要手动 softmax\n",
    "\n",
    "# -------------------------------\n",
    "# 定义真实标签 targets\n",
    "# -------------------------------\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "# 说明：\n",
    "# - targets 的长度等于 batch 大小\n",
    "# - 每个元素是对应样本的正确类别索引（整数，从 0 开始）\n",
    "# - 例如 targets_1[0]=0 对应 logits_1[0,0] 为正确类别\n",
    "\n",
    "# -------------------------------\n",
    "# 计算交叉熵损失\n",
    "# -------------------------------\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "# 参数说明：\n",
    "# - logits_1: 模型输出的 logits，形状 [batch_size, num_classes]\n",
    "# - targets_1: 真实类别索引，形状 [batch_size]\n",
    "# 功能：\n",
    "# - 对每个样本计算交叉熵损失\n",
    "# - 内部实现为先对 logits 做 softmax，然后计算 -log(p_true_class)\n",
    "# - 返回标量 loss，表示整个 batch 的平均损失\n",
    "\n",
    "# -------------------------------\n",
    "# 打印损失值\n",
    "# -------------------------------\n",
    "print(loss_1)\n",
    "# 输出示例：\n",
    "# tensor(0.6265)（数值可能略有差异）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd3244-8886-4505-92e9-367d28529e1e",
   "metadata": {
    "id": "5edd3244-8886-4505-92e9-367d28529e1e"
   },
   "source": [
    "- 现在，再添加一个训练示例，正如预期的那样，会影响损失值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nvVMuil89v9N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvVMuil89v9N",
    "outputId": "6d4683d4-5bfc-4a8c-de2a-95ecb2e716b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 定义模型输出 logits\n",
    "# -------------------------------\n",
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],   # 第 1 个训练样本的输出 logits\n",
    "     [-0.5, 1.5],   # 第 2 个训练样本的输出 logits\n",
    "     [-0.5, 1.5]]   # 第 3 个训练样本的输出 logits（新增）\n",
    ")\n",
    "# 说明：\n",
    "# - logits 是模型在每个类别上的原始预测分数（未经过 softmax）\n",
    "# - 行表示 batch 中的样本，列表示类别\n",
    "# - cross_entropy 会在内部对 logits 自动应用 softmax\n",
    "\n",
    "# -------------------------------\n",
    "# 定义真实标签 targets\n",
    "# -------------------------------\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "# 说明：\n",
    "# - targets 的长度等于 batch 大小（这里 batch_size=3）\n",
    "# - 每个元素是对应样本的类别索引（从 0 开始）\n",
    "# - 例如 targets_2[0]=0 对应 logits_2[0,0] 为正确类别\n",
    "\n",
    "# -------------------------------\n",
    "# 计算交叉熵损失\n",
    "# -------------------------------\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "# 参数说明：\n",
    "# - logits_2: 模型输出的 logits，形状 [batch_size, num_classes]，这里为 [3,2]\n",
    "# - targets_2: 真实类别索引，形状 [batch_size]，这里为 [3]\n",
    "# 功能：\n",
    "# - 对每个样本计算交叉熵损失\n",
    "# - 内部实现为先对 logits 做 softmax，再计算 -log(p_true_class)\n",
    "# - 返回标量 loss，表示整个 batch 的平均损失\n",
    "\n",
    "# -------------------------------\n",
    "# 打印损失值\n",
    "# -------------------------------\n",
    "print(loss_2)\n",
    "# 输出示例：\n",
    "# tensor(0.6265)（数值可能略有差异）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dca331-40e0-468b-b690-189fe156ba8f",
   "metadata": {
    "id": "54dca331-40e0-468b-b690-189fe156ba8f"
   },
   "source": [
    "- 让我们看看如果将其中一个示例的类别标签替换为 -100，会发生什么。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "RTyB1vah9p56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTyB1vah9p56",
    "outputId": "da05302e-3fe0-439e-d1ed-82066bceb122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 定义包含 ignore_index 的真实标签 targets_3\n",
    "# -------------------------------\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "# 说明：\n",
    "# - targets_3 中的 -100 用作 ignore_index，表示计算 loss 时忽略该样本\n",
    "# - 其余元素表示对应样本的正确类别索引\n",
    "# - cross_entropy 函数允许 targets 中存在 ignore_index\n",
    "\n",
    "# -------------------------------\n",
    "# 计算交叉熵损失\n",
    "# -------------------------------\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "# 参数说明：\n",
    "# - logits_2: 模型输出 logits，形状 [3,2]\n",
    "# - targets_3: 真实类别索引，形状 [3]，其中 -100 表示忽略该样本\n",
    "# 功能：\n",
    "# - 对每个样本计算交叉熵损失\n",
    "# - 遇到 ignore_index（-100）的样本，该样本不会对 loss 贡献\n",
    "# - 返回标量 loss，表示有效样本的平均损失（忽略 ignore_index 样本）\n",
    "\n",
    "# -------------------------------\n",
    "# 打印损失值\n",
    "# -------------------------------\n",
    "print(loss_3)\n",
    "# 输出示例：\n",
    "# tensor(0.6265)（数值可能略有差异，与 loss_1 相同，因为第三个样本被忽略）\n",
    "\n",
    "# -------------------------------\n",
    "# 对比 loss_1 与 loss_3 是否相等\n",
    "# -------------------------------\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)\n",
    "# 说明：\n",
    "# - 由于 targets_3 的第三个样本被 ignore_index 忽略，实际计算 loss 时只考虑前两个样本\n",
    "# - 因此 loss_1 与 loss_3 数值相同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef09d21-b652-4760-abea-4f76920e6a25",
   "metadata": {
    "id": "cef09d21-b652-4760-abea-4f76920e6a25"
   },
   "source": [
    "- 如我们所见，这 3 个训练示例的损失与前面 2 个训练示例计算出的损失相同，这意味着交叉熵损失函数忽略了标签为 -100 的训练示例。  \n",
    "- 默认情况下，PyTorch 的 `cross_entropy(..., ignore_index=-100)` 设置会忽略对应标签为 -100 的示例。  \n",
    "- 利用这个 -100 `ignore_index`，我们可以忽略用于将训练示例填充到相同长度的批次中的额外 end-of-text（填充）token。  \n",
    "- 然而，我们不想忽略 end-of-text（填充）token（50256）的第一个实例，因为它可以帮助向 LLM 发出响应已完成的信号。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524",
   "metadata": {
    "id": "6a4e9c5f-7c49-4321-9f1b-a50468a84524"
   },
   "source": [
    "- 在实际操作中，也常常将对应于指令的目标 token ID 进行掩码处理，如下图所示（这是完成本章后推荐的读者练习）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39",
   "metadata": {
    "id": "fab8f0ed-80e8-4fd9-bf84-e5d0e0bc0a39"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/13.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96",
   "metadata": {
    "id": "bccaf048-ec95-498c-9155-d5b3ccba6c96"
   },
   "source": [
    "&nbsp;\n",
    "## 7.4 为指令数据集创建数据加载器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50",
   "metadata": {
    "id": "e6b8e656-3af3-4db6-8dde-d8c216a12f50"
   },
   "source": [
    "- 在本节中，我们使用 `InstructionDataset` 类和 `custom_collate_fn` 函数来实例化训练集、验证集和测试集的数据加载器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffe390-b226-4d5c-983f-9f4da773cb82",
   "metadata": {
    "id": "9fffe390-b226-4d5c-983f-9f4da773cb82"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/14.webp\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932677e9-9317-42e8-b461-7b0269518f97",
   "metadata": {
    "id": "932677e9-9317-42e8-b461-7b0269518f97"
   },
   "source": [
    "- 上一个 `custom_collate_fn` 函数的另一个细节是，我们现在直接将数据移动到目标设备（例如 GPU），而不是在主训练循环中执行，这提高了效率，因为当我们将 `custom_collate_fn` 用作数据加载器的一部分时，它可以作为后台进程执行。  \n",
    "- 使用 Python 标准库 `functools` 中的 `partial` 函数，我们可以创建一个新函数，将原函数的 `device` 参数预先填充好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "etpqqWh8phKc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etpqqWh8phKc",
    "outputId": "b4391c33-1a89-455b-faaa-5f874b6eb409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 定义计算设备（device）\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 说明：\n",
    "# - torch.device 表示 PyTorch 张量要放置在哪个设备上（CPU、GPU 或 Apple MPS）\n",
    "# - torch.cuda.is_available()：检查当前系统是否有可用的 NVIDIA GPU\n",
    "# - 如果有 GPU，则选择 \"cuda\"（即 GPU 设备）\n",
    "# - 否则使用 \"cpu\"\n",
    "# - 返回结果会是 torch.device(\"cuda\") 或 torch.device(\"cpu\")\n",
    "\n",
    "# -------------------------------\n",
    "# 说明（Note）：\n",
    "# 以下代码块提供了对 Apple Silicon（如 M1、M2、M3 芯片）的支持。\n",
    "# 如果取消注释（即删除井号 #），则代码会自动检测并使用 MPS（Metal Performance Shaders），\n",
    "# 在 Apple 芯片上运行速度通常比 CPU 更快。\n",
    "# 不过，由于硬件实现略有不同，计算得到的损失值可能会有极小的数值差异。\n",
    "# -------------------------------\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")   # 如果有 NVIDIA GPU，则使用 CUDA 加速\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")    # 如果有 Apple Silicon 芯片，则使用 MPS 加速\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")    # 否则使用 CPU 运行\n",
    "\n",
    "# -------------------------------\n",
    "# 打印当前选择的计算设备\n",
    "# -------------------------------\n",
    "print(\"Device:\", device)\n",
    "# 输出示例：\n",
    "# Device: cuda  （如果使用 NVIDIA GPU）\n",
    "# Device: mps   （如果使用 Apple Silicon）\n",
    "# Device: cpu   （如果没有 GPU）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
   "metadata": {
    "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
   },
   "outputs": [],
   "source": [
    "from functools import partial  # 从 Python 标准库导入 partial，用于创建“部分函数”\n",
    "\n",
    "# -------------------------------\n",
    "# 使用 partial 创建一个自定义的 collate 函数\n",
    "# -------------------------------\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,      # 原始函数：我们之前定义的 custom_collate_fn，用于批处理（collate）样本\n",
    "    device=device,          # 固定参数：指定张量移动到的计算设备（如 \"cpu\"、\"cuda\" 或 \"mps\"）\n",
    "    allowed_max_length=1024 # 固定参数：设置序列的最大允许长度（超过则截断）\n",
    ")\n",
    "# 说明：\n",
    "# - partial() 的作用是“预先填充”部分参数，生成一个新的函数。\n",
    "# - 新函数 customized_collate_fn() 的行为与 custom_collate_fn 相同，\n",
    "#   但其中 device 和 allowed_max_length 参数已经固定，无需在每次调用时重复传入。\n",
    "# - 这样可以方便地将 customized_collate_fn 作为 DataLoader 的 collate_fn 参数使用。\n",
    "\n",
    "# 示例：\n",
    "# DataLoader(dataset, batch_size=8, collate_fn=customized_collate_fn)\n",
    "# 在加载数据时，PyTorch 会自动调用该函数来对 batch 进行统一 padding、截断等处理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a",
   "metadata": {
    "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a"
   },
   "source": [
    "- 接下来，我们实例化数据加载器，类似于前几章，不过这次我们为批处理过程提供了自定义的 collate 函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "BtWkgir6Hlpe",
   "metadata": {
    "id": "BtWkgir6Hlpe"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader  # 从 PyTorch 工具库中导入 DataLoader，用于批量加载数据\n",
    "\n",
    "# -------------------------------\n",
    "# 定义 DataLoader 的关键参数\n",
    "# -------------------------------\n",
    "num_workers = 0  # 数据加载的子进程数，0 表示在主进程中加载数据（Windows 系统或调试时常用）\n",
    "batch_size = 8   # 每个 batch 的样本数量（即一次送入模型的样本数）\n",
    "\n",
    "# -------------------------------\n",
    "# 设置随机种子以确保实验结果可复现\n",
    "# -------------------------------\n",
    "torch.manual_seed(123)\n",
    "# 说明：\n",
    "# - 设置随机种子后，随机打乱（shuffle）或随机采样结果在每次运行中保持一致。\n",
    "# - 这样可以保证模型训练或数据处理的可重复性。\n",
    "\n",
    "# -------------------------------\n",
    "# 创建训练数据集对象\n",
    "# -------------------------------\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "# 参数说明：\n",
    "# - train_data：训练数据的原始样本（通常是文本指令、输入输出对等）\n",
    "# - tokenizer：分词器，用于将文本转换为 token id 列表\n",
    "# 功能：\n",
    "# - InstructionDataset 是一个自定义的 Dataset 类（需提前定义），\n",
    "#   负责将每条训练样本封装为可被 DataLoader 迭代的形式。\n",
    "\n",
    "# -------------------------------\n",
    "# 创建 DataLoader 对象\n",
    "# -------------------------------\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,             # 数据源：Dataset 对象\n",
    "    batch_size=batch_size,     # 每个 batch 的样本数量\n",
    "    collate_fn=customized_collate_fn,  # 自定义的批处理函数，用于对 batch 中样本进行 padding、截断、device 转移等处理\n",
    "    shuffle=True,              # 是否在每个 epoch 开始时打乱数据（True 可提高模型泛化能力）\n",
    "    drop_last=True,            # 若最后一个 batch 样本数不足 batch_size，则丢弃该 batch\n",
    "    num_workers=num_workers    # 数据加载时使用的子进程数，0 表示主线程加载\n",
    ")\n",
    "# 说明：\n",
    "# - DataLoader 负责从 Dataset 中按 batch 提取样本，并通过 collate_fn 进行统一处理。\n",
    "# - customized_collate_fn 由 partial() 创建，已自动固定 device 和 allowed_max_length 参数。\n",
    "# - shuffle=True 使每次迭代的数据顺序不同，有助于训练过程更稳定。\n",
    "# - drop_last=True 常用于确保 batch 大小一致，以避免最后一个 batch 数量过少影响训练。\n",
    "# - num_workers 可用于加速数据加载（Linux 环境常设为 2、4 或更高）。\n",
    "\n",
    "# -------------------------------\n",
    "# 现在 train_loader 可以直接在训练循环中使用，例如：\n",
    "# for batch_inputs, batch_targets in train_loader:\n",
    "#     ...\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
   "metadata": {
    "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 创建验证集 Dataset 对象\n",
    "# -------------------------------\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "# 参数说明：\n",
    "# - val_data：验证集原始数据（格式与训练集相同）\n",
    "# - tokenizer：分词器，用于将文本转换为 token id 序列\n",
    "# 功能：\n",
    "# - 将验证数据封装为 Dataset 对象，方便 DataLoader 批量读取\n",
    "\n",
    "# -------------------------------\n",
    "# 创建验证集 DataLoader\n",
    "# -------------------------------\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,                # 数据集来源\n",
    "    batch_size=batch_size,      # 每个 batch 的样本数量（与训练保持一致）\n",
    "    collate_fn=customized_collate_fn,  # 自定义 collate 函数，用于 padding、截断和 device 转移\n",
    "    shuffle=False,              # 验证集不需要打乱顺序，保持数据稳定\n",
    "    drop_last=False,            # 不丢弃最后一个 batch，即使样本数不足 batch_size\n",
    "    num_workers=num_workers     # 数据加载进程数（0 表示主进程加载）\n",
    ")\n",
    "# 说明：\n",
    "# - 验证集用于在训练过程中评估模型性能（不参与梯度更新）\n",
    "# - shuffle=False 是常规做法，保持验证集顺序固定，便于结果对比\n",
    "# - drop_last=False 保留所有样本，确保验证结果完整\n",
    "\n",
    "# -------------------------------\n",
    "# 创建测试集 Dataset 对象\n",
    "# -------------------------------\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "# 参数说明：\n",
    "# - test_data：测试集原始数据\n",
    "# - tokenizer：分词器（保持一致以保证输入格式匹配模型）\n",
    "# 功能：\n",
    "# - 将测试数据封装为 Dataset 对象，供最终性能评估使用\n",
    "\n",
    "# -------------------------------\n",
    "# 创建测试集 DataLoader\n",
    "# -------------------------------\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,               # 数据集来源\n",
    "    batch_size=batch_size,      # 每个 batch 的样本数量\n",
    "    collate_fn=customized_collate_fn,  # 自定义 collate 函数（保证与训练、验证一致）\n",
    "    shuffle=False,              # 测试集同样不打乱顺序\n",
    "    drop_last=False,            # 保留最后一个不满 batch 的样本\n",
    "    num_workers=num_workers     # 数据加载进程数\n",
    ")\n",
    "# 说明：\n",
    "# - 测试集 DataLoader 用于模型最终评估\n",
    "# - 不打乱顺序（shuffle=False），确保结果可重复、可比较\n",
    "# - 不丢弃最后一批样本，保证所有测试数据都参与评估\n",
    "\n",
    "# -------------------------------\n",
    "# 小结：\n",
    "# train_loader → 模型训练（shuffle=True）\n",
    "# val_loader   → 模型验证（shuffle=False）\n",
    "# test_loader  → 模型测试（shuffle=False）\n",
    "# 三者结构统一，方便代码复用。\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0",
   "metadata": {
    "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0"
   },
   "source": [
    "- 让我们来看一下生成的输入和目标批次的维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "GGs1AI3vHpnX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGs1AI3vHpnX",
    "outputId": "f6a74c8b-1af3-4bc1-b48c-eda64b0200d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# 打印训练集 DataLoader 的基本信息\n",
    "# -------------------------------------------\n",
    "\n",
    "print(\"Train loader:\")  \n",
    "# 打印提示文字，表示下面输出的是训练集数据加载器的内容\n",
    "\n",
    "# 使用 for 循环从 train_loader 中按批次读取数据\n",
    "# DataLoader 每次会返回一个 batch，其中包含：\n",
    "# - inputs：模型输入的 token 序列（经过 padding 后）\n",
    "# - targets：对应的目标输出序列（通常是要预测的 token）\n",
    "for inputs, targets in train_loader:\n",
    "    \n",
    "    # 打印每个 batch 中输入和目标张量的形状（shape）\n",
    "    # 通常为 (batch_size, sequence_length)\n",
    "    # 例如：torch.Size([8, 1024]) 表示每批 8 条样本，每条样本长度为 1024\n",
    "    print(inputs.shape, targets.shape)\n",
    "    \n",
    "# -------------------------------------------\n",
    "# 教学说明：\n",
    "# - train_loader 是通过 DataLoader 封装后的迭代器，每次迭代返回一个批次。\n",
    "# - 通过 print(inputs.shape, targets.shape) 可以检查：\n",
    "#     1️⃣ batch_size 是否设置正确；\n",
    "#     2️⃣ 序列长度是否符合 allowed_max_length（例如 1024）；\n",
    "#     3️⃣ inputs 和 targets 是否形状一致（除特殊任务外一般应一致）。\n",
    "# - 这是调试数据管线非常重要的一步。\n",
    "# -------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657",
   "metadata": {
    "id": "0c8e8dd7-d46a-4cc3-8a7e-c1d31e1b4657"
   },
   "source": [
    "- 如上面输出所示，所有批次的 batch size 都为 8，但长度各不相同，这是预期的结果。  \n",
    "- 我们还可以通过打印 `inputs` 批次中第一个训练示例的内容，来再次确认输入包含对应 token ID 50256 的 `<|endoftext|>` 填充 token。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21b8fd02-014f-4481-9b71-5bfee8f9dfcd",
    "outputId": "1b8ad342-2b5b-4f12-ad1a-3cb2a6c712ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
      "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
      "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360",
   "metadata": {
    "id": "5f1f3647-8971-4006-89e0-6a2a1ec1d360"
   },
   "source": [
    "- 同样地，我们可以通过可视化方式再次确认目标中包含 -100 占位符 token。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51649ab4-1a7e-4a9e-92c5-950a24fde211",
    "outputId": "5e8c23f8-6a05-4c13-9f92-373b75b57ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         2882,   326, 20431, 32543,   262,  2581,    13,   198,   198, 21017,\n",
      "        46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,   985,\n",
      "          576,    13,   198,   198, 21017, 23412,    25,   198,   464,  5156,\n",
      "          318,   845, 13779,    13,   198,   198, 21017, 18261,    25,   198,\n",
      "          464,  5156,   318,   355, 13779,   355,   257,  4936,    13, 50256,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aad445-8f19-4238-b9bf-db80767fb91a",
   "metadata": {
    "id": "d6aad445-8f19-4238-b9bf-db80767fb91a"
   },
   "source": [
    "## 7.5 加载预训练的大型语言模型（LLM）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b",
   "metadata": {
    "id": "5a5c07d1-4fc9-4846-94cf-b11a085a667b"
   },
   "source": [
    "- 在本节中，我们使用与第 5 章 5.5 节和第 6 章 6.4 节相同的代码加载一个预训练的 GPT 模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4",
   "metadata": {
    "id": "8d1b438f-88af-413f-96a9-f059c6c55fc4"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/15.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2",
   "metadata": {
    "id": "8c68eda7-e02e-4caa-846b-ca6dbd396ca2"
   },
   "source": [
    "- 但是，我们不是加载最小的 1.24 亿参数模型，而是加载中等规模的 3.55 亿参数模型，因为 1.24 亿参数模型对于通过指令微调获得合理的结果来说太小了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
    "outputId": "386ebd49-51d7-4a62-c590-91cdccce5fb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 95.5kiB/s]\n",
      "encoder.json: 100%|██████████████████████████████████████████████████████████████| 1.04M/1.04M [00:00<00:00, 1.40MiB/s]\n",
      "hparams.json: 100%|█████████████████████████████████████████████████████████████████| 91.0/91.0 [00:00<00:00, 136kiB/s]\n",
      "model.ckpt.data-00000-of-00001:  86%|█████████████████████████████████████▉      | 1.22G/1.42G [09:26<01:30, 2.16MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary URL (https://openaipublic.blob.core.windows.net/gpt-2/models\\355M\\model.ckpt.data-00000-of-00001) failed. Attempting backup URL: https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "Failed to download from both primary URL (https://openaipublic.blob.core.windows.net/gpt-2/models\\355M\\model.ckpt.data-00000-of-00001) and backup URL (https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\\355M\\model.ckpt.data-00000-of-00001).\n",
      "Check your internet connection or the file availability.\n",
      "For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.ckpt.index: 100%|██████████████████████████████████████████████████████████| 10.4k/10.4k [00:00<00:00, 6.74MiB/s]\n",
      "model.ckpt.meta: 100%|█████████████████████████████████████████████████████████████| 927k/927k [00:00<00:00, 1.04MiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:01<00:00, 329kiB/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Read fewer bytes than requested",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m\n\u001b[0;32m     63\u001b[0m model_size \u001b[38;5;241m=\u001b[39m CHOOSE_MODEL\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 解释：\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# CHOOSE_MODEL = \"gpt2-medium (355M)\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# 通过字符串操作提取括号中的“355M”，用于后续下载正确的权重文件。\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# ⬇️ 下载并加载 GPT-2 模型的权重参数\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m settings, params \u001b[38;5;241m=\u001b[39m download_and_load_gpt2(\n\u001b[0;32m     74\u001b[0m     model_size\u001b[38;5;241m=\u001b[39mmodel_size,   \u001b[38;5;66;03m# 传入上面提取的模型大小（如 \"355M\"）\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     models_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m        \u001b[38;5;66;03m# 指定权重文件存放目录（若不存在会自动创建）\u001b[39;00m\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# 返回两个对象：\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# - settings：模型配置信息（例如层数、隐藏维度等）\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# - params：GPT-2 预训练权重参数字典（用于加载到模型中）\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# 🧩 初始化 GPT 模型结构\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m     86\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTModel(BASE_CONFIG)\n",
      "File \u001b[1;32m~\\AI\\LLMs-from-scratch-main\\LLMs-from-scratch-main\\ch07\\01_main-chapter-code\\gpt_download.py:43\u001b[0m, in \u001b[0;36mdownload_and_load_gpt2\u001b[1;34m(model_size, models_dir)\u001b[0m\n\u001b[0;32m     41\u001b[0m tf_ckpt_path \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlatest_checkpoint(model_dir)\n\u001b[0;32m     42\u001b[0m settings \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparams.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 43\u001b[0m params \u001b[38;5;241m=\u001b[39m load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m settings, params\n",
      "File \u001b[1;32m~\\AI\\LLMs-from-scratch-main\\LLMs-from-scratch-main\\ch07\\01_main-chapter-code\\gpt_download.py:102\u001b[0m, in \u001b[0;36mload_gpt2_params_from_tf_ckpt\u001b[1;34m(ckpt_path, settings)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Iterate over each variable in the checkpoint\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlist_variables(ckpt_path):\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# Load the variable and remove singleton dimensions\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     variable_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mload_variable(ckpt_path, name))\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# Process the variable name to extract relevant parts\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     variable_name_parts \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip the 'model/' prefix\u001b[39;00m\n",
      "File \u001b[1;32mD:\\AI\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py:114\u001b[0m, in \u001b[0;36mload_variable\u001b[1;34m(ckpt_dir_or_file, name)\u001b[0m\n\u001b[0;32m    112\u001b[0m   name \u001b[38;5;241m=\u001b[39m name[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    113\u001b[0m reader \u001b[38;5;241m=\u001b[39m load_checkpoint(ckpt_dir_or_file)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mget_tensor(name)\n",
      "File \u001b[1;32mD:\\AI\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py:66\u001b[0m, in \u001b[0;36mget_tensor\u001b[1;34m(self, tensor_str)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the tensor from the Checkpoint object.\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m CheckpointReader\u001b[38;5;241m.\u001b[39mCheckpointReader_GetTensor(\n\u001b[0;32m     67\u001b[0m       \u001b[38;5;28mself\u001b[39m, compat\u001b[38;5;241m.\u001b[39mas_bytes(tensor_str))\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mIndexError\u001b[0m: Read fewer bytes than requested"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📦 导入必要模块\n",
    "# ============================================================\n",
    "\n",
    "from gpt_download import download_and_load_gpt2           # 自定义函数，用于下载并加载 GPT-2 模型的权重参数\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt  # 从前几章定义的脚本中导入 GPT 模型结构类和加载权重的函数\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 💡 如果本地没有 `previous_chapters.py` 文件，\n",
    "# 可以直接从 pip 安装的 `llms-from-scratch` 包中导入。\n",
    "# （该包由 Sebastian Raschka 发布，用于学习从零构建 LLM）\n",
    "# 官方仓库：https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg\n",
    "# 例如：\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import download_and_load_gpt2, load_weights_into_gpt\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ⚙️ 定义 GPT 模型的基础配置 BASE_CONFIG\n",
    "# ============================================================\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # 词汇表大小（GPT-2 默认的 Byte Pair Encoding 词表大小为 50257）\n",
    "    \"context_length\": 1024,  # 模型最大上下文长度（即一次可处理的最大 token 数）\n",
    "    \"drop_rate\": 0.0,        # Dropout 概率（防止过拟合；GPT-2 预训练时通常设置为 0）\n",
    "    \"qkv_bias\": True         # 是否在 QKV（Query-Key-Value）线性层中启用偏置项\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🧠 定义不同规模 GPT-2 模型的超参数配置\n",
    "# ============================================================\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},    # 小型模型（1.24亿参数）\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},  # 中型模型（3.55亿参数）\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},   # 大型模型（7.74亿参数）\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},     # 超大模型（15.58亿参数）\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🎯 选择要加载的 GPT-2 模型类型\n",
    "# ============================================================\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"   # 选择 “gpt2-medium” 作为示例模型\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔧 将模型结构参数更新到基础配置 BASE_CONFIG 中\n",
    "# ============================================================\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "# 这行代码将 `model_configs` 中所选模型的配置（如 emb_dim, n_layers, n_heads）\n",
    "# 合并进 BASE_CONFIG，形成完整的模型结构定义。\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🗂️ 从模型名称字符串中提取模型大小信息（例如 \"355M\"）\n",
    "# ============================================================\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "# 解释：\n",
    "# CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "# 通过字符串操作提取括号中的“355M”，用于后续下载正确的权重文件。\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ⬇️ 下载并加载 GPT-2 模型的权重参数\n",
    "# ============================================================\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,   # 传入上面提取的模型大小（如 \"355M\"）\n",
    "    models_dir=\"gpt2\"        # 指定权重文件存放目录（若不存在会自动创建）\n",
    ")\n",
    "# 返回两个对象：\n",
    "# - settings：模型配置信息（例如层数、隐藏维度等）\n",
    "# - params：GPT-2 预训练权重参数字典（用于加载到模型中）\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🧩 初始化 GPT 模型结构\n",
    "# ============================================================\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "# 根据前面定义的 BASE_CONFIG 创建 GPT 模型实例，\n",
    "# 此时模型仅有随机初始化的参数，还未加载预训练权重。\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔁 将下载好的权重参数加载到模型中\n",
    "# ============================================================\n",
    "\n",
    "load_weights_into_gpt(model, params)\n",
    "# 这个函数会将 HuggingFace GPT-2 的权重映射到当前自定义 GPT 模型的结构中，\n",
    "# 保证参数名称和形状一一对应。\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 💤 将模型设置为评估模式\n",
    "# ============================================================\n",
    "\n",
    "model.eval();  \n",
    "# 切换为“评估模式”（evaluation mode）：\n",
    "# 1️⃣ 关闭 dropout、batchnorm 等随机行为；\n",
    "# 2️⃣ 确保推理时结果稳定、可重复。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5",
   "metadata": {
    "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5"
   },
   "source": [
    "- 在下一节开始微调模型之前，让我们先看看它在一个验证任务上的表现如何。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
    "outputId": "c1276a91-e7da-495b-be0f-70a96872dbe6"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🎲 固定随机数种子（保证结果可复现）\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# 作用：\n",
    "#   - 让 PyTorch 的随机行为（如随机初始化、数据洗牌等）保持一致；\n",
    "#   - 只要设置相同的随机种子，程序每次运行都会得到相同的结果；\n",
    "#   - 对调试和教学尤其有用（避免“每次结果都不一样”的困惑）。\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🧾 从验证集 (val_data) 中取出第一个样本，并格式化为模型输入\n",
    "# ============================================================\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "# 解释：\n",
    "#   - `val_data`：验证集数据列表（通常每个元素是一条指令/问答样本）；\n",
    "#   - `val_data[0]`：取出验证集中的第一个样本；\n",
    "#   - `format_input()`：自定义函数，用于将样本格式化成模型可读的字符串，\n",
    "#       通常包括拼接“指令 + 输入 + 输出模板”等；\n",
    "#       例如：\n",
    "#         {\"instruction\": \"翻译英文\", \"input\": \"Hello\", \"output\": \"\"}\n",
    "#         => 转换为字符串：\n",
    "#         \"### Instruction:\\n翻译英文\\n\\n### Input:\\nHello\\n\\n### Response:\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 👀 打印格式化后的输入文本\n",
    "# ============================================================\n",
    "\n",
    "print(input_text)\n",
    "# 打印结果以便检查 format_input() 的输出是否正确，\n",
    "# 通常应当显示为模型的 prompt（提示词）形式，\n",
    "# 例如：\n",
    "#   ### Instruction:\n",
    "#   Translate the sentence into Chinese.\n",
    "#\n",
    "#   ### Input:\n",
    "#   The sun rises in the east.\n",
    "#\n",
    "#   ### Response:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa",
   "metadata": {
    "id": "2e3e68e0-2627-4c65-b4e7-1e0667e4f6fa"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📦 导入 GPT 推理相关函数\n",
    "# ============================================================\n",
    "\n",
    "from previous_chapters import (\n",
    "    generate,             # 核心生成函数，用于从模型生成 token 序列\n",
    "    text_to_token_ids,    # 将文本转换为 token id 序列的函数\n",
    "    token_ids_to_text     # 将生成的 token id 序列还原为文本的函数\n",
    ")\n",
    "# ⚠️ 替代方案：\n",
    "# 如果本地没有 previous_chapters.py 文件，可以从 llms-from-scratch 包中导入：\n",
    "# from llms_from_scratch.ch05 import generate, text_to_token_ids, token_ids_to_text\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔢 将输入文本转换为 token id 序列\n",
    "# ============================================================\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,                                         # 已加载权重的 GPT 模型\n",
    "    idx=text_to_token_ids(input_text, tokenizer),       # 将输入文本 input_text 转为 token id 序列\n",
    "    max_new_tokens=35,                                  # 本次生成的最大 token 数量（生成长度限制）\n",
    "    context_size=BASE_CONFIG[\"context_length\"],         # 上下文窗口长度（context window），决定模型能“看到”多少历史 token\n",
    "    eos_id=50256,                                       # 结束符 token id（遇到此 token 时停止生成）\n",
    ")\n",
    "# 说明：\n",
    "# - text_to_token_ids() 会将字符串拆分为模型词表对应的 id 序列；\n",
    "# - generate() 会根据输入 token 序列 idx 迭代生成新的 token；\n",
    "# - max_new_tokens 控制生成文本的长度，防止生成无限长文本；\n",
    "# - context_size 保证模型的输入长度不会超过训练时的最大 context；\n",
    "# - eos_id 可用于在生成到结尾符时停止，保证生成文本完整且合理。\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔄 将生成的 token id 序列还原为可读文本\n",
    "# ============================================================\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "# 说明：\n",
    "# - token_ids_to_text() 会将模型生成的 token id 序列转换为字符串；\n",
    "# - 得到的 generated_text 即模型根据 input_text 生成的自然语言文本输出；\n",
    "# - 该文本可直接打印或进一步用于评估和展示。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2fda5-f796-4954-8f72-1dd1123e3344",
   "metadata": {
    "id": "36e2fda5-f796-4954-8f72-1dd1123e3344"
   },
   "source": [
    "- 请注意，我们在前几章使用的 `generate` 函数会返回合并后的输入和输出文本，这在上一节用于生成可读文本时非常方便。  \n",
    "- 为了单独提取模型的响应，我们可以从 `generated_text` 的开头减去指令的长度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
    "outputId": "3e231f03-c5dc-4397-8778-4995731176a3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📝 提取模型生成的回答文本\n",
    "# ============================================================\n",
    "\n",
    "response_text = (\n",
    "    generated_text[len(input_text):]  # 从生成文本中去掉原始输入部分\n",
    "    .replace(\"### Response:\", \"\")     # 移除模板中可能存在的 \"### Response:\" 标签\n",
    "    .strip()                          # 去除文本首尾多余空格或换行\n",
    ")\n",
    "# 说明：\n",
    "# 1️⃣ generated_text 包含整个输入文本 + 模型生成的文本；\n",
    "# 2️⃣ len(input_text) 用于计算原始输入长度，从而截取生成部分；\n",
    "# 3️⃣ replace(\"### Response:\", \"\") 用于清理生成文本中的模板标记；\n",
    "# 4️⃣ strip() 保证输出文本整洁，没有多余空格或换行符。\n",
    "\n",
    "# ============================================================\n",
    "# 🖨️ 打印提取后的模型回答\n",
    "# ============================================================\n",
    "\n",
    "print(response_text)\n",
    "# 输出示例：\n",
    "# 模型生成的自然语言回答（仅包含响应部分，不包含原始输入或模板标签）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44080b2-a4c5-4520-a797-549519f66a3e",
   "metadata": {
    "id": "d44080b2-a4c5-4520-a797-549519f66a3e"
   },
   "source": [
    "- 如我们所见，该模型尚无法按照指令执行；它会生成一个“Response”部分，但仅仅是重复原始输入句子和指令内容。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d27b9d-a942-4cf5-b797-848c5f01e723",
   "metadata": {
    "id": "70d27b9d-a942-4cf5-b797-848c5f01e723"
   },
   "source": [
    "## 7.6 在指令数据上微调大型语言模型（LLM）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a",
   "metadata": {
    "id": "314b2a39-88b4-44d8-8c85-1c5b0cd6cc4a"
   },
   "source": [
    "- 在本节中，我们对模型进行微调。\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/16.webp\" width=500px>\n",
    "\n",
    "- 注意，我们可以重用前几章中使用的所有损失计算和训练函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65444865-df87-4d98-9faf-875e1c4be860",
   "metadata": {
    "id": "65444865-df87-4d98-9faf-875e1c4be860"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📦 导入训练相关函数\n",
    "# ============================================================\n",
    "\n",
    "from previous_chapters import (\n",
    "    calc_loss_loader,   # 计算 DataLoader 上模型平均损失的函数\n",
    "    train_model_simple  # 简化版训练函数，用于训练 GPT 模型\n",
    ")\n",
    "\n",
    "# ⚠️ 替代方案：\n",
    "# 如果本地没有 previous_chapters.py 文件，\n",
    "# 可以从 pip 安装的 llms-from-scratch 包中导入：\n",
    "# from llms_from_scratch.ch05 import (\n",
    "#    calc_loss_loader,    # 计算损失\n",
    "#    train_model_simple,  # 简化训练函数\n",
    "# )\n",
    "# 说明：\n",
    "# - calc_loss_loader() 接受一个 DataLoader 和模型实例，返回平均交叉熵损失；\n",
    "# - train_model_simple() 是一个轻量化训练循环封装，通常用于教学或快速实验。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00",
   "metadata": {
    "id": "00083059-aa41-4d37-8a17-1c72d1b1ca00"
   },
   "source": [
    "- 在开始训练之前，让我们计算初始的训练集和验证集损失（与前几章一样，目标是最小化损失）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
    "outputId": "a3f5e1b0-093a-4c51-e7fc-c9cac48c2ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.8259087562561036\n",
      "Validation loss: 3.761933708190918\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🖥️ 将模型移动到指定计算设备（CPU 或 GPU）\n",
    "# ============================================================\n",
    "\n",
    "model.to(device)\n",
    "# 说明：\n",
    "# - device 是之前定义的计算设备，例如 \"cpu\"、\"cuda\" 或 \"mps\"\n",
    "# - .to(device) 会将模型的所有参数和缓冲区移动到该设备\n",
    "# - 这样在后续训练或推理时，输入张量也需在同一设备，否则会报错\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🎯 设置随机数种子以保证结果可复现\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# 作用：\n",
    "# - 固定 PyTorch 的随机数生成器\n",
    "# - 确保每次计算 loss 时（例如 dropout 或权重初始化）得到一致结果\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 💡 在不计算梯度的上下文中计算训练和验证损失\n",
    "# ============================================================\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 说明：\n",
    "    # - torch.no_grad() 上下文管理器用于禁用梯度计算\n",
    "    # - 避免内存占用增加，也避免意外修改模型参数\n",
    "    # - 通常在验证集或快速测试时使用\n",
    "\n",
    "    train_loss = calc_loss_loader(\n",
    "        train_loader,  # 训练集 DataLoader\n",
    "        model,         # 已加载权重的 GPT 模型\n",
    "        device,        # 计算设备\n",
    "        num_batches=5  # 仅计算前 5 个 batch 的平均损失，用于快速估计\n",
    "    )\n",
    "\n",
    "    val_loss = calc_loss_loader(\n",
    "        val_loader,    # 验证集 DataLoader\n",
    "        model,\n",
    "        device,\n",
    "        num_batches=5  # 同样仅取前 5 个 batch\n",
    "    )\n",
    "# 输出：\n",
    "# - train_loss：训练集的平均交叉熵损失\n",
    "# - val_loss：验证集的平均交叉熵损失\n",
    "# ⚠️ 注意：这里只是快速检查损失，实际训练时应遍历整个数据集\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🖨️ 打印训练集和验证集的损失\n",
    "# ============================================================\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "# 输出格式示例：\n",
    "# Training loss: 2.34\n",
    "# Validation loss: 2.56\n",
    "# 可用于评估模型当前参数在训练和验证集上的表现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9",
   "metadata": {
    "id": "12a6da8f-15b3-42b0-a136-619b7a35c3e9"
   },
   "source": [
    "- 请注意，由于我们使用的是更大的模型（3.55 亿参数，而不是 1.24 亿参数），训练比前几章稍微耗费更多计算资源。  \n",
    "- 下表显示了在不同设备上的运行时间，仅供参考（在兼容 GPU 的设备上运行此笔记本无需修改代码）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b57fb-e689-4550-931c-6d34a932487c",
   "metadata": {
    "id": "db4b57fb-e689-4550-931c-6d34a932487c"
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    \n",
    "| 模型               | 设备                  | 两轮训练运行时间       |\n",
    "|--------------------|-----------------------|----------------------|\n",
    "| gpt2-medium (355M) | CPU (M3 MacBook Air)  | 15.78 分钟           |\n",
    "| gpt2-medium (355M) | GPU (M3 MacBook Air)  | 10.77 分钟           |\n",
    "| gpt2-medium (355M) | GPU (L4)              | 1.83 分钟            |\n",
    "| gpt2-medium (355M) | GPU (A100)            | 0.86 分钟            |\n",
    "| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 分钟            |\n",
    "| gpt2-small (124M)  | GPU (M3 MacBook Air)  | 3.73 分钟            |\n",
    "| gpt2-small (124M)  | GPU (L4)              | 0.69 分钟            |\n",
    "| gpt2-small (124M)  | GPU (A100)            | 0.39 分钟            |\n",
    "\n",
    "</div>\n",
    "\n",
    "- 我使用 `\"gpt2-medium (355M)\"` 模型运行了本笔记本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
    "outputId": "ecb9a3dd-97c0-492d-8a51-fbd175bb139b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
      "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103\n",
      "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
      "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
      "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
      "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
      "Ep 1 (Step 000030): Train loss 0.800, Val loss 0.836\n",
      "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.809\n",
      "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
      "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
      "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.782\n",
      "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
      "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
      "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
      "Ep 1 (Step 000070): Train loss 0.536, Val loss 0.732\n",
      "Ep 1 (Step 000075): Train loss 0.569, Val loss 0.739\n",
      "Ep 1 (Step 000080): Train loss 0.603, Val loss 0.734\n",
      "Ep 1 (Step 000085): Train loss 0.518, Val loss 0.717\n",
      "Ep 1 (Step 000090): Train loss 0.575, Val loss 0.699\n",
      "Ep 1 (Step 000095): Train loss 0.505, Val loss 0.689\n",
      "Ep 1 (Step 000100): Train loss 0.507, Val loss 0.683\n",
      "Ep 1 (Step 000105): Train loss 0.570, Val loss 0.676\n",
      "Ep 1 (Step 000110): Train loss 0.564, Val loss 0.671\n",
      "Ep 1 (Step 000115): Train loss 0.522, Val loss 0.666\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
      "Ep 2 (Step 000120): Train loss 0.439, Val loss 0.671\n",
      "Ep 2 (Step 000125): Train loss 0.454, Val loss 0.685\n",
      "Ep 2 (Step 000130): Train loss 0.448, Val loss 0.681\n",
      "Ep 2 (Step 000135): Train loss 0.406, Val loss 0.678\n",
      "Ep 2 (Step 000140): Train loss 0.412, Val loss 0.678\n",
      "Ep 2 (Step 000145): Train loss 0.372, Val loss 0.680\n",
      "Ep 2 (Step 000150): Train loss 0.381, Val loss 0.674\n",
      "Ep 2 (Step 000155): Train loss 0.419, Val loss 0.672\n",
      "Ep 2 (Step 000160): Train loss 0.417, Val loss 0.680\n",
      "Ep 2 (Step 000165): Train loss 0.383, Val loss 0.683\n",
      "Ep 2 (Step 000170): Train loss 0.328, Val loss 0.679\n",
      "Ep 2 (Step 000175): Train loss 0.334, Val loss 0.668\n",
      "Ep 2 (Step 000180): Train loss 0.391, Val loss 0.656\n",
      "Ep 2 (Step 000185): Train loss 0.418, Val loss 0.657\n",
      "Ep 2 (Step 000190): Train loss 0.341, Val loss 0.648\n",
      "Ep 2 (Step 000195): Train loss 0.330, Val loss 0.633\n",
      "Ep 2 (Step 000200): Train loss 0.313, Val loss 0.631\n",
      "Ep 2 (Step 000205): Train loss 0.354, Val loss 0.628\n",
      "Ep 2 (Step 000210): Train loss 0.365, Val loss 0.629\n",
      "Ep 2 (Step 000215): Train loss 0.394, Val loss 0.634\n",
      "Ep 2 (Step 000220): Train loss 0.301, Val loss 0.647\n",
      "Ep 2 (Step 000225): Train loss 0.347, Val loss 0.661\n",
      "Ep 2 (Step 000230): Train loss 0.297, Val loss 0.659\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
      "Training completed in 0.93 minutes.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ⏱️ 导入时间模块，用于计算训练耗时\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()  # 记录训练开始时间（秒级时间戳）\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🎯 固定随机数种子，保证训练可复现\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# 说明：\n",
    "# - 固定 PyTorch 随机种子，确保每次训练过程（权重初始化、dropout、采样等）一致\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔧 定义优化器\n",
    "# ============================================================\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),  # 模型参数列表，用于优化\n",
    "    lr=0.00005,          # 学习率（learning rate），控制参数更新步长\n",
    "    weight_decay=0.1     # 权重衰减（L2 正则化），防止过拟合\n",
    ")\n",
    "# 说明：\n",
    "# - AdamW 是 Adam 优化器的改进版，内置权重衰减\n",
    "# - 常用于 Transformer / GPT 模型训练\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔄 设置训练轮数\n",
    "# ============================================================\n",
    "\n",
    "num_epochs = 2  # 训练总轮数（整个训练集遍历次数）\n",
    "# ⚠️ 教学/演示时可用较小轮数，实际训练通常需要几十轮或更多\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🚀 调用训练函数进行模型训练\n",
    "# ============================================================\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,                  # 已初始化的 GPT 模型\n",
    "    train_loader,           # 训练集 DataLoader\n",
    "    val_loader,             # 验证集 DataLoader\n",
    "    optimizer,              # 优化器\n",
    "    device,                 # 计算设备（CPU / GPU / MPS）\n",
    "    num_epochs=num_epochs,  # 总训练轮数\n",
    "    eval_freq=5,            # 每隔多少 batch 进行一次训练/验证损失评估\n",
    "    eval_iter=5,            # 每次评估只计算前 eval_iter 个 batch（快速估计）\n",
    "    start_context=format_input(val_data[0]),  # 可选初始上下文文本，用于生成示例或 warm-up\n",
    "    tokenizer=tokenizer     # 分词器，用于将文本与 token id 相互转换\n",
    ")\n",
    "# 输出：\n",
    "# - train_losses: 每次 eval 时的训练集损失列表\n",
    "# - val_losses: 每次 eval 时的验证集损失列表\n",
    "# - tokens_seen: 已训练的 token 数量列表（用于绘制学习曲线或分析训练进度）\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ⏳ 计算训练总耗时\n",
    "# ============================================================\n",
    "\n",
    "end_time = time.time()  # 记录训练结束时间\n",
    "execution_time_minutes = (end_time - start_time) / 60  # 转换为分钟\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "# 输出示例：\n",
    "# Training completed in 12.34 minutes.\n",
    "# 可用于评估训练效率和性能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ise3wGjlB-iq",
   "metadata": {
    "id": "Ise3wGjlB-iq"
   },
   "source": [
    "- 从上面的输出可以看出，模型训练得很好，这可以从训练损失和验证损失的下降趋势看出。\n",
    "- 此外，根据每个 epoch 后打印的响应文本，我们可以看到模型能够正确地遵循指令，将输入句子 `'The chef cooks the meal every day.'` 转换为被动语态 `'The meal is cooked every day by the chef.'`（我们将在后面的章节中对响应进行格式化和评估）。\n",
    "- 最后，让我们来看一下训练和验证损失曲线。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
    "outputId": "2f5c99e0-7ed0-4f42-d67c-e07c375e6158"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📦 导入绘图函数\n",
    "# ============================================================\n",
    "\n",
    "from previous_chapters import plot_losses\n",
    "# ⚠️ 替代方案：\n",
    "# 如果本地没有 previous_chapters.py 文件，可从 llms-from-scratch 包导入：\n",
    "# from llms_from_scratch.ch05 import plot_losses\n",
    "# 说明：\n",
    "# - plot_losses() 用于绘制训练集与验证集损失随训练进度变化的曲线\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔢 构造用于绘图的 epoch 张量\n",
    "# ============================================================\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "# 说明：\n",
    "# - torch.linspace(start, end, steps) 生成从 start 到 end 的均匀间隔张量\n",
    "# - start=0，end=num_epochs，steps=len(train_losses)\n",
    "# - 作用：将每次记录的训练损失映射到对应的 epoch 位置，便于绘制曲线\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 📈 绘制训练和验证损失曲线\n",
    "# ============================================================\n",
    "\n",
    "plot_losses(\n",
    "    epochs_tensor,  # x 轴：训练轮数或评估时对应的 epoch\n",
    "    tokens_seen,    # 可选参数：已训练 token 数量，作为辅助信息\n",
    "    train_losses,   # y1 轴：训练集损失\n",
    "    val_losses      # y2 轴：验证集损失\n",
    ")\n",
    "# 功能：\n",
    "# - 将训练过程中的损失变化可视化\n",
    "# - 便于观察模型收敛情况、是否过拟合或欠拟合\n",
    "# - train_losses 与 val_losses 的对比曲线是分析模型性能的关键\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0",
   "metadata": {
    "id": "6777e0c4-d82c-46d8-84fb-1376c4f8bae0"
   },
   "source": [
    "- 如我们所见，在第一个 epoch 开始时，损失迅速下降，这意味着模型开始快速学习。\n",
    "- 我们可以看到，大约在训练 1 个 epoch 时，出现了轻微的过拟合现象。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3",
   "metadata": {
    "id": "87b79a47-13f9-4d1f-87b1-3339bafaf2a3"
   },
   "source": [
    "## 7.7 提取并保存模型生成的响应\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49",
   "metadata": {
    "id": "5a25cc88-1758-4dd0-b8bf-c044cbf2dd49"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/18.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427",
   "metadata": {
    "id": "17510e9d-7727-4d58-ba9a-d82ec23c1427"
   },
   "source": [
    "- 在本节中，我们将保存测试集的模型响应，以便在下一节进行评分\n",
    "- 我们还会保存一份模型副本，以便将来使用\n",
    "- 但首先，让我们简要看看微调后的模型生成的响应\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "VQ2NZMbfucAc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQ2NZMbfucAc",
    "outputId": "066c56ff-b52a-4ee6-eae7-1bddfc74d0c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🎯 固定随机数种子，保证结果可复现\n",
    "# ============================================================\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# 说明：\n",
    "# - 固定 PyTorch 随机种子，确保模型生成文本的随机性一致\n",
    "# - 对比实验或教学演示时非常重要\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔄 遍历测试集中的前 3 个样本\n",
    "# ============================================================\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    # entry 是一个字典，通常包含：\n",
    "    # - entry['instruction']: 指令文本\n",
    "    # - entry['input']: 输入文本（可选）\n",
    "    # - entry['output']: 正确输出文本\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 📝 将测试样本格式化为模型输入文本\n",
    "    # ============================================================\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "    # 作用：\n",
    "    # - 将指令、输入等字段组合成 GPT 模型可读的 prompt\n",
    "    # - 通常包含分隔符，例如 \"### Instruction:\" / \"### Input:\" / \"### Response:\"\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔢 将输入文本转换为 token id，并调用模型生成输出\n",
    "    # ============================================================\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,                                     # 已训练 GPT 模型\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),  # 文本 → token id → 移动到设备\n",
    "        max_new_tokens=256,                              # 本次生成最大 token 数量\n",
    "        context_size=BASE_CONFIG[\"context_length\"],      # 上下文窗口长度\n",
    "        eos_id=50256                                     # 遇到结束符时停止生成\n",
    "    )\n",
    "    # 输出：\n",
    "    # - token_ids：模型生成的 token id 序列\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔄 将生成的 token id 转换回可读文本\n",
    "    # ============================================================\n",
    "\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    # 功能：\n",
    "    # - token id → 可读文本\n",
    "    # - 包含原始输入 + 模型生成的响应文本\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 📝 提取模型实际生成的回答部分\n",
    "    # ============================================================\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]  # 去掉原始输入部分\n",
    "        .replace(\"### Response:\", \"\")     # 移除模板标签\n",
    "        .strip()                          # 去掉首尾空格\n",
    "    )\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🖨️ 打印输入文本、正确输出和模型生成的输出\n",
    "    # ============================================================\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")\n",
    "    # 输出示例：\n",
    "    # 1️⃣ 原始输入文本（包含指令/上下文）\n",
    "    # 2️⃣ 正确输出（测试集标注）\n",
    "    # 3️⃣ 模型生成输出\n",
    "    # 4️⃣ 分隔线，便于阅读每个样本的对比\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab64c1-586f-4939-8def-23feeb1b3599",
   "metadata": {
    "id": "49ab64c1-586f-4939-8def-23feeb1b3599"
   },
   "source": [
    "- 从测试集的指令、给定的答案以及模型生成的响应来看，模型表现相对不错\n",
    "- 第一条和最后一条指令的答案显然是正确的\n",
    "- 第二条答案接近正确；模型回答为“cumulus cloud”（积云）而不是“cumulonimbus”（积雨云）（不过请注意，积云可以发展成积雨云，而积雨云有能力产生雷暴）\n",
    "- 最重要的是，可以看出模型评估不像上一章那样简单，在上一章中，我们只需计算垃圾邮件/非垃圾邮件分类正确的百分比即可获得分类准确率\n",
    "- 在实际操作中，经过指令微调的 LLM（如聊天机器人）的评估方法有多种：\n",
    "  - 短答案和多选题基准测试，例如 MMLU（\"Measuring Massive Multitask Language Understanding\"，[https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)），用于测试模型的知识\n",
    "  - 与其他 LLM 的人工偏好比较，例如 LMSYS 聊天机器人竞技场 ([https://arena.lmsys.org](https://arena.lmsys.org))\n",
    "  - 自动化对话基准测试，其中使用另一个 LLM（如 GPT-4）评估响应，例如 AlpacaEval ([https://tatsu-lab.github.io/alpaca_eval/](https://tatsu-lab.github.io/alpaca_eval/))\n",
    "\n",
    "- 在下一节中，我们将使用类似 AlpacaEval 的方法，使用另一个 LLM 来评估我们模型的响应；不过，我们将使用自己的测试集，而不是公开的基准数据集\n",
    "- 为此，我们将模型响应添加到 `test_data` 字典中，并保存为 `\"instruction-data-with-response.json\"` 文件进行记录，以便在需要时可以在独立的 Python 会话中加载和分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "-PNGKzY4snKP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-PNGKzY4snKP",
    "outputId": "37b22a62-9860-40b7-c46f-b297782b944c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:20<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📦 导入 tqdm，用于显示进度条\n",
    "# ============================================================\n",
    "\n",
    "from tqdm import tqdm\n",
    "# 说明：\n",
    "# - tqdm 是 Python 的进度条库\n",
    "# - 在处理较大数据集时，可以实时显示循环进度，便于监控推理过程\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔄 遍历测试集，并使用 tqdm 显示进度条\n",
    "# ============================================================\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    # i: 当前样本索引\n",
    "    # entry: 测试样本字典\n",
    "    # total=len(test_data): tqdm 显示进度条总长度\n",
    "\n",
    "    # ============================================================\n",
    "    # 📝 格式化输入文本\n",
    "    # ============================================================\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "    # 将测试样本格式化为 GPT 模型可读的 prompt\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔢 将文本转换为 token id 并生成模型输出\n",
    "    # ============================================================\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,                                     # 已训练 GPT 模型\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),  # 文本 → token id → 移动到设备\n",
    "        max_new_tokens=256,                              # 最大生成 token 数\n",
    "        context_size=BASE_CONFIG[\"context_length\"],      # 上下文窗口长度\n",
    "        eos_id=50256                                     # 遇到结束符停止生成\n",
    "    )\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔄 将 token id 转换回可读文本\n",
    "    # ============================================================\n",
    "\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    # 输出包含原始输入 + 模型生成的文本\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 📝 提取模型生成的响应文本\n",
    "    # ============================================================\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]  # 去掉输入部分\n",
    "        .replace(\"### Response:\", \"\")     # 移除模板标签\n",
    "        .strip()                          # 去掉首尾空格\n",
    "    )\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 💾 将模型生成的响应保存回测试数据字典\n",
    "    # ============================================================\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "    # 每个样本新增一个字段 \"model_response\"，存储模型生成结果\n",
    "    # 便于后续分析、对比或保存到文件\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 📂 将包含模型响应的测试数据保存为 JSON 文件\n",
    "# ============================================================\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(\n",
    "        test_data,   # 要保存的数据\n",
    "        file,        # 文件对象\n",
    "        indent=4     # 缩进格式化，使 JSON 文件可读性更好\n",
    "    )\n",
    "# 说明：\n",
    "# - 保存后的文件每个样本包含原始输入、正确输出、以及模型生成的响应\n",
    "# - 可用于后续评估、展示或共享\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d6fa7-d162-44c3-bef1-4013c027b155",
   "metadata": {
    "id": "228d6fa7-d162-44c3-bef1-4013c027b155"
   },
   "source": [
    "- 我们再检查一个条目，以确认模型生成的回答是否已正确添加到 `test_data` 字典中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "u-AvCCMTnPSE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-AvCCMTnPSE",
    "outputId": "7bcd9600-1446-4829-b773-5259b13d256a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a",
   "metadata": {
    "id": "c1b2f3f6-8569-405a-9db6-d47cba65608a"
   },
   "source": [
    "- 最后，我们还将保存模型，以便将来如果需要可以重新使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cBU0iHmVfOI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cBU0iHmVfOI",
    "outputId": "135849ed-9acd-43a2-f438-053d07dae9b2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-medium355M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📦 导入正则表达式模块\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "# 说明：\n",
    "# - re 模块用于处理字符串模式匹配和替换\n",
    "# - 这里用于生成文件名时去掉不合法字符（如空格、括号）\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔧 构造模型保存文件名\n",
    "# ============================================================\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-sft.pth\"\n",
    "# 说明：\n",
    "# 1️⃣ CHOOSE_MODEL: 模型名称字符串，例如 \"gpt2-medium (355M)\"\n",
    "# 2️⃣ re.sub(r'[ ()]', '', CHOOSE_MODEL):\n",
    "#    - 使用正则表达式删除空格和括号\n",
    "#    - 结果: \"gpt2-medium355M\"\n",
    "# 3️⃣ f\"...-sft.pth\":\n",
    "#    - 将处理后的模型名称与后缀 \"-sft.pth\" 拼接\n",
    "#    - 最终文件名: \"gpt2-medium355M-sft.pth\"\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 💾 保存模型权重\n",
    "# ============================================================\n",
    "\n",
    "torch.save(model.state_dict(), file_name)\n",
    "# 说明：\n",
    "# - model.state_dict(): 获取模型的所有参数和缓冲区（字典形式）\n",
    "# - torch.save(..., file_name): 将模型权重保存为文件，可用于后续加载或共享\n",
    "# - 文件扩展名 \".pth\" 是 PyTorch 常用保存格式\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🖨️ 输出保存信息\n",
    "# ============================================================\n",
    "\n",
    "print(f\"Model saved as {file_name}\")\n",
    "# 输出示例：\n",
    "# Model saved as gpt2-medium355M-sft.pth\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ⚠️ 加载已保存模型权重示例\n",
    "# ============================================================\n",
    "\n",
    "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))\n",
    "# 说明：\n",
    "# - torch.load(...) 读取保存的权重字典\n",
    "# - model.load_state_dict(...) 将权重加载到模型中\n",
    "# - 加载后可继续训练或用于推理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obgoGI89dgPm",
   "metadata": {
    "id": "obgoGI89dgPm"
   },
   "source": [
    "## 7.8 评估微调后的 LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b9d30-7336-499f-abb5-4a21be3129f5",
   "metadata": {
    "id": "805b9d30-7336-499f-abb5-4a21be3129f5"
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/19.webp?1\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1",
   "metadata": {
    "id": "68d2b9d3-b6ff-4533-a89d-7b66079b4fd1"
   },
   "source": [
    "- 在本节中，我们使用另一个更大的 LLM 来自动评估微调后的 LLM 响应\n",
    "- 具体而言，我们使用 Meta AI 的指令微调版 80 亿参数 Llama 3 模型，该模型可以通过 ollama ([https://ollama.com](https://ollama.com)) 在本地运行\n",
    "- （或者，如果你希望使用更强大的 LLM，比如通过 OpenAI API 的 GPT-4，请参阅 [llm-instruction-eval-openai.ipynb](../03_model-evaluation/llm-instruction-eval-openai.ipynb) 笔记本）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9",
   "metadata": {
    "id": "ea427a30-36ba-44e3-bb1f-eb0d7008d6e9"
   },
   "source": [
    "- Ollama 是一个高效运行 LLM 的应用程序\n",
    "- 它是对 llama.cpp 的封装 ([https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp))，后者使用纯 C/C++ 实现 LLM，以最大化运行效率\n",
    "- 请注意，它是用于生成文本（推理）的工具，而不是用于训练或微调 LLM\n",
    "- 在运行下面代码之前，请访问 [https://ollama.com](https://ollama.com) 并按照说明安装 ollama（例如，点击“Download”按钮并下载适合你操作系统的 ollama 应用程序）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cd444-3156-46ad-8243-f9e7a55e66e7",
   "metadata": {},
   "source": [
    "- 对于 macOS 和 Windows 用户，点击你下载的 ollama 应用程序；如果提示是否安装命令行使用功能，请选择“yes”\n",
    "- Linux 用户可以使用 ollama 网站提供的安装命令\n",
    "\n",
    "- 一般来说，在命令行中使用 ollama 之前，我们必须要么启动 ollama 应用程序，要么在另一个终端中运行 `ollama serve`\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/20.webp?1\" width=700px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30266e32-63c4-4f6c-8be3-c99e05ed05b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**注意**：\n",
    "\n",
    "- 如上所述，在终端运行 `ollama serve` 时，可能会遇到错误信息 `Error: listen tcp 127.0.0.1:11434: bind: address already in use`\n",
    "- 如果出现这种情况，可以尝试使用命令 `OLLAMA_HOST=127.0.0.1:11435 ollama serve`（如果该地址也被占用，可以将端口号依次加 1，直到找到未被占用的地址）\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822",
   "metadata": {
    "id": "747a2fc7-282d-47ec-a987-ed0a23ed6822"
   },
   "source": [
    "- 在另一个终端中运行 ollama 应用程序或 `ollama serve` 后，在命令行执行以下命令以尝试 8 十亿参数的 Llama 3 模型（该模型首次执行命令时会自动下载，占用 4.7 GB 存储空间）\n",
    "\n",
    "```bash\n",
    "# 8B 模型\n",
    "ollama run llama3\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "输出示例如下：\n",
    "\n",
    "```\n",
    "$ ollama run llama3\n",
    "pulling manifest\n",
    "pulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB\n",
    "pulling 4fa551d4f938... 100% ▕████████████████▏  12 KB\n",
    "pulling 8ab4849b038c... 100% ▕████████████████▏  254 B\n",
    "pulling 577073ffcc6c... 100% ▕████████████████▏  110 B\n",
    "pulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B\n",
    "verifying sha256 digest\n",
    "writing manifest\n",
    "removing any unused layers\n",
    "success\n",
    "```\n",
    "\n",
    "注意，llama3 指的是经过指令微调的 8 十亿参数 Llama 3 模型\n",
    "\n",
    "使用 ollama 和 \"llama3\" 模型（8B 参数）需要 16 GB RAM；如果你的设备不支持，可以尝试更小的模型，例如 3.8B 参数的 phi-3 模型，设置 model = \"phi-3\"，只需要 8 GB RAM\n",
    "\n",
    "另外，如果设备支持，你也可以使用更大的 70 十亿参数 Llama 3 模型，只需将 llama3 替换为 llama3:70b\n",
    "\n",
    "下载完成后，你会看到一个命令行提示符，可以与模型进行交互\n",
    "\n",
    "尝试输入提示，例如 \"What do llamas eat?\"，输出示例如下：\n",
    "\n",
    "```\n",
    ">>> What do llamas eat?\n",
    "Llamas are ruminant animals, which means they have a four-chambered\n",
    "stomach and eat plants that are high in fiber. In the wild, llamas\n",
    "typically feed on:\n",
    "1. Grasses: They love to graze on various types of grasses, including tall\n",
    "grasses, wheat, oats, and barley.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4",
   "metadata": {
    "id": "7b7b341c-ba0e-40bb-a52c-cb328bbd1fe4"
   },
   "source": [
    "- 你可以使用输入 `/bye` 来结束本次会话\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3",
   "metadata": {
    "id": "faaf3e02-8ca0-4edf-be23-60625a5b14e3"
   },
   "source": [
    "- 以下代码用于在继续使用 ollama 评估上一节生成的测试集响应之前，检查 ollama 会话是否运行正常\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e8570-071e-48a2-aa38-64d7be35f288",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "026e8570-071e-48a2-aa38-64d7be35f288",
    "outputId": "e30d3533-e1f5-4aa9-b24f-33273fc7b30e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📦 导入 psutil 模块，用于系统进程管理\n",
    "# ============================================================\n",
    "\n",
    "import psutil\n",
    "# 说明：\n",
    "# - psutil 是 Python 的系统监控库\n",
    "# - 可获取系统进程、CPU、内存等信息\n",
    "# - 这里用于检查特定进程是否正在运行\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔍 定义函数：检查指定进程是否运行\n",
    "# ============================================================\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "    - 检查系统中是否存在指定名称的进程\n",
    "    参数：\n",
    "    - process_name (str): 要检查的进程名称，例如 \"ollama\"\n",
    "    返回：\n",
    "    - running (bool): True 表示进程正在运行，False 表示未运行\n",
    "    \"\"\"\n",
    "    running = False  # 初始化状态为未运行\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        # proc.info[\"name\"] 获取进程名称\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True  # 找到进程，设置为 True\n",
    "            break           # 找到一个即可，退出循环\n",
    "    return running\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔎 检查 Ollama 是否正在运行\n",
    "# ============================================================\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "# 说明：\n",
    "# - 调用自定义函数，返回 True/False\n",
    "# - 用于确保后续操作依赖 Ollama 服务时不会出错\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ⚠️ 如果 Ollama 未运行，则抛出异常\n",
    "# ============================================================\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "# 说明：\n",
    "# - RuntimeError 会中断程序\n",
    "# - 提示用户先启动 Ollama 服务\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🖨️ 输出 Ollama 是否运行状态\n",
    "# ============================================================\n",
    "\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))\n",
    "# 输出示例：\n",
    "# Ollama running: True\n",
    "# 可用于确认 Ollama 已经启动\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0",
   "metadata": {
    "id": "723c9b00-e3cd-4092-83c3-6e48b5cf65b0"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ⚠️ 可选单元格说明\n",
    "# ============================================================\n",
    "\n",
    "# 说明：\n",
    "# - 本单元格可用于重启 Notebook 后仅运行 7.7 节（测试/推理部分）\n",
    "# - 无需重新运行前面训练模型等耗时操作\n",
    "# - 适合教学演示或快速测试模型推理结果\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 📦 导入 JSON 和 tqdm 库\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# 说明：\n",
    "# - json: 用于加载和保存 JSON 文件\n",
    "# - tqdm: 用于显示循环进度条，方便观察处理进度\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 📂 指定包含模型生成响应的测试数据文件路径\n",
    "# ============================================================\n",
    "\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "# 说明：\n",
    "# - 文件中包含测试样本及模型生成的 \"model_response\" 字段\n",
    "# - 之前推理保存的 JSON 文件\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔄 从 JSON 文件加载测试数据\n",
    "# ============================================================\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "# 说明：\n",
    "# - json.load(file) 将 JSON 文件解析为 Python 列表或字典\n",
    "# - test_data: 列表，每个元素是一个字典，包含：\n",
    "#   - 'instruction': 指令文本\n",
    "#   - 'input': 输入文本\n",
    "#   - 'output': 正确输出\n",
    "#   - 'model_response': 模型生成的响应（如果已生成）\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 📝 定义函数：格式化输入文本以供模型推理\n",
    "# ============================================================\n",
    "\n",
    "def format_input(entry):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "    - 将单个测试样本 entry 格式化为 GPT 模型可读的 prompt\n",
    "    参数：\n",
    "    - entry: 测试样本字典，包含 'instruction' 和可选 'input'\n",
    "    返回：\n",
    "    - input_text: 拼接后的字符串，用于模型生成\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔹 构造 instruction 部分\n",
    "    # ============================================================\n",
    "\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    # 说明：\n",
    "    # - 模板说明任务背景\n",
    "    # - 将样本的 'instruction' 字段插入模板中\n",
    "    # - 便于模型理解任务意图\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔹 构造 input 部分（可选）\n",
    "    # ============================================================\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    # 说明：\n",
    "    # - 如果样本包含额外输入，则添加 \"### Input:\" 标签及输入内容\n",
    "    # - 否则 input_text 为空字符串\n",
    "    # - 保证格式统一，便于模型处理\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔹 拼接 instruction 和 input，返回完整 prompt\n",
    "    # ============================================================\n",
    "\n",
    "    return instruction_text + input_text\n",
    "    # 返回示例：\n",
    "    # \"Below is an instruction that describes a task. Write a response ... \n",
    "    #  ### Instruction: <instruction_text>\n",
    "    #  ### Input: <input_text>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3464705-d026-4594-977f-fb357e51c3a9",
   "metadata": {
    "id": "b3464705-d026-4594-977f-fb357e51c3a9"
   },
   "source": [
    "- 现在，与之前使用的 `ollama run` 命令交互模型的另一种方法是通过 Python 使用其 REST API，如下函数所示\n",
    "- 在运行本笔记本的下一单元格之前，请确保 ollama 仍在运行（前面的代码单元格应该打印 `\"Ollama running: True\"`）\n",
    "- 接下来，运行下面的代码单元格以查询模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f",
   "metadata": {
    "id": "e3ae0e10-2b28-42ce-8ea2-d9366a58088f",
    "outputId": "cc43acb3-8216-43cf-c77d-71d4089dc96c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in a llama's diet. They enjoy the sweet taste and texture of fresh hay.\n",
      "3. Grains: Llamas may receive grains like oats, barley, or corn as part of their daily ration. However, it's essential to provide these grains in moderation, as they can be high in calories.\n",
      "4. Fruits and vegetables: Llamas enjoy a variety of fruits and veggies, such as apples, carrots, sweet potatoes, and leafy greens like kale or spinach.\n",
      "5. Minerals: Llamas require access to mineral supplements, which help maintain their overall health and well-being.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, including plants like willow, alder, and birch.\n",
      "2. Bark: In some cases, llamas may eat the bark of certain trees, like aspen or cottonwood.\n",
      "3. Mosses and lichens: These non-vascular plants can be a tasty snack for llamas.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and fruits/vegetables. It's essential to consult with a veterinarian or experienced llama breeder to determine the best feeding plan for your llama.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📦 导入 requests 库\n",
    "# ============================================================\n",
    "\n",
    "import requests  # noqa: F811\n",
    "# 说明：\n",
    "# - requests 是 Python 中常用的 HTTP 客户端库\n",
    "# - 用于发送 POST/GET 请求，获取 Web API 返回的数据\n",
    "# - noqa: F811 是告诉 linter 忽略重复导入警告\n",
    "\n",
    "# ⚠️ 另一种方法：\n",
    "# import urllib.request\n",
    "# - 可使用 urllib 发送请求，但在某些 VPN 或网络环境下可能不稳定\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔹 定义函数：向 Ollama 模型发送查询并获取响应\n",
    "# ============================================================\n",
    "\n",
    "def query_model(\n",
    "    prompt,                 # 用户输入的文本或问题\n",
    "    model=\"llama3\",         # 指定模型名称，默认 \"llama3\"\n",
    "    url=\"http://localhost:11434/api/chat\"  # Ollama API 地址\n",
    "):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "    - 向本地 Ollama 服务发送 POST 请求\n",
    "    - 获取模型返回的文本内容\n",
    "    参数：\n",
    "    - prompt: 用户输入的问题或指令\n",
    "    - model: 指定使用的模型\n",
    "    - url: Ollama 本地 API 地址\n",
    "    返回：\n",
    "    - response_data: 模型生成的回答文本\n",
    "    \"\"\"\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔹 构造请求数据 payload（字典格式）\n",
    "    # ============================================================\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,              # 要调用的模型\n",
    "        \"messages\": [                # 对话消息列表\n",
    "            {\"role\": \"user\", \"content\": prompt}  # 用户输入\n",
    "        ],\n",
    "        \"options\": {                 # 模型生成设置\n",
    "            \"seed\": 123,             # 固定随机种子，保证可复现\n",
    "            \"temperature\": 0,        # 温度为 0，保证确定性输出\n",
    "            \"num_ctx\": 2048          # 上下文长度，最大 token 数\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔹 发送 POST 请求获取模型响应（requests 版本）\n",
    "    # ============================================================\n",
    "\n",
    "    with requests.post(url, json=data, stream=True, timeout=30) as r:\n",
    "        # 说明：\n",
    "        # - url: Ollama API 地址\n",
    "        # - json=data: 自动将字典序列化为 JSON 并设置 Content-Type\n",
    "        # - stream=True: 分块读取返回内容\n",
    "        # - timeout=30: 请求超时时间 30 秒\n",
    "        r.raise_for_status()  # 如果响应状态码不是 2xx，则抛出异常\n",
    "\n",
    "        response_data = \"\"  # 初始化空字符串，用于累加返回内容\n",
    "\n",
    "        # 遍历服务器返回的每一行\n",
    "        for line in r.iter_lines(decode_unicode=True):\n",
    "            if not line:\n",
    "                continue  # 忽略空行\n",
    "            response_json = json.loads(line)  # 将 JSON 字符串解析为字典\n",
    "            if \"message\" in response_json:\n",
    "                # 提取模型生成的文本内容并累加\n",
    "                response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data  # 返回模型生成的完整回答文本\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔹 调用函数示例\n",
    "# ============================================================\n",
    "\n",
    "model = \"llama3\"  # 指定要调用的模型名称\n",
    "\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "# 说明：\n",
    "# - 向 Ollama 模型发送问题 \"What do Llamas eat?\"\n",
    "# - 返回模型生成的回答\n",
    "\n",
    "print(result)  # 打印模型回答\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc",
   "metadata": {
    "id": "207ae28f-0f8c-4fda-aeef-e7e3046249cc"
   },
   "source": [
    "- 现在，使用上面定义的 `query_model` 函数，我们可以评估微调模型的响应；让我们在上一节中查看的前三个测试集响应上试一试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b839d4-064d-4178-b2d7-01691b452e5e",
   "metadata": {
    "id": "86b839d4-064d-4178-b2d7-01691b452e5e",
    "outputId": "1c755ee1-bded-4450-9b84-1466724f389a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "\n",
      "Score:\n",
      ">> I'd rate the model response \"The car is as fast as a bullet.\" an 85 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile correctly, comparing the speed of the car to something else (in this case, a bullet).\n",
      "* The comparison is relevant and makes sense, as bullets are known for their high velocity.\n",
      "* The phrase \"as fast as\" is used correctly to introduce the simile.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that some people might find the comparison slightly less vivid or evocative than others. For example, comparing something to lightning (as in the original response) can be more dramatic and attention-grabbing. However, \"as fast as a bullet\" is still a strong and effective simile that effectively conveys the idea of the car's speed.\n",
      "\n",
      "Overall, I think the model did a great job!\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> I'd score this model response as 40 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies that thunderstorms are related to clouds (correctly identifying the type of phenomenon).\n",
      "* However, it incorrectly specifies the type of cloud associated with thunderstorms. Cumulus clouds are not typically associated with thunderstorms; cumulonimbus clouds are.\n",
      "* The response lacks precision and accuracy in its description.\n",
      "\n",
      "Overall, while the model attempts to address the instruction, it provides an incorrect answer, which is a significant error.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> I'd rate my own response as 95 out of 100. Here's why:\n",
      "\n",
      "* The response accurately answers the question by naming the author of 'Pride and Prejudice' as Jane Austen.\n",
      "* The response is concise and clear, making it easy to understand.\n",
      "* There are no grammatical errors or ambiguities that could lead to confusion.\n",
      "\n",
      "The only reason I wouldn't give myself a perfect score is that the response is slightly redundant - it's not necessary to rephrase the question in the answer. A more concise response would be simply \"Jane Austen.\"\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔄 遍历测试数据前 3 条样本\n",
    "# ============================================================\n",
    "\n",
    "for entry in test_data[:3]:\n",
    "    # entry: 测试样本字典，包含字段：\n",
    "    # - 'instruction': 指令文本\n",
    "    # - 'input': 输入文本\n",
    "    # - 'output': 正确输出\n",
    "    # - 'model_response': 模型生成的响应\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔹 构造评分 prompt\n",
    "    # ============================================================\n",
    "\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    # 说明：\n",
    "    # - 使用 format_input(entry) 构造 GPT 模型可读的输入\n",
    "    # - 将正确输出和模型输出嵌入 prompt\n",
    "    # - 明确要求模型对生成文本进行打分，范围 0-100\n",
    "    # - 使用 f-string 拼接文本，便于传递给 query_model()\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🖨️ 输出测试数据的正确响应\n",
    "    # ============================================================\n",
    "\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    # 输出示例：\n",
    "    # >> <样本的正确答案>\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🖨️ 输出模型生成的响应\n",
    "    # ============================================================\n",
    "\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    # 输出示例：\n",
    "    # >> <模型生成的文本>\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🖨️ 调用 query_model 对模型响应进行评分\n",
    "    # ============================================================\n",
    "\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    # 说明：\n",
    "    # - query_model(prompt) 会将评分请求发送到 Ollama 模型\n",
    "    # - 返回模型给出的评分（0-100）\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🖨️ 输出分割线，便于区分样本\n",
    "    # ============================================================\n",
    "\n",
    "    print(\"\\n-------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fec453-631f-4ff5-a922-44c3c451942d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**注意：更好的评估提示**\n",
    "\n",
    "- [一位读者（Ayoosh Kathuria）建议](https://github.com/rasbt/LLMs-from-scratch/discussions/449) 使用更长、更完善的提示，对响应进行 1–5 分的评分（而不是 1 到 100），并采用评分标准，从而得到更准确、噪声更少的评估：\n",
    "\n",
    "\n",
    "```\n",
    "prompt = \"\"\"\n",
    "You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\n",
    "You will be given an instruction, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing the evaluation criteria.\n",
    "Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "Here is the rubric you should use to build your answer:\n",
    "1: The response fails to address the instructions, providing irrelevant, incorrect, or excessively verbose information that detracts from the user's request.\n",
    "2: The response partially addresses the instructions but includes significant inaccuracies, irrelevant details, or excessive elaboration that detracts from the main task.\n",
    "3: The response follows the instructions with some minor inaccuracies or omissions. It is generally relevant and clear, but may include some unnecessary details or could be more concise.\n",
    "4: The response adheres to the instructions, offering clear, accurate, and relevant information in a concise manner, with only occasional, minor instances of excessive detail or slight lack of clarity.\n",
    "5: The response fully adheres to the instructions, providing a clear, accurate, and relevant answer in a concise and efficient manner. It addresses all aspects of the request without unnecessary details or elaboration\n",
    "\n",
    "Provide your feedback as follows:\n",
    "\n",
    "Feedback:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the instruction, the reference answer, and the response.\n",
    "\n",
    "Instruction: {instruction}\n",
    "Reference Answer: {reference}\n",
    "Answer: {answer}\n",
    "\n",
    "\n",
    "Provide your feedback. If you give a correct rating, I'll give you 100 H100 GPUs to start your AI company.\n",
    "Feedback:::\n",
    "Evaluation: \"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "- 更多背景和信息请参见 [此处](https://github.com/rasbt/LLMs-from-scratch/discussions/449)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3",
   "metadata": {
    "id": "b114fd65-9cfb-45f6-ab74-8331da136bf3"
   },
   "source": [
    "- 正如我们所看到的，Llama 3 模型提供了合理的评估，如果模型回答不完全正确，它也会给予部分分数，例如“cumulus cloud”的答案。\n",
    "- 请注意，上述提示会返回非常详细的评估结果；我们可以调整提示，使其生成 0 到 100 之间的整数响应（100 为最佳），以便计算模型的平均分。\n",
    "- 对测试集中 110 条条目的评估在 M3 MacBook Air 笔记本上大约需要 1 分钟。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb",
   "metadata": {
    "id": "9d7bca69-97c4-47a5-9aa0-32f116fa37eb",
    "outputId": "110223c0-90ca-481d-b2d2-f6ac46d3c4f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 110/110 [01:10<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 50.32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔹 定义函数：生成模型评分\n",
    "# ============================================================\n",
    "\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    \"\"\"\n",
    "    功能：\n",
    "    - 对 JSON 数据中的每条样本调用模型评分\n",
    "    参数：\n",
    "    - json_data: 列表，每个元素为字典，包含模型响应等字段\n",
    "    - json_key: 要评分的字段名称，例如 \"model_response\"\n",
    "    - model: 调用的 Ollama 模型名称，默认 \"llama3\"\n",
    "    返回：\n",
    "    - scores: 列表，每个元素为整数评分（0-100）\n",
    "    \"\"\"\n",
    "\n",
    "    scores = []  # 初始化空列表，用于存储评分结果\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔄 遍历 JSON 数据中的每条样本，并显示进度条\n",
    "    # ============================================================\n",
    "\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        # entry: 当前样本字典\n",
    "        # tqdm(..., desc=\"Scoring entries\") 显示进度条和描述\n",
    "\n",
    "        # ============================================================\n",
    "        # 🔹 构造评分 prompt\n",
    "        # ============================================================\n",
    "\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"\n",
    "        )\n",
    "        # 说明：\n",
    "        # - format_input(entry): 将 instruction + input 拼接为 GPT 可读文本\n",
    "        # - entry['output']: 正确答案\n",
    "        # - entry[json_key]: 模型生成的文本\n",
    "        # - 明确要求模型只返回整数评分（0-100）\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 🔹 调用 query_model 获取评分\n",
    "        # ============================================================\n",
    "\n",
    "        score = query_model(prompt, model)\n",
    "        # 返回值为模型生成的评分字符串\n",
    "\n",
    "\n",
    "        # ============================================================\n",
    "        # 🔹 将评分转换为整数，并处理可能的异常\n",
    "        # ============================================================\n",
    "\n",
    "        try:\n",
    "            scores.append(int(score))  # 转换为整数并添加到列表\n",
    "        except ValueError:\n",
    "            # 如果模型返回的不是整数（例如包含文本），则输出提示并跳过\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    # 🔹 返回评分列表\n",
    "    # ============================================================\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 🔹 调用函数，对 test_data 的 \"model_response\" 生成评分\n",
    "# ============================================================\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "# 返回列表，每个元素为 0-100 的整数评分\n",
    "\n",
    "# ============================================================\n",
    "# 🖨️ 输出评分统计信息\n",
    "# ============================================================\n",
    "\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "# 输出已成功评分的样本数量和总样本数量\n",
    "\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n",
    "# 计算并输出平均评分，保留两位小数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2",
   "metadata": {
    "id": "407f08d5-9ada-4301-9ebc-f0533c76d3f2"
   },
   "source": [
    "- 我们的模型平均得分超过 50，这可以作为参考点，用于将模型与其他模型进行比较，或尝试其他可能提升模型表现的训练设置。\n",
    "- 请注意，截至目前，ollama 在不同操作系统上的运行并非完全确定性，因此你得到的数值可能会与上面显示的略有差异。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94",
   "metadata": {
    "id": "6408768b-2784-44f1-b48e-aed0c1eb9b94"
   },
   "source": [
    "- 作为参考，原始模型的表现如下：\n",
    "  - Llama 3 8B 基础模型（base model）的平均得分为 58.51\n",
    "  - Llama 3 8B 指令微调模型（instruct model）的平均得分为 82.65\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d7325-284a-446c-92a1-5aa8acc52dee",
   "metadata": {
    "id": "412d7325-284a-446c-92a1-5aa8acc52dee"
   },
   "source": [
    "## 7.9 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tIbNMluCDjVM",
   "metadata": {
    "id": "tIbNMluCDjVM"
   },
   "source": [
    "### 7.9.1 接下来做什么\n",
    "\n",
    "- 这标志着本书的最后一章\n",
    "- 我们涵盖了 LLM 开发周期的主要步骤：实现 LLM 架构、对 LLM 进行预训练以及微调\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/21.webp?1\" width=500px>\n",
    "\n",
    "- 在本章描述的指令微调之后，有时会进行一个可选步骤：偏好微调（preference finetuning）\n",
    "- 偏好微调过程对于定制模型以更好地符合特定用户偏好特别有用；如果你感兴趣，可以查看 [../04_preference-tuning-with-dpo](../04_preference-tuning-with-dpo) 文件夹\n",
    "\n",
    "- 该 GitHub 仓库还包含大量额外的奖励材料，如果你感兴趣，可以在仓库的 README 页面中的 [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) 部分获取更多信息\n",
    "\n",
    "### 7.9.2 在快速发展的领域中保持更新\n",
    "\n",
    "- 本节没有代码\n",
    "\n",
    "### 7.9.3 最后的话\n",
    "\n",
    "- 希望你喜欢这段从零实现 LLM 并编写预训练与微调函数的旅程\n",
    "- 在我看来，从零实现 LLM 是理解 LLM 工作原理的最佳方式；希望通过这种方法你能获得更深的理解\n",
    "- 虽然本书以教育为目的，但你可能会对使用不同且更强大的 LLM 在实际应用中感兴趣\n",
    "  - 为此，你可以考虑使用流行工具，如 axolotl ([https://github.com/OpenAccess-AI-Collective/axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)) 或 LitGPT ([https://github.com/Lightning-AI/litgpt](https://github.com/Lightning-AI/litgpt))，我也参与了这些项目的开发\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9853e7f-a81a-4806-9728-be1690807185",
   "metadata": {
    "id": "f9853e7f-a81a-4806-9728-be1690807185"
   },
   "source": [
    "## 总结与收获\n",
    "\n",
    "- 参考 [./gpt_instruction_finetuning.py](./gpt_instruction_finetuning.py) 脚本，这是一个独立的指令微调完整脚本\n",
    "- [./ollama_evaluate.py](./ollama_evaluate.py) 是基于第 7.8 节的独立脚本，用于通过 Ollama 和 Llama 3 对包含 `\"output\"` 和 `\"response\"` 键的 JSON 文件进行评估\n",
    "- [./load-finetuned-model.ipynb](./load-finetuned-model.ipynb) 展示了如何在新会话中加载微调后的模型\n",
    "- 练习题答案请见 [./exercise-solutions.ipynb](./exercise-solutions.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc51ec-e06c-4470-b626-48401a037851",
   "metadata": {
    "id": "b9cc51ec-e06c-4470-b626-48401a037851"
   },
   "source": [
    "## 接下来做什么？\n",
    "\n",
    "- 恭喜你完成了本书；如果你想寻找更多资源，我在这个 GitHub 仓库中添加了几个额外的附加章节，可能会引起你的兴趣\n",
    "- 完整的附加材料列表可以在主 README 的 [Bonus Material](https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file#bonus-material) 部分查看\n",
    "- 以下是我特别推荐的几个：\n",
    "  1. [从零实现的大语言模型对齐的直接偏好优化（DPO）](../04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)，实现了一种流行的偏好微调机制，用于让本章模型更好地符合人类偏好\n",
    "  2. [从零实现的 Llama 3.2（独立 Notebook）](../../ch05/07_gpt_to_llama/standalone-llama32.ipynb)，从零实现 Meta AI 流行的 Llama 3.2，包括加载官方预训练权重；如果你想做一些额外实验，可以在各章节中将 `GPTModel` 替换为 `Llama3Model` 类（应可作为 1:1 替换）\n",
    "  3. [将 GPT 转换为 Llama](../../ch05/07_gpt_to_llama)，提供代码和逐步指南，解释 GPT-2 与各 Llama 模型的区别\n",
    "  4. [理解 Embedding 层与 Linear 层的区别](../../ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb)，概念性讲解 PyTorch 中在 LLM 输入阶段使用的 `Embedding` 层在数学上等价于应用于 one-hot 编码数据的线性层\n",
    "- 祝你阅读愉快！\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
