{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "以下代码为 <a href=\"http://mng.bz/orYv\">《从零开始构建大型语言模型》</a> 一书的补充代码，作者为 <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>中文翻译和代码详细注释由Lux整理，Github下载地址：<a href=\"https://github.com/luxianyu\">https://github.com/luxianyu</a>\n",
    "    \n",
    "<br>Lux的Github上还有吴恩达深度学习Pytorch版学习笔记及中文详细注释的代码下载\n",
    "    \n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa40e3-5109-433f-9153-f5770531fe94",
   "metadata": {},
   "source": [
    "# 第2章：文本数据处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
   "metadata": {},
   "source": [
    "本笔记本中使用的包：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6b9621-2b4d-4b40-9e85-4f8fc4386ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.9.0+cpu\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块用于查看当前 Python 环境中安装的库（如 torch、tiktoken）的版本号。\n",
    "# 这在调试环境或撰写教学笔记时非常重要，因为不同版本的库可能导致代码行为不同。\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 从 Python 内置模块 importlib.metadata 导入 version 函数\n",
    "# ------------------------------------------------\n",
    "# importlib.metadata 是 Python 3.8 及以上版本的标准库\n",
    "# 用于访问当前环境中已安装的包的元数据，包括版本号、依赖信息、作者、许可证等。\n",
    "#\n",
    "# 函数 version(package_name)\n",
    "# 作用：返回指定包的版本号，返回类型为字符串\n",
    "# 参数：\n",
    "#     - package_name：字符串类型，表示要查询的包名（必须是已安装的库）\n",
    "#       示例：\"torch\" 或 \"tiktoken\"\n",
    "# 返回值：\n",
    "#     - 字符串类型，例如 \"2.3.0\" 或 \"0.6.0\"\n",
    "# 注意：\n",
    "#     若查询不存在的包，会抛出 importlib.metadata.PackageNotFoundError 异常\n",
    "# ------------------------------------------------\n",
    "from importlib.metadata import version\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 打印 PyTorch 库的版本号\n",
    "# ------------------------------------------------\n",
    "# 使用 print() 输出文本和变量结果\n",
    "# 参数说明：\n",
    "# - 第一个参数：\"torch version:\" 是一个字符串，用于提示输出内容\n",
    "# - 第二个参数：version(\"torch\") 返回当前安装的 torch 版本号\n",
    "# 输出示例：\"torch version: 2.3.1\"\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 打印 tiktoken 库的版本号\n",
    "# ------------------------------------------------\n",
    "# tiktoken 是 OpenAI 提供的高性能分词库，用于 GPT 模型的分词和 token 计数\n",
    "# version(\"tiktoken\") 获取当前安装的 tiktoken 库版本\n",
    "# 输出示例：\"tiktoken version: 0.7.0\"\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
   "metadata": {},
   "source": [
    "* 本章介绍如何进行数据准备与采样，以使输入数据为大型语言模型（LLM）做好“就绪”准备。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417139b-2357-44d2-bd67-23f5d7f52ae7",
   "metadata": {},
   "source": [
    "## 2.1 理解词嵌入（Word Embeddings）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6816ae-e927-43a9-b4dd-e47a9b0e1cf6",
   "metadata": {},
   "source": [
    "- 本节不包含任何代码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69dab7-a433-427a-9e5b-b981062d6296",
   "metadata": {},
   "source": [
    "- 嵌入（embeddings）有多种形式；在本书中，我们关注文本嵌入（text embeddings）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08d16f-f237-4166-bf89-0e9fe703e7b4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c4faf-b93a-4616-9276-7a4aa4b5e9ba",
   "metadata": {},
   "source": [
    "- 大型语言模型（LLMs）在高维空间中处理嵌入（即数千维度）  \n",
    "- 由于我们无法可视化如此高维的空间（人类思维通常在 1、2 或 3 维），下图展示了一个二维嵌入空间\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b80160-1f10-4aad-a85e-9c79444de9e6",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
   "metadata": {},
   "source": [
    "## 2.2 文本分词（Tokenizing text）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80",
   "metadata": {},
   "source": [
    "- 在本节中，我们对文本进行分词，这意味着将文本拆分为更小的单元，例如单个单词和标点符号\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceaa18-833d-46b6-b211-b20c53902805",
   "metadata": {},
   "source": [
    "- 加载我们要处理的原始文本  \n",
    "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) 是一篇公有领域的短篇小说\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f9d9b1-6d32-485a-825a-a95392a86d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块用于下载文本文件 \"the-verdict.txt\"。\n",
    "# 如果文件已经存在，则不会重复下载。\n",
    "# 使用 requests 库进行下载，比 urllib 更稳健，特别是存在 VPN 或网络限制时。\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 导入必要的 Python 库\n",
    "# ------------------------------------------------\n",
    "import os          # 用于操作文件路径和判断文件是否存在\n",
    "import requests    # 用于发送 HTTP 请求，从网络获取文件内容\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 判断文件是否已存在\n",
    "# ------------------------------------------------\n",
    "# os.path.exists(file_path) 用于检查指定路径的文件是否存在\n",
    "# 如果文件不存在，则执行下载操作\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 定义文件的下载 URL\n",
    "    # ------------------------------------------------\n",
    "    # Python 支持字符串拼接，这里分多行写更易读\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    \n",
    "    # 定义本地保存路径\n",
    "    file_path = \"the-verdict.txt\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 发送 HTTP GET 请求下载文件\n",
    "    # ------------------------------------------------\n",
    "    # requests.get(url, timeout=30) 发送 GET 请求\n",
    "    # 参数说明：\n",
    "    # - url: 要下载文件的完整 URL\n",
    "    # - timeout: 请求超时时间（秒），防止网络长时间阻塞\n",
    "    response = requests.get(url, timeout=30)\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 检查请求是否成功\n",
    "    # ------------------------------------------------\n",
    "    # response.raise_for_status() 会抛出异常，如果 HTTP 返回状态码不是 2xx\n",
    "    # 这样可以保证只有在下载成功时才继续写入文件\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 将下载内容写入本地文件\n",
    "    # ------------------------------------------------\n",
    "    # 使用 with open(...) as f 打开文件，确保操作结束后自动关闭文件\n",
    "    # 参数说明：\n",
    "    # - file_path: 文件保存路径\n",
    "    # - \"wb\": 以二进制写模式打开文件\n",
    "    # - response.content: 获取 HTTP 响应内容（二进制）\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 备注：\n",
    "# ------------------------------------------------\n",
    "# 原书中使用 urllib.request.urlretrieve 下载文件：\n",
    "# import urllib.request\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "# 但是 urllib 在某些环境（如使用 VPN）下可能出现网络协议问题\n",
    "# requests 更现代、更稳健，并且使用方式更灵活\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56488f2c-a2b8-49f1-aaeb-461faad08dce",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "#### SSL 证书错误排查\n",
    "\n",
    "- 一些读者在 VSCode 或 Jupyter 中运行 `urllib.request.urlretrieve` 时，报告出现 `ssl.SSLCertVerificationError: SSL: CERTIFICATE_VERIFY_FAILED` 错误。  \n",
    "- 这通常意味着 Python 的证书包已过期。\n",
    "\n",
    "**解决方法**\n",
    "\n",
    "- 使用 Python ≥ 3.9；你可以通过执行以下代码检查 Python 版本：\n",
    "\n",
    "```python\n",
    "import sys\n",
    "print(sys.__version__)\n",
    "```\n",
    "- 升级证书包：  \n",
    "  - pip: `pip install --upgrade certifi`  \n",
    "  - uv: `uv pip install --upgrade certifi`  \n",
    "- 升级后重启 Jupyter 内核。  \n",
    "- 如果在执行前面的代码单元时仍遇到 `ssl.SSLCertVerificationError`，请参阅 GitHub 上的讨论：[更多信息](https://github.com/rasbt/LLMs-from-scratch/pull/403)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a769e87-470a-48b9-8bdb-12841b416198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块用于读取本地文本文件 \"the-verdict.txt\" 的内容\n",
    "# 并统计字符总数，同时展示前 100 个字符作为示例\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 打开文件并读取内容\n",
    "# ------------------------------------------------\n",
    "# 使用 with open(...) as f 打开文件，确保操作结束后自动关闭文件\n",
    "# 参数说明：\n",
    "# - \"the-verdict.txt\": 要读取的文件路径\n",
    "# - \"r\": 以只读模式打开文件\n",
    "# - encoding=\"utf-8\": 指定文件编码为 UTF-8，保证中文或特殊字符不会报错\n",
    "#\n",
    "# f.read(): 读取文件中所有内容，返回字符串\n",
    "# 将内容保存到变量 raw_text 中\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出文件的字符总数\n",
    "# ------------------------------------------------\n",
    "# len(raw_text): 计算字符串长度，即文件中字符总数\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出文件前 100 个字符作为示例\n",
    "# ------------------------------------------------\n",
    "# raw_text[:99]: 切片操作，获取从索引 0 到 98 的字符（共 99 个字符）\n",
    "# 用于快速查看文件开头内容，方便检查文件是否正确读取\n",
    "print(raw_text[:99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e",
   "metadata": {},
   "source": [
    "- 目标是对这段文本进行分词并嵌入，以供 LLM 使用  \n",
    "- 我们先基于一些简单示例文本开发一个简单的分词器，然后再将其应用到上面的文本  \n",
    "- 以下正则表达式将根据空格进行分割\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何使用 Python 内置 re 模块进行字符串分割\n",
    "# 并展示保留分隔符（这里是空格）的效果\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 导入正则表达式模块\n",
    "# ------------------------------------------------\n",
    "# re 模块提供正则表达式操作功能，包括搜索、匹配、替换和分割等\n",
    "import re\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 定义要处理的字符串\n",
    "# ------------------------------------------------\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "# text 是一个普通字符串\n",
    "# 示例内容中包含逗号、句号和空格，用于展示正则分割效果\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 使用 re.split() 按空格分割字符串\n",
    "# ------------------------------------------------\n",
    "# re.split(pattern, string) 按正则 pattern 分割字符串\n",
    "# 参数说明：\n",
    "# - pattern: 正则表达式模式，这里是 r'(\\s)'\n",
    "#     - r'' 前缀表示原生字符串（raw string），避免反斜杠被转义\n",
    "#     - (\\s) 表示匹配任意空白字符（空格、制表符、换行等）\n",
    "#       括号表示捕获组，分割结果会保留分隔符\n",
    "# - string: 要处理的字符串，这里是 text\n",
    "#\n",
    "# 返回值：\n",
    "# - 列表类型，包含原始子串和分隔符（空格）交替出现\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出分割结果\n",
    "# ------------------------------------------------\n",
    "# 打印列表 result，方便查看分割效果\n",
    "# 示例输出：\n",
    "# ['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c40c18-a9d5-4703-bf71-8261dbcc5ee3",
   "metadata": {},
   "source": [
    "- 我们不仅希望按空格分割，还希望按逗号和句号分割，因此我们修改正则表达式以实现这一功能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea02489d-01f9-4247-b7dd-a0d63f62ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何使用 re.split() 按多种分隔符分割字符串\n",
    "# 分隔符包括逗号、句号和空格，同时保留这些分隔符\n",
    "# ================================================================\n",
    "\n",
    "import re  # 导入正则表达式模块\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 定义要处理的字符串\n",
    "# ------------------------------------------------\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "# 示例字符串包含逗号、句号和空格，用于展示多分隔符分割效果\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 使用 re.split() 按逗号、句号或空格分割字符串\n",
    "# ------------------------------------------------\n",
    "# re.split(pattern, string) 按正则 pattern 分割字符串\n",
    "# 参数说明：\n",
    "# - pattern: r'([,.]|\\s)'\n",
    "#     - r'' 表示原生字符串（raw string），避免反斜杠被转义\n",
    "#     - [,.] 表示匹配字符集中的任意一个字符，这里是逗号 , 或句号 .\n",
    "#     - \\s 表示匹配任意空白字符（空格、制表符、换行等）\n",
    "#     - | 表示“或”，即匹配 [,.] 或 \\s\n",
    "#     - () 括号表示捕获组，分割结果会保留匹配到的分隔符\n",
    "# - string: 要处理的字符串，这里是 text\n",
    "#\n",
    "# 返回值：\n",
    "# - 列表类型，包含原始子串和分隔符交替出现\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出分割结果\n",
    "# ------------------------------------------------\n",
    "# 打印列表 result，方便查看分割效果\n",
    "# 示例输出：\n",
    "# ['Hello', ',', ' ', 'world', '.', ' ', 'This', ',', ' ', 'is', ' ', 'a', ' ', 'test', '.']\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d0c86-e3af-4f87-8fae-594a9ca9b6ad",
   "metadata": {},
   "source": [
    "- 如我们所见，这会生成空字符串，因此我们需要将其移除\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8a6fb7-2e62-4a12-ad06-ccb04f25fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何对列表进行清洗：\n",
    "# 去掉每个元素的前后空白，并过滤掉空字符串\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 列表推导式处理列表 result\n",
    "# ------------------------------------------------\n",
    "# [item for item in result if item.strip()]\n",
    "# 逐项解释：\n",
    "# - item for item in result\n",
    "#     遍历列表 result 中的每个元素，赋值给变量 item\n",
    "# - item.strip()\n",
    "#     - strip() 是字符串方法\n",
    "#     - 去掉字符串前后空白字符（空格、制表符、换行等）\n",
    "#     - 如果字符串去掉空白后长度为 0，则返回 False（在 if 中相当于 False）\n",
    "# - if item.strip()\n",
    "#     - 过滤条件：只有去掉空白后非空的字符串才会保留在新列表中\n",
    "#\n",
    "# 最终生成的新列表：\n",
    "# - 每个元素都已去掉前后空白\n",
    "# - 空字符串或只有空白字符的元素被移除\n",
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出清洗后的列表\n",
    "# ------------------------------------------------\n",
    "# 打印处理结果，方便检查\n",
    "# 示例输出：\n",
    "# ['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e8694-181e-496f-895d-7cb7d92b5562",
   "metadata": {},
   "source": [
    "- 这看起来相当不错，但我们还需要处理其他类型的标点符号，例如句号、问号等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3a9467-04b4-49d9-96c5-b8042bcf8374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何使用正则表达式将字符串按多种标点符号和空白字符分割，\n",
    "# 并清洗结果，去掉空字符串和前后空白\n",
    "# ================================================================\n",
    "\n",
    "import re  # 导入正则表达式模块\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 定义要处理的字符串\n",
    "# ------------------------------------------------\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "# 示例字符串包含逗号、句号、问号、破折号、空格等\n",
    "# 用于展示复杂分隔符分割效果\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 使用 re.split() 按多种标点符号和空白字符分割\n",
    "# ------------------------------------------------\n",
    "# re.split(pattern, string) 按正则表达式 pattern 分割字符串\n",
    "# 参数说明：\n",
    "# - pattern: r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "#     - [] 表示字符集，匹配其中任意一个字符\n",
    "#       这里匹配: , . : ; ? _ ! \" ( ) '\n",
    "#     - | 表示“或”，可以匹配多个条件\n",
    "#     - -- 匹配两个连字符（破折号）\n",
    "#     - \\s 匹配任意空白字符（空格、制表符、换行等）\n",
    "#     - () 括号表示捕获组，分割结果会保留匹配到的分隔符\n",
    "# - string: 要处理的字符串，这里是 text\n",
    "#\n",
    "# 返回值：\n",
    "# - 列表类型，包含原始子串和分隔符交替出现\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 清洗列表：去掉空字符串和前后空白\n",
    "# ------------------------------------------------\n",
    "# 列表推导式：\n",
    "# - item.strip() 去掉每个元素的前后空白\n",
    "# - if item.strip() 过滤掉空字符串（去掉空格后长度为0）\n",
    "# 最终列表 result 中：\n",
    "# - 每个元素都已去掉前后空白\n",
    "# - 空字符串或只有空格的元素被移除\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出清洗后的列表\n",
    "# ------------------------------------------------\n",
    "# 打印处理结果\n",
    "# 示例输出：\n",
    "# ['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbea70b-c030-45d9-b09d-4318164c0bb4",
   "metadata": {},
   "source": [
    "- 这已经相当不错，现在我们可以将此分词方法应用到原始文本上\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c567caa-8ff5-49a8-a5cc-d365b0a78a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何对文本 raw_text 进行预处理：\n",
    "# 使用正则表达式将文本按标点符号和空白字符分割，\n",
    "# 并清洗结果，去掉空字符串和前后空白\n",
    "# ================================================================\n",
    "\n",
    "import re  # 导入正则表达式模块\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 使用 re.split() 按标点符号和空白字符分割文本\n",
    "# ------------------------------------------------\n",
    "# raw_text 是之前读取的整个文本文件内容\n",
    "# re.split(pattern, string) 按正则表达式 pattern 分割字符串\n",
    "# 参数说明：\n",
    "# - pattern: r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "#     - [] 表示字符集，匹配其中任意一个字符\n",
    "#       这里匹配: , . : ; ? _ ! \" ( ) '\n",
    "#     - | 表示“或”，可以匹配多个条件\n",
    "#     - -- 匹配破折号\n",
    "#     - \\s 匹配任意空白字符（空格、制表符、换行等）\n",
    "#     - () 括号表示捕获组，分割结果会保留匹配到的分隔符\n",
    "# - string: 要处理的文本，这里是 raw_text\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 清洗列表：去掉空字符串和前后空白\n",
    "# ------------------------------------------------\n",
    "# 列表推导式：\n",
    "# - item.strip() 去掉每个元素的前后空白字符\n",
    "# - if item.strip() 过滤掉空字符串（去掉空格或换行后长度为0）\n",
    "# 最终列表 preprocessed 中：\n",
    "# - 每个元素都是非空字符串\n",
    "# - 保留原来的标点符号\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出清洗后的前 30 个元素\n",
    "# ------------------------------------------------\n",
    "# 打印前 30 个元素作为示例，方便检查文本预处理效果\n",
    "# 注意：这里使用切片 preprocessed[:30] 获取列表前 30 个元素\n",
    "print(preprocessed[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a19e1a-5105-4ddb-812a-b7d3117eab95",
   "metadata": {},
   "source": [
    "- 让我们计算令牌（tokens）的总数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231",
   "metadata": {},
   "source": [
    "## 2.3 将令牌转换为令牌 ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
   "metadata": {},
   "source": [
    "- 接下来，我们将文本令牌转换为令牌 ID，以便稍后通过嵌入层处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5973794-7002-4202-8b12-0900cd779720",
   "metadata": {},
   "source": [
    "- 从这些令牌中，我们现在可以构建一个词汇表，其中包含所有唯一的令牌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块用于构建文本的词汇表（vocabulary）：\n",
    "# - 去重\n",
    "# - 排序\n",
    "# - 统计词汇量\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 创建词汇表\n",
    "# ------------------------------------------------\n",
    "# set(preprocessed) \n",
    "# - 将列表 preprocessed 转换为集合（set）\n",
    "# - 集合的特点是元素唯一，自动去重\n",
    "# sorted(set(preprocessed))\n",
    "# - 将去重后的集合按字母顺序排序\n",
    "# - 返回一个列表，其中元素为去重且排序后的单词/符号\n",
    "all_words = sorted(set(preprocessed))\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 统计词汇表大小（词汇量）\n",
    "# ------------------------------------------------\n",
    "# len(all_words) 返回列表中元素个数，即词汇表大小\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出词汇表大小\n",
    "# ------------------------------------------------\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d00d96-881f-4691-bb03-84fec2a75a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块用于构建词汇表映射（token 到索引的字典）\n",
    "# 方便后续将文本转换为整数序列，用于模型训练\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 使用字典推导式创建 token -> index 映射\n",
    "# ------------------------------------------------\n",
    "# {token: integer for integer, token in enumerate(all_words)}\n",
    "# 逐项解释：\n",
    "# - enumerate(all_words) \n",
    "#     - 对 all_words 列表进行枚举\n",
    "#     - 返回每个元素的索引和元素本身，形式为 (索引, token)\n",
    "#     - 示例: (0, 'Hello'), (1, ','), (2, 'World'), ...\n",
    "# - for integer, token in enumerate(all_words)\n",
    "#     - 遍历枚举对象，将索引赋值给 integer，元素赋值给 token\n",
    "# - token: integer\n",
    "#     - 在字典中，key 是 token（单词或符号）\n",
    "#     - value 是 integer（索引）\n",
    "# - 最终生成 vocab 字典，形式如：\n",
    "#     {'Hello': 0, ',': 1, 'World': 2, ...}\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 教学提示：\n",
    "# ------------------------------------------------\n",
    "# 1. vocab 可以将文本中的每个 token 转换为唯一整数索引\n",
    "# 2. 构建 token->index 的字典是 NLP 模型文本编码的第一步\n",
    "# 3. 对应的反向映射可以用：\n",
    "#    inverse_vocab = {index: token for token, index in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3",
   "metadata": {},
   "source": [
    "- 以下是该词汇表中的前 50 个条目：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何遍历词汇表字典，并打印前 50 个 token->index 项\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 使用 enumerate 遍历 vocab 字典的 items()\n",
    "# ------------------------------------------------\n",
    "# vocab.items() \n",
    "# - 返回字典中所有 (key, value) 对的视图\n",
    "# - 每个 item 是一个元组 (token, index)\n",
    "#\n",
    "# enumerate(vocab.items())\n",
    "# - 对字典项进行枚举\n",
    "# - 返回 (i, item)\n",
    "#   - i: 当前迭代的索引（0 开始）\n",
    "#   - item: 字典中的 (token, index) 元组\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 打印当前字典项\n",
    "    # ------------------------------------------------\n",
    "    # item 是一个元组 (token, index)\n",
    "    # 示例输出: ('Hello', 0)\n",
    "    print(item)\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 控制输出数量\n",
    "    # ------------------------------------------------\n",
    "    # if i >= 50: break\n",
    "    # - 当打印 51 个元素后停止循环\n",
    "    # - 避免输出太多内容，便于教学展示\n",
    "    if i >= 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19",
   "metadata": {},
   "source": [
    "- 下面通过一个小型词汇表演示短文本的分词过程：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a",
   "metadata": {},
   "source": [
    "- 现在将上述步骤整合到一个分词器类（tokenizer class）中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块定义一个简单的分词器类 SimpleTokenizerV1\n",
    "# 功能：\n",
    "# 1. encode(text) - 将文本转换为整数序列\n",
    "# 2. decode(ids) - 将整数序列还原为文本\n",
    "# 适合 NLP 教学演示，理解 token -> index 映射与文本预处理\n",
    "# ================================================================\n",
    "\n",
    "import re  # 导入正则表达式模块，用于文本分割与空格处理\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 定义分词器类\n",
    "# ------------------------------------------------\n",
    "class SimpleTokenizerV1:\n",
    "\n",
    "    # ===========================================================\n",
    "    # 初始化方法\n",
    "    # ===========================================================\n",
    "    # 参数：\n",
    "    # - vocab: 字典类型，形式为 {token: index}，表示词汇表\n",
    "    # 功能：\n",
    "    # - 保存 token -> index 映射，便于将文本编码为整数\n",
    "    # - 构建 index -> token 反向映射，便于解码\n",
    "    def __init__(self, vocab):\n",
    "        # 保存 token -> index 映射\n",
    "        # vocab 是外部传入的字典，例如：{'Hello': 0, ',': 1, 'world': 2, ...}\n",
    "        self.str_to_int = vocab\n",
    "\n",
    "        # 构建 index -> token 的反向映射字典\n",
    "        # 字典推导式 {i: s for s, i in vocab.items()} 遍历原字典\n",
    "        # - s: token\n",
    "        # - i: index\n",
    "        # 最终结果为 {0: 'Hello', 1: ',', 2: 'world', ...}\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    # ===========================================================\n",
    "    # 文本编码方法\n",
    "    # ===========================================================\n",
    "    # 参数：\n",
    "    # - text: 待编码的字符串，例如 \"Hello, world!\"\n",
    "    # 返回值：\n",
    "    # - ids: 对应整数索引列表，例如 [0, 1, 2, 3]\n",
    "    def encode(self, text):\n",
    "        # ------------------------------------------------\n",
    "        # 1. 文本预处理\n",
    "        # ------------------------------------------------\n",
    "        # 使用正则表达式将文本按标点符号和空白字符分割\n",
    "        # re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        # 参数说明：\n",
    "        # - [,.:;?_!\"()\\'] : 匹配逗号、句号、冒号、分号、问号、下划线、感叹号、双引号、括号、单引号\n",
    "        # - --             : 匹配破折号\n",
    "        # - \\s             : 匹配任意空白字符（空格、制表符、换行符）\n",
    "        # - ()             : 捕获组，保证分隔符保留在返回列表中\n",
    "        # 返回值：\n",
    "        # - preprocessed: 初步分割后的列表，包含 token 和分隔符\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # 2. 去除空字符串和前后空白\n",
    "        # ------------------------------------------------\n",
    "        # 列表推导式 [item.strip() for item in preprocessed if item.strip()]\n",
    "        # 逐步解释：\n",
    "        # - item.strip() 去掉 token 前后的空白字符\n",
    "        # - if item.strip() 过滤掉长度为 0 的元素（空字符串或仅空格）\n",
    "        # 返回值：\n",
    "        # - preprocessed: 清洗后的列表，仅包含有效 token 和标点\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # 3. 将 token 转换为整数索引\n",
    "        # ------------------------------------------------\n",
    "        # 列表推导式 [self.str_to_int[s] for s in preprocessed]\n",
    "        # 逐步解释：\n",
    "        # - 遍历清洗后的 preprocessed 列表\n",
    "        # - self.str_to_int[s]: 查找 token s 对应的整数索引\n",
    "        # 返回值：\n",
    "        # - ids: 整数序列列表，例如 [0, 1, 2, 3]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "        # 返回整数序列\n",
    "        return ids\n",
    "\n",
    "    # ===========================================================\n",
    "    # 整数序列解码方法\n",
    "    # ===========================================================\n",
    "    # 参数：\n",
    "    # - ids: 整数序列列表，例如 [0, 1, 2, 3]\n",
    "    # 返回值：\n",
    "    # - text: 还原后的字符串，例如 \"Hello, world!\"\n",
    "    def decode(self, ids):\n",
    "        # ------------------------------------------------\n",
    "        # 1. 将整数序列转换回 token 列表\n",
    "        # ------------------------------------------------\n",
    "        # 列表推导式 [self.int_to_str[i] for i in ids]\n",
    "        # 逐步解释：\n",
    "        # - 遍历 ids 列表\n",
    "        # - self.int_to_str[i]: 查找索引 i 对应的 token\n",
    "        # - \" \".join(...) 将 token 列表拼接为字符串，中间用空格分隔\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # 2. 修复标点符号前的空格\n",
    "        # ------------------------------------------------\n",
    "        # re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        # 逐步解释：\n",
    "        # - 正则 r'\\s+([,.?!\"()\\'])'\n",
    "        #   - \\s+ 匹配一个或多个空白字符\n",
    "        #   - ([,.?!\"()\\']) 捕获组，匹配标点符号\n",
    "        # - 替换为 r'\\1'\n",
    "        #   - \\1 表示捕获组中的标点符号\n",
    "        #   - 相当于去掉标点符号前的空格\n",
    "        # 返回值：\n",
    "        # - text: 修正空格后的自然文本\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "        # 返回解码后的文本\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6d66f-5b47-4639-891a-e3813d70a3c5",
   "metadata": {},
   "source": [
    "| 对比点      | `self.int_to_str = {i: s for s, i in vocab.items()}` | `enumerate(vocab.items())` |\n",
    "| -------- | ---------------------------------------------------- | -------------------------- |\n",
    "| **作用**   | 生成新的字典，将 key-value 反转                                | 遍历字典，给每一项加索引               |\n",
    "| **返回值**  | 新字典 `{index: token}`                                 | 迭代器 `(i, (token, index))`  |\n",
    "| **索引 i** | 来自原字典的 value                                         | 枚举序号，从 0 开始，与字典 value 无关   |\n",
    "| **用途**   | 构建解码映射                                               | 打印或遍历字典，调试/教学用             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
   "metadata": {},
   "source": [
    "- `encode` 函数将文本转换为 **token ID**  \n",
    "- `decode` 函数则将 **token ID** 转换回文本\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
   "metadata": {},
   "source": [
    "- 我们可以使用分词器将文本编码（即分词）为整数  \n",
    "- 这些整数随后可以被嵌入（作为 LLM 的输入）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何使用 SimpleTokenizerV1 对文本进行编码\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 创建分词器实例\n",
    "# ------------------------------------------------\n",
    "# tokenizer = SimpleTokenizerV1(vocab)\n",
    "# - vocab: 前面创建的 token->index 映射字典\n",
    "# - SimpleTokenizerV1(vocab) 调用类的 __init__ 方法\n",
    "#     - 保存 str_to_int 映射\n",
    "#     - 构建 int_to_str 反向映射\n",
    "# - tokenizer 是分词器对象，可以调用 encode 和 decode 方法\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 定义待编码文本\n",
    "# ------------------------------------------------\n",
    "# text: 待编码的字符串\n",
    "# 这里是多行字符串（Python 中 \"\"\" ... \"\"\" 支持多行）\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 编码文本\n",
    "# ------------------------------------------------\n",
    "# ids = tokenizer.encode(text)\n",
    "# - 调用分词器对象的 encode 方法，将文本转换为整数序列\n",
    "# 编码流程详细说明：\n",
    "# 1) 文本预处理：\n",
    "#    - 使用正则表达式将文本按标点符号和空白字符分割\n",
    "#    - 捕获标点符号，保留在列表中\n",
    "# 2) 清洗列表：\n",
    "#    - 去掉空字符串和前后空白\n",
    "# 3) token -> index 映射：\n",
    "#    - 对每个 token 使用 self.str_to_int 查找对应整数索引\n",
    "#    - 最终生成整数序列列表 ids\n",
    "ids = tokenizer.encode(text)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. 打印整数序列\n",
    "# ------------------------------------------------\n",
    "# print(ids)\n",
    "# - 输出编码后的整数序列\n",
    "# - 每个整数对应原文本中的一个 token\n",
    "# - 示例输出（根据 vocab 不同，实际值可能不同）：\n",
    "#   [12, 7, 34, 56, 23, 89, 14, 5, 77, 3, ...]\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201706e-a487-4b60-b99d-5765865f29a0",
   "metadata": {},
   "source": [
    "- 我们可以将这些整数解码回文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何使用 SimpleTokenizerV1 的 decode 方法\n",
    "# 将整数序列还原为可读文本\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 方法调用\n",
    "# ------------------------------------------------\n",
    "# tokenizer.decode(ids)\n",
    "# - tokenizer: SimpleTokenizerV1 分词器实例\n",
    "#   - 已经初始化时传入 vocab（token->index 映射）\n",
    "# - decode(ids): 分词器方法，用于将整数序列还原为文本\n",
    "# - ids: 待解码的整数序列列表\n",
    "#   例如：\n",
    "#     ids = [12, 7, 34, 56, 23, 89, 14, 5, 77, 3, ...]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# decode 方法内部流程（详细讲解）\n",
    "# ------------------------------------------------\n",
    "# 1. 整数索引 → token 映射\n",
    "#    [self.int_to_str[i] for i in ids]\n",
    "#    - 遍历 ids 列表中的每个整数 i\n",
    "#    - 使用 int_to_str 字典查找 i 对应的 token（单词或标点）\n",
    "#    - 返回 token 列表，例如：\n",
    "#        ['It', \"'\", 's', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',']\n",
    "#\n",
    "# 2. 拼接 token 为字符串\n",
    "#    \" \".join([...])\n",
    "#    - 将 token 列表用空格 \" \" 连接成字符串\n",
    "#    - 此时标点符号前可能有多余空格，例如：\n",
    "#        \"It ' s the last he painted , you know ,\"\n",
    "#\n",
    "# 3. 去掉标点前多余空格\n",
    "#    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "#    - 使用正则表达式替换标点前多余空格\n",
    "#    - r'\\s+([,.?!\"()\\'])' 匹配空格 + 标点\n",
    "#    - r'\\1' 表示只保留标点，删除前面空格\n",
    "#    - 最终得到自然文本：\n",
    "#        \"It's the last he painted, you know,\"\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 执行解码\n",
    "# ------------------------------------------------\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出解码后的文本\n",
    "# ------------------------------------------------\n",
    "# print(decoded_text)\n",
    "# - decoded_text 是 ids 对应的原始文本（token化后的空格格式可能略有差异，但标点和单词保持一致）\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本：\n",
      "\"It's the last he painted, you know,\" \n",
      "           Mrs. Gisburn said with pardonable pride.\n",
      "\n",
      "编码-解码后的文本：\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 本模块演示如何使用 SimpleTokenizerV1 对文本进行“编码-解码”完整流程\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 示例文本\n",
    "# ------------------------------------------------\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "# - text: 待处理的字符串，可以是多行文本\n",
    "# - 示例中包含单引号、逗号、句号等标点符号\n",
    "# - 可以用来演示分词器对标点和空格的处理\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 编码-解码流程\n",
    "# ------------------------------------------------\n",
    "# tokenizer.encode(text)\n",
    "# - 将原始文本 text 分割为 token，并映射为整数序列 ids\n",
    "# - encode 方法内部流程：\n",
    "#   1) 使用正则表达式将文本按标点符号和空白字符分割\n",
    "#   2) 去掉空字符串和多余空白\n",
    "#   3) 使用 str_to_int 映射将每个 token 转换为对应整数\n",
    "# - 返回 ids 列表，例如：\n",
    "#   ids = [12, 7, 34, 56, 23, 89, 14, 5, 77, 3, ...]\n",
    "\n",
    "# tokenizer.decode(...)\n",
    "# - 将整数序列 ids 还原为文本字符串\n",
    "# - decode 方法内部流程：\n",
    "#   1) 整数索引 → token 映射\n",
    "#   2) token 列表 → 空格拼接字符串\n",
    "#   3) 使用正则去掉标点前多余空格\n",
    "# - 返回可读文本 decoded_text\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 组合调用：先编码再解码\n",
    "# ------------------------------------------------\n",
    "decoded_text = tokenizer.decode(tokenizer.encode(text))\n",
    "# - tokenizer.encode(text): 返回整数序列 ids\n",
    "# - tokenizer.decode(...): 将 ids 还原为文本\n",
    "# - decoded_text: 解码后的文本字符串\n",
    "# - 这个组合展示了 encode 和 decode 的“正向-逆向”操作\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 输出结果\n",
    "# ------------------------------------------------\n",
    "print(\"原始文本：\")\n",
    "print(text)\n",
    "print(\"\\n编码-解码后的文本：\")\n",
    "print(decoded_text)\n",
    "\n",
    "# ================================================================\n",
    "# 教学要点总结\n",
    "# ================================================================\n",
    "# 1. encode(text) 将文本 token 化并映射为整数序列\n",
    "# 2. decode(ids) 将整数序列还原为文本\n",
    "# 3. tokenizer.decode(tokenizer.encode(text)) 相当于对文本进行一次完整的“编码-解码”循环\n",
    "# 4. 输出结果可以用于教学，观察标点和空格处理效果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7",
   "metadata": {},
   "source": [
    "## 2.4 添加特殊上下文令牌\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d6d15-a3e2-44e0-b384-bb37f17cf443",
   "metadata": {},
   "source": [
    "- 添加一些“特殊”令牌非常有用，例如用于表示未知单词或文本结尾\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fc96c-e1fd-44fb-b7f5-229d7c7922a4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd",
   "metadata": {},
   "source": [
    "- 一些分词器使用特殊令牌来为 LLM 提供额外上下文  \n",
    "- 常见的特殊令牌包括：\n",
    "  - `[BOS]`（序列开始）标记文本的开始  \n",
    "  - `[EOS]`（序列结束）标记文本的结束（通常用于连接多个不相关的文本，例如两篇不同的维基百科文章或两本不同的书籍）  \n",
    "  - `[PAD]`（填充）如果我们以大于 1 的批量大小训练 LLM（可能包含多条长度不同的文本；使用填充令牌可以将较短的文本填充到最长文本长度，使所有文本长度一致）  \n",
    "  - `[UNK]` 表示不在词汇表中的单词\n",
    "\n",
    "- 注意，GPT-2 并不需要上述任何特殊令牌，只使用 `<|endoftext|>` 令牌以降低复杂度  \n",
    "- `<|endoftext|>` 类似于上文提到的 `[EOS]`  \n",
    "- GPT 还使用 `<|endoftext|>` 进行填充（由于训练批量输入时通常使用掩码，填充令牌不会被关注，因此这些令牌的具体内容无关紧要）  \n",
    "- GPT-2 不使用 `<UNK>` 处理词汇表外的单词，而是使用字节对编码（BPE）分词器，将单词拆分为子词单元，我们将在后续章节讨论\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336b43b-7173-49e7-bd80-527ad4efb271",
   "metadata": {},
   "source": [
    "- 我们在两个独立文本源之间使用 `<|endoftext|>` 令牌：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52442951-752c-4855-9752-b121a17fef55",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661a397-da06-4a86-ac27-072dbe7cb172",
   "metadata": {},
   "source": [
    "- 让我们看看对以下文本进行分词会发生什么：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5767eff-440c-4de1-9289-f789349d6b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 30\u001b[0m\n\u001b[0;32m     21\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea. Is this-- a test?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# - text: 待编码的字符串文本\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - 示例文本包含：\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#     - 单词: Hello, do, you, like, tea, Is, this, a, test\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 3. 编码文本\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "Cell \u001b[1;32mIn[15], line 79\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     68\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 3. 将 token 转换为整数索引\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# 返回值：\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# - ids: 整数序列列表，例如 [0, 1, 2, 3]\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# 返回整数序列\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何使用 SimpleTokenizerV1 对文本进行编码（encode）\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 创建分词器实例\n",
    "# ------------------------------------------------\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "# - tokenizer: SimpleTokenizerV1 类的实例对象\n",
    "# - vocab: 前面创建的 token -> index 字典，用于编码文本\n",
    "# - 调用 __init__ 方法时：\n",
    "#     1) 保存 str_to_int 映射（token -> index）\n",
    "#     2) 构建 int_to_str 反向映射（index -> token）\n",
    "# - tokenizer 对象可以调用 encode() 方法将文本转为整数序列\n",
    "#   也可以调用 decode() 方法将整数序列还原为文本\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 定义待编码文本\n",
    "# ------------------------------------------------\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "# - text: 待编码的字符串文本\n",
    "# - 示例文本包含：\n",
    "#     - 单词: Hello, do, you, like, tea, Is, this, a, test\n",
    "#     - 标点: 逗号, 句号, 问号, 破折号 (--)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 编码文本\n",
    "# ------------------------------------------------\n",
    "tokenizer.encode(text)\n",
    "# - 调用 tokenizer.encode() 方法\n",
    "# - 内部流程：\n",
    "#     1) 文本预处理：\n",
    "#         - 使用正则表达式 re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "#         - 将文本按标点符号、破折号、空格分割，并保留标点\n",
    "#     2) 清理列表：\n",
    "#         - 列表推导式 [item.strip() for item in preprocessed if item.strip()]\n",
    "#         - 去掉空白字符或空字符串\n",
    "#     3) token -> index 映射：\n",
    "#         - [self.str_to_int[s] for s in preprocessed]\n",
    "#         - 将每个 token 转换为整数索引\n",
    "# - 返回整数列表 ids，例如：\n",
    "#     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 教学要点总结\n",
    "# ------------------------------------------------\n",
    "# 1. encode() 是 NLP 模型输入的关键步骤，将文本转为整数序列\n",
    "# 2. 正则表达式可以处理标点符号、空格和破折号\n",
    "# 3. 返回的整数序列可以直接用于神经网络模型输入\n",
    "# 4. 本示例展示了如何将文本 token 化并编码为整数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53ee0c-fe2b-4cd8-a946-5471f7651acf",
   "metadata": {},
   "source": [
    "- 上述操作会报错，因为单词 \"Hello\" 不在词汇表中  \n",
    "- 为了处理这种情况，我们可以向词汇表中添加特殊令牌，例如 `\"<|unk|>\"`，用于表示未知单词  \n",
    "- 既然我们已经在扩展词汇表，可以再添加一个名为 `\"<|endoftext|>\"` 的令牌，它在 GPT-2 训练中用于表示文本结束（同时也用于连接文本，例如当训练数据集包含多篇文章、书籍等时）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 构建文本的词汇表（vocabulary）并为每个 token 分配唯一整数索引\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 获取所有唯一 token 并排序\n",
    "# ------------------------------------------------\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "# - preprocessed: 已经经过预处理的 token 列表（文本被分割、去空白）\n",
    "#   例如：['Hello', ',', 'do', 'you', 'like', 'tea', '.', 'Is', 'this', '--', 'a', 'test']\n",
    "# - set(preprocessed):\n",
    "#   - 将列表转换为集合，去掉重复的 token\n",
    "#   - 集合中元素无序\n",
    "# - list(set(preprocessed)):\n",
    "#   - 将集合转换回列表，便于排序和索引\n",
    "# - sorted(...):\n",
    "#   - 对 token 列表进行字母顺序排序\n",
    "#   - 确保每次生成的 vocab 顺序一致\n",
    "# - all_tokens 示例输出：\n",
    "#   ['--', '.', ',', 'Is', 'Hello', 'a', 'do', 'like', 'tea', 'test', 'this', 'you']\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 添加特殊 token\n",
    "# ------------------------------------------------\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "# - extend 方法将列表元素逐个添加到 all_tokens 末尾\n",
    "# - 特殊 token 用途：\n",
    "#     1) \"<|endoftext|>\": 文本结束标记\n",
    "#     2) \"<|unk|>\": 未知 token（在词汇表中不存在的单词）\n",
    "# - 添加特殊 token 后，all_tokens 列表完整，包含所有实际 token 和特殊 token\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 构建 token -> index 映射字典\n",
    "# ------------------------------------------------\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "# - 字典推导式用于生成词汇表映射\n",
    "# - enumerate(all_tokens)：\n",
    "#     - 遍历 all_tokens 列表\n",
    "#     - 返回索引（integer）和对应 token\n",
    "#     - 示例：\n",
    "#         0 -> '--'\n",
    "#         1 -> '.'\n",
    "#         2 -> ','\n",
    "#         ...\n",
    "# - {token: integer for integer, token in enumerate(all_tokens)}：\n",
    "#     - 将 token 作为字典 key，整数索引作为 value\n",
    "#     - 构建 token -> index 映射\n",
    "# - vocab 示例：\n",
    "#     {\n",
    "#         '--': 0,\n",
    "#         '.': 1,\n",
    "#         ',': 2,\n",
    "#         'Is': 3,\n",
    "#         'Hello': 4,\n",
    "#         'a': 5,\n",
    "#         'do': 6,\n",
    "#         'like': 7,\n",
    "#         'tea': 8,\n",
    "#         'test': 9,\n",
    "#         'this': 10,\n",
    "#         'you': 11,\n",
    "#         '<|endoftext|>': 12,\n",
    "#         '<|unk|>': 13\n",
    "#     }\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 教学要点总结\n",
    "# ------------------------------------------------\n",
    "# 1. 使用 set 去重，确保每个 token 唯一\n",
    "# 2. sorted 保证词汇表顺序固定，便于教学和调试\n",
    "# 3. 添加特殊 token 是 NLP 模型的标准做法\n",
    "# 4. enumerate + 字典推导式生成 token -> index 映射，方便后续编码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57c3143b-e860-4d3b-a22a-de22b547a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表总大小： 1132\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 计算词汇表（vocab）中包含的 token 数量\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. vocab.items()\n",
    "# ------------------------------------------------\n",
    "# - vocab: 之前创建的 token -> index 映射字典\n",
    "# - vocab.items() 返回字典的所有键值对（key-value tuple）\n",
    "#   - 格式为可迭代对象，每个元素是 (token, index) 元组\n",
    "#   - 示例：\n",
    "#       dict_items([('--', 0), ('.', 1), (',', 2), ('Is', 3), ...])\n",
    "# - 这个方法可以遍历词汇表的每个 token 和对应索引\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. len(...)\n",
    "# ------------------------------------------------\n",
    "# - len() 内置函数，用于计算可迭代对象元素的数量\n",
    "# - len(vocab.items()) 返回 vocab 字典中键值对的数量\n",
    "# - 也就是词汇表中不同 token 的总数，包括特殊 token\n",
    "# - 示例输出：\n",
    "#     如果 vocab 中有 14 个 token，则 len(vocab.items()) = 14\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 教学用法\n",
    "# ------------------------------------------------\n",
    "# 这行代码可以帮助学生：\n",
    "# 1) 直观了解词汇表大小\n",
    "# 2) 检查是否包含特殊 token，如 \"<|endoftext|>\", \"<|unk|>\"\n",
    "# 3) 为编码和模型输入维度提供参考\n",
    "vocab_size = len(vocab.items())\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. 输出词汇表大小\n",
    "# ------------------------------------------------\n",
    "print(\"词汇表总大小：\", vocab_size)\n",
    "# - 输出示例：\n",
    "#   词汇表总大小： 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50e51bb1-ae05-4aa8-a9ff-455b65ed1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何查看词汇表（vocab）中最后几个 token 及其索引\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 遍历词汇表最后 5 个元素\n",
    "# ------------------------------------------------\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    # vocab.items(): 返回字典的所有键值对，格式为 dict_items([(token, index), ...])\n",
    "    # list(vocab.items()): 将 dict_items 转换为列表，以便支持切片操作\n",
    "    # [-5:]: 切片操作，获取列表最后 5 个元素\n",
    "    # enumerate(...):\n",
    "    #   - 遍历切片后的列表\n",
    "    #   - i 是循环索引（从 0 开始）\n",
    "    #   - item 是当前元素，即 (token, index) 元组\n",
    "    #     例如：('<|endoftext|>', 12)\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 2. 输出当前 token 和索引\n",
    "    # ------------------------------------------------\n",
    "    print(item)\n",
    "    # - 输出格式为 (token, index)\n",
    "    # - 示例输出：\n",
    "    #     ('this', 10)\n",
    "    #     ('you', 11)\n",
    "    #     ('<|endoftext|>', 12)\n",
    "    #     ('<|unk|>', 13)\n",
    "    # - 可用于教学观察词汇表末尾的特殊 token 或最后几个普通 token\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 教学要点总结\n",
    "# ------------------------------------------------\n",
    "# 1. list(vocab.items())[-5:] 可以快速获取词汇表最后几个 token\n",
    "# 2. enumerate 可以同时获得索引和元素，便于在循环中操作或打印\n",
    "# 3. 输出内容帮助学生直观理解词汇表结构和特殊 token 的位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1daa2b0-6e75-412b-ab53-1f6fb7b4d453",
   "metadata": {},
   "source": [
    "- 我们还需要相应地调整分词器，以便它知道何时以及如何使用新的 `<unk>` 令牌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "948861c5-3f30-4712-a234-725f20d26f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# SimpleTokenizerV2 类实现了一个改进的简单分词器\n",
    "# 相比 V1，增加了对未知 token 的处理\n",
    "# ================================================================\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    # ------------------------------------------------\n",
    "    # 1. 初始化方法\n",
    "    # ------------------------------------------------\n",
    "    def __init__(self, vocab):\n",
    "        # vocab: 传入的词汇表字典，token -> index\n",
    "        # self.str_to_int: 保存 token -> index 映射\n",
    "        self.str_to_int = vocab\n",
    "        \n",
    "        # self.int_to_str: 生成 index -> token 反向映射\n",
    "        # 字典推导式 {i:s for s,i in vocab.items()}：\n",
    "        # - 遍历 vocab.items() 返回 (token, index)\n",
    "        # - 交换顺序，将 index 作为 key，token 作为 value\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # 2. 编码方法\n",
    "    # ------------------------------------------------\n",
    "    def encode(self, text):\n",
    "        # preprocessed: 使用正则分割文本，捕获标点和空白\n",
    "        # re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        # - 括号内的模式表示捕获组，匹配标点符号、破折号和空格\n",
    "        # - 捕获组会保留在分割结果列表中\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        \n",
    "        # 去掉空白或空字符串，并去除首尾空格\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        \n",
    "        # 处理未知 token：\n",
    "        # - 遍历 preprocessed\n",
    "        # - 如果 token 存在于 str_to_int 中，则保持原 token\n",
    "        # - 否则替换为 \"<|unk|>\"，表示未知 token\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        # token -> index 映射\n",
    "        # - 遍历 preprocessed 列表\n",
    "        # - 使用 str_to_int 将每个 token 转为整数索引\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        \n",
    "        # 返回编码后的整数序列 ids\n",
    "        return ids\n",
    "        \n",
    "    # ------------------------------------------------\n",
    "    # 3. 解码方法\n",
    "    # ------------------------------------------------\n",
    "    def decode(self, ids):\n",
    "        # ids: 待解码的整数序列列表\n",
    "        # [self.int_to_str[i] for i in ids]:\n",
    "        # - 将每个整数索引映射回 token\n",
    "        # - 返回 token 列表\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # - 用空格拼接 token 列表\n",
    "        # - 此时标点前可能存在多余空格\n",
    "        \n",
    "        # 使用正则去掉标点前的多余空格\n",
    "        # re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        # - 匹配空格后跟标点符号的情况\n",
    "        # - \\1 表示保留标点，删除空格\n",
    "        # - 返回自然文本\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        \n",
    "        # 返回解码后的文本字符串\n",
    "        return text\n",
    "\n",
    "# ================================================================\n",
    "# 教学要点总结\n",
    "# ================================================================\n",
    "# 1. SimpleTokenizerV2 是 V1 的升级版，增加了对未知 token 的处理\n",
    "# 2. encode 方法：\n",
    "#    - 分割文本 → 去空白 → 替换未知 token → 转为整数序列\n",
    "# 3. decode 方法：\n",
    "#    - 整数序列 → token 列表 → 拼接字符串 → 修正标点空格\n",
    "# 4. \"<|unk|>\" 可以保证模型在遇到词汇表外的 token 时不会报错\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa728dd1-9d35-4ac7-938f-d411d73083f6",
   "metadata": {},
   "source": [
    "- 让我们尝试使用修改后的分词器对文本进行分词：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何使用 SimpleTokenizerV2 构建分词器实例并处理多段文本\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 创建分词器实例\n",
    "# ------------------------------------------------\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "# - tokenizer: SimpleTokenizerV2 类的实例\n",
    "# - vocab: 之前创建的 token -> index 映射字典\n",
    "# - 实例化时会初始化：\n",
    "#     - self.str_to_int: token -> index 映射\n",
    "#     - self.int_to_str: index -> token 反向映射\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 定义多段文本\n",
    "# ------------------------------------------------\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "# - text1: 第一段文本字符串\n",
    "\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "# - text2: 第二段文本字符串\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 合并多段文本\n",
    "# ------------------------------------------------\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "# - tuple (text1, text2): 将两个文本组合成元组，作为 join 的可迭代对象\n",
    "# - \" <|endoftext|> \".join(...):\n",
    "#     - 在每段文本之间插入特殊 token \"<|endoftext|>\" 作为段落/文本分隔符\n",
    "#     - 生成新的字符串 text：\n",
    "#       \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"\n",
    "# - 这样做可以让模型或分词器在处理多段文本时知道段落边界\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. 输出合并后的文本\n",
    "# ------------------------------------------------\n",
    "print(text)\n",
    "# - 输出示例：\n",
    "#   Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
    "# - 教学用途：\n",
    "#   1) 展示如何用特殊 token 分隔多段文本\n",
    "#   2) 便于后续进行 encode 编码，保留段落信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ed395fe-dc1b-4ed2-b85b-457cc35aab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "编码后的整数序列 ids：\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何使用 SimpleTokenizerV2 对合并后的多段文本进行编码（encode）\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 方法调用\n",
    "# ------------------------------------------------\n",
    "tokenizer.encode(text)\n",
    "# - tokenizer: SimpleTokenizerV2 类实例\n",
    "#   - 已经用 vocab 初始化，包含 str_to_int 映射（token -> index）\n",
    "# - encode(text): 分词器方法，用于将文本 text 转换为整数序列 ids\n",
    "# - text: 合并后的多段文本，段落间用特殊 token \"<|endoftext|>\" 分隔\n",
    "#   例如：\n",
    "#     \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. encode 方法内部流程（详细解析）\n",
    "# ------------------------------------------------\n",
    "# 1) 文本分割：\n",
    "#    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "#    - 按标点符号、破折号和空格分割文本\n",
    "#    - 保留捕获的标点符号\n",
    "#\n",
    "# 2) 去掉空白：\n",
    "#    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "#    - 去掉首尾空格\n",
    "#    - 去掉空字符串或仅空格的 token\n",
    "#\n",
    "# 3) 处理未知 token：\n",
    "#    preprocessed = [\n",
    "#        item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed\n",
    "#    ]\n",
    "#    - 如果 token 在词汇表中，保留原 token\n",
    "#    - 如果 token 不在词汇表中，替换为 \"<|unk|>\"\n",
    "#    - 特别注意：\"<|endoftext|>\" 已经在 vocab 中，确保不会被替换为 \"<|unk|>\"\n",
    "#\n",
    "# 4) token -> index 映射：\n",
    "#    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "#    - 将每个 token 映射为对应整数索引\n",
    "#    - 返回整数序列 ids，便于作为模型输入\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 输出编码结果\n",
    "# ------------------------------------------------\n",
    "ids = tokenizer.encode(text)\n",
    "print(\"编码后的整数序列 ids：\")\n",
    "print(ids)\n",
    "# - ids 是整数列表\n",
    "# - 可以直接用于模型输入\n",
    "# - 特殊 token \"<|endoftext|>\" 对应特定整数索引\n",
    "# - 教学用途：观察文本被分割、token 映射为整数的全过程\n",
    "\n",
    "# ================================================================\n",
    "# 教学要点总结\n",
    "# ================================================================\n",
    "# 1. encode 方法将文本转换为整数序列，是 NLP 模型输入的核心步骤\n",
    "# 2. 支持多段文本和特殊 token \"<|endoftext|>\"\n",
    "# 3. 内部流程：\n",
    "#    分割文本 → 去空白 → 处理未知 token → token 映射为整数\n",
    "# 4. 输出 ids 可以用来理解 token 化和编码的效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "059367f9-7a60-4c0d-8a00-7c4c766d0ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本：\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "\n",
      "编码-解码后的文本：\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示对多段文本进行编码后立即解码的完整流程\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 组合调用 encode 和 decode\n",
    "# ------------------------------------------------\n",
    "#tokenizer.decode(tokenizer.encode(text))\n",
    "# - tokenizer.encode(text):\n",
    "#     1) 将文本 text 分割成 token\n",
    "#     2) 去掉空白和空字符串\n",
    "#     3) 对未知 token 替换为 \"<|unk|>\"\n",
    "#     4) 将 token 映射为整数索引，返回 ids 列表\n",
    "# - tokenizer.decode(...):\n",
    "#     1) 将整数索引 ids 转回 token\n",
    "#     2) 用空格拼接 token 列表\n",
    "#     3) 使用正则去掉标点前多余空格\n",
    "#     4) 返回可读文本字符串\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 内部流程示例（假设 text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"）\n",
    "# ------------------------------------------------\n",
    "# 1) encode(text) 生成整数序列 ids：\n",
    "#    例如：\n",
    "#    ids = [4, 2, 6, 11, 7, 8, 3, 12, 10, 5, 15, ...]\n",
    "#    - 其中 12 对应 \"<|endoftext|>\"，其他整数对应词汇表中的 token\n",
    "# 2) decode(ids) 将整数序列还原：\n",
    "#    - ids -> token 列表\n",
    "#    - token 列表 -> 空格拼接字符串\n",
    "#    - 去掉标点前多余空格\n",
    "#    - 最终得到与原文本非常接近的文本，保留 \"<|endoftext|>\" 作为段落分隔符\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 输出结果\n",
    "# ------------------------------------------------\n",
    "decoded_text = tokenizer.decode(tokenizer.encode(text))\n",
    "print(\"原始文本：\")\n",
    "print(text)\n",
    "print(\"\\n编码-解码后的文本：\")\n",
    "print(decoded_text)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. 教学要点总结\n",
    "# ------------------------------------------------\n",
    "# 1. encode -> decode 组合展示了分词器的完整流程\n",
    "# 2. 多段文本和特殊 token \"<|endoftext|>\" 可以被正确处理\n",
    "# 3. 输出结果可用于教学观察：\n",
    "#    - token 化和整数映射\n",
    "#    - 解码后的文本是否接近原文\n",
    "# 4. 对于词汇表外的 token，会被替换为 \"<|unk|>\" 并解码回 \"<|unk|>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {},
   "source": [
    "## 2.5 字节对编码（BytePair Encoding, BPE）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
   "metadata": {},
   "source": [
    "- GPT-2 使用字节对编码（BytePair Encoding, BPE）作为其分词器  \n",
    "- 它允许模型将不在预定义词汇表中的单词拆分为更小的子词单元甚至单个字符，从而处理词汇表外的单词  \n",
    "- 例如，如果 GPT-2 的词汇表中没有单词 \"unfamiliarword\"，它可能会将其分词为 [\"unfam\", \"iliar\", \"word\"] 或其他子词拆分方式，这取决于训练时的 BPE 合并规则  \n",
    "- 原始 BPE 分词器可在此找到：[https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)  \n",
    "- 在本章中，我们使用 OpenAI 开源库 [tiktoken](https://github.com/openai/tiktoken) 提供的 BPE 分词器，该库的核心算法用 Rust 实现，以提升计算性能  \n",
    "- 我在 [./bytepair_encoder](../02_bonus_bytepair-encoder) 中创建了一个笔记本，对比了这两种实现（在示例文本上 tiktoken 大约快 5 倍）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ede1d41f-934b-4bf4-8184-54394a257a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何导入 tiktoken 库并查看其版本号\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 导入模块\n",
    "# ------------------------------------------------\n",
    "import importlib\n",
    "# - importlib: Python 内置库，用于动态导入模块、获取模块元数据等\n",
    "# - 本例中用于获取已安装包的版本号\n",
    "\n",
    "import tiktoken\n",
    "# - tiktoken: OpenAI 提供的高性能分词库\n",
    "# - 可用于 GPT 模型的分词（tokenization）和 token 计数\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 打印 tiktoken 版本\n",
    "# ------------------------------------------------\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
    "# - importlib.metadata.version(\"tiktoken\"):\n",
    "#     - 获取当前安装的 tiktoken 库的版本号（字符串类型）\n",
    "#     - 例如：\"0.7.0\"\n",
    "# - print(...):\n",
    "#     - 输出示例：\n",
    "#       tiktoken version: 0.7.0\n",
    "# - 教学用途：\n",
    "#     1) 确认所用 tiktoken 版本，便于复现代码\n",
    "#     2) 避免因版本差异导致 API 或分词结果不同\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 使用 tiktoken 创建 GPT-2 编码器（tokenizer）\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 创建 GPT-2 编码器\n",
    "# ------------------------------------------------\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# - tiktoken.get_encoding(\"gpt2\"):\n",
    "#     - 调用 tiktoken 库的 get_encoding 函数\n",
    "#     - 参数 \"gpt2\": 指定编码器类型为 GPT-2 使用的 BPE (Byte-Pair Encoding)\n",
    "# - 返回值：\n",
    "#     - tokenizer 对象，用于文本的编码（encode）和解码（decode）\n",
    "#     - 支持方法：\n",
    "#         1) encode(text) -> 将文本转换为 token id 列表\n",
    "#         2) decode(ids) -> 将 token id 列表还原为文本\n",
    "#         3) encode_ordinary(text) / decode_ordinary(text) 等高级方法\n",
    "# - 教学用途：\n",
    "#     1) 演示如何获取官方 GPT-2 分词器\n",
    "#     2) 准备后续文本编码和 token 数量统计\n",
    "#     3) 与 SimpleTokenizerV2 对比，展示官方 BPE 分词器和自定义分词器的区别\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ff2cd85-7cfb-4325-b390-219938589428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 使用 tiktoken 的 GPT-2 编码器对文本进行编码（encode）\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 定义待编码文本\n",
    "# ------------------------------------------------\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "# - text: 待编码的字符串文本\n",
    "# - 注意：\n",
    "#     1) 使用了 Python 的括号和字符串拼接特性\n",
    "#        - 两个字符串直接放在括号内，会自动连接\n",
    "#        - \"terraces\" 和 \"of someunknownPlace.\" 会被合并为一个连续字符串\n",
    "#     2) 文本中包含：\n",
    "#        - 普通单词: Hello, do, you, like, tea, In, the, sunlit, terraces, of, someunknownPlace\n",
    "#        - 标点: , ? .\n",
    "#        - 特殊 token: <|endoftext|>，用于指示文本结束或段落分隔\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 编码文本\n",
    "# ------------------------------------------------\n",
    "integers = tokenizer.encode(\n",
    "    text,\n",
    "    allowed_special={\"<|endoftext|>\"}\n",
    ")\n",
    "# - tokenizer: tiktoken.get_encoding(\"gpt2\") 返回的编码器对象\n",
    "# - encode(text, allowed_special={...}) 方法：\n",
    "#     1) 将文本 text 转换为 token id 列表（整数序列）\n",
    "#     2) allowed_special: 指定允许保留的特殊 token\n",
    "#        - \"<|endoftext|>\": 会被作为单个 token 保留，不会拆分或报错\n",
    "#     3) 遇到其他非词汇表 token 会根据 GPT-2 BPE 规则拆分为子 token\n",
    "# - 返回值 integers：\n",
    "#     - 一个整数列表，每个元素表示对应 token 的 id\n",
    "#     - 示例输出可能类似：\n",
    "#       [15496, 11, 703, 345, 284, 30, 50256, 315, 262, 12850, 2394, 13]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 输出编码结果\n",
    "# ------------------------------------------------\n",
    "print(integers)\n",
    "# - 输出整数列表\n",
    "# - 教学用途：\n",
    "#     1) 观察文本被拆分为 GPT-2 token 的过程\n",
    "#     2) 了解特殊 token \"<|endoftext|>\" 被保留为单个 token\n",
    "#     3) 与自定义 SimpleTokenizer 对比，展示官方 BPE 分词器处理未知单词的方式\n",
    "#        - 例如 \"someunknownPlace\" 会被拆分为多个子 token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 使用 tiktoken 的 GPT-2 编码器对整数 token id 列表进行解码（decode）\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 解码整数序列\n",
    "# ------------------------------------------------\n",
    "strings = tokenizer.decode(integers)\n",
    "# - tokenizer: tiktoken.get_encoding(\"gpt2\") 返回的编码器对象\n",
    "# - decode(integers) 方法：\n",
    "#     1) 将整数列表 integers 转换回文本字符串\n",
    "#     2) 每个整数对应一个 token id\n",
    "#     3) 解码器会按照 GPT-2 BPE 规则将 token id 转回对应的子词或单词\n",
    "#     4) 特殊 token \"<|endoftext|>\" 会被恢复为文本 \"<|endoftext|>\"\n",
    "# - 返回值 strings：\n",
    "#     - 解码后的字符串文本\n",
    "#     - 示例：\n",
    "#       \"Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\"\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 输出解码结果\n",
    "# ------------------------------------------------\n",
    "print(strings)\n",
    "# - 输出文本字符串\n",
    "# - 教学用途：\n",
    "#     1) 对比原始文本和解码文本\n",
    "#     2) 观察 GPT-2 BPE 分词器处理未知单词和特殊 token 的效果\n",
    "#     3) 理解 encode -> decode 的可逆性（除了 BPE 可能对空格有微调）\n",
    "# - 注意：\n",
    "#     - 在 BPE 解码中，可能出现原始空格与 token 拼接的差异\n",
    "#     - 例如 \"terracesof\" 是原文本 \"terraces of\" 被拆分为子 token 后重新拼接的结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
   "metadata": {},
   "source": [
    "- BPE 分词器将未知单词拆分为子词和单个字符：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    "## 2.6 使用滑动窗口进行数据采样\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
   "metadata": {},
   "source": [
    "- 我们训练 LLM 以一次生成一个单词，因此需要相应地准备训练数据，其中序列中的下一个单词作为预测目标：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "848d5ade-fd1f-46c3-9e31-1426e315c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 使用 tiktoken GPT-2 编码器将完整文本文件编码为 token id 列表\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 读取文本文件\n",
    "# ------------------------------------------------\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "# - open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\"):\n",
    "#     1) 打开当前目录下的 \"the-verdict.txt\" 文件\n",
    "#     2) \"r\" 表示读取模式\n",
    "#     3) encoding=\"utf-8\" 保证文本按 UTF-8 编码读取\n",
    "# - f.read():\n",
    "#     1) 读取文件中所有内容\n",
    "#     2) 返回字符串类型，保存到变量 raw_text\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 使用 tiktoken GPT-2 编码器编码文本\n",
    "# ------------------------------------------------\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "# - tokenizer: tiktoken.get_encoding(\"gpt2\") 返回的编码器对象\n",
    "# - encode(raw_text):\n",
    "#     1) 将文本 raw_text 转换为整数 token id 列表\n",
    "#     2) 对文本进行 BPE 分词\n",
    "#     3) 每个 token 对应一个整数 id\n",
    "# - enc_text:\n",
    "#     - 一个整数列表，长度等于文本被拆分后的 token 数量\n",
    "#     - 教学用途：\n",
    "#         1) 展示大文本在 GPT-2 分词器下被拆分成多少 token\n",
    "#         2) 为模型输入长度和 token 计数做参考\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 输出编码后的 token 数量\n",
    "# ------------------------------------------------\n",
    "print(len(enc_text))\n",
    "# - len(enc_text):\n",
    "#     1) 计算编码后的整数列表长度\n",
    "#     2) 即文本被 GPT-2 分词器拆分后的 token 数量\n",
    "# - 输出示例：\n",
    "#     15234\n",
    "# - 教学用途：\n",
    "#     1) 了解大文本经过 GPT-2 分词后的 token 数量\n",
    "#     2) 观察 token 数量与原字符数、单词数的关系\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd0657-5543-43ca-8011-2ae6bd0a5810",
   "metadata": {},
   "source": [
    "- 对于每个文本块，我们需要输入（inputs）和目标（targets）  \n",
    "- 由于我们希望模型预测下一个单词，目标就是将输入向右移动一个位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e84424a7-646d-45b6-99e3-80d15fb761f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 从编码后的 token 序列中提取子序列（slice）\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 提取 token 子序列\n",
    "# ------------------------------------------------\n",
    "enc_sample = enc_text[50:]\n",
    "# - enc_text: 之前用 tokenizer.encode(raw_text) 编码得到的整数 token 列表\n",
    "# - [50:]: 切片操作\n",
    "#     1) 从列表的第 50 个元素（索引 50）开始\n",
    "#     2) 直到列表末尾\n",
    "# - enc_sample:\n",
    "#     - 一个新的整数列表\n",
    "#     - 包含原始 enc_text 中从第 50 个 token 开始到最后的所有 token\n",
    "# - 教学用途：\n",
    "#     1) 演示如何对 token 列表进行切片操作\n",
    "#     2) 可以用于生成模型输入的子序列或 mini-batch\n",
    "#     3) 便于观察部分文本编码效果而不处理整个大文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfbff852-a92f-48c8-a46d-143a0f109f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何从 token 序列中构建简单的上下文窗口 (context window)\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 定义上下文窗口大小\n",
    "# ------------------------------------------------\n",
    "context_size = 4\n",
    "# - context_size: 整数，表示我们希望取多少个连续 token 作为输入上下文\n",
    "# - 教学用途：\n",
    "#     1) 模拟语言模型的输入长度\n",
    "#     2) 每次用 context_size 个 token 来预测下一个 token\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 构建输入序列 x 和目标序列 y\n",
    "# ------------------------------------------------\n",
    "x = enc_sample[:context_size]\n",
    "# - x: 输入序列 (context tokens)\n",
    "# - enc_sample[:context_size]:\n",
    "#     1) 切片操作，取 enc_sample 列表的前 context_size 个 token\n",
    "#     2) 索引范围：[0, context_size-1]\n",
    "# - 教学用途：\n",
    "#     - 模型用 x 来预测下一个 token\n",
    "\n",
    "y = enc_sample[1:context_size+1]\n",
    "# - y: 目标序列 (target tokens)\n",
    "# - enc_sample[1:context_size+1]:\n",
    "#     1) 从索引 1 开始取 context_size 个 token\n",
    "#     2) 与 x 对应，表示 x 的每个 token 之后的下一个 token\n",
    "# - 教学用途：\n",
    "#     - 用于训练语言模型时作为监督目标\n",
    "#     - 保证 y 与 x 对齐，长度相同，但每个元素是 x 对应的下一个 token\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 输出结果\n",
    "# ------------------------------------------------\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "# - 输出示例：\n",
    "#   x: [50256, 15496, 11, 703]\n",
    "#   y: [15496, 11, 703, 345]\n",
    "# - 教学用途：\n",
    "#     1) 直观展示上下文与预测目标的对应关系\n",
    "#     2) x[i] 的下一个 token 对应 y[i]\n",
    "#     3) 为后续语言模型训练提供基础理解\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815014ef-62f7-4476-a6ad-66e20e42b7c3",
   "metadata": {},
   "source": [
    "- 一个接一个地，预测过程如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d97b031e-ed55-409d-95f2-aeb38c6fe366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何使用动态长度的上下文序列来预测下一个 token\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 遍历 1 到 context_size 的长度\n",
    "# ------------------------------------------------\n",
    "for i in range(1, context_size+1):\n",
    "    # range(1, context_size+1):\n",
    "    # - 从 1 开始，到 context_size 结束（包含 context_size）\n",
    "    # - 用于动态构建不同长度的上下文\n",
    "    # - 教学用途：\n",
    "    #     1) 展示从 1 个 token 到 context_size 个 token 的预测示例\n",
    "    #     2) 帮助理解语言模型的上下文感受野\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. 提取当前上下文序列\n",
    "    # ------------------------------------------------\n",
    "    context = enc_sample[:i]\n",
    "    # - context: 当前上下文 token 列表\n",
    "    # - enc_sample[:i]:\n",
    "    #     1) 切片操作，从第 0 个 token 到第 i-1 个 token\n",
    "    #     2) 动态生成长度为 i 的上下文\n",
    "    # - 教学用途：\n",
    "    #     - 用 context 作为模型输入，预测下一个 token\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3. 提取当前预测目标\n",
    "    # ------------------------------------------------\n",
    "    desired = enc_sample[i]\n",
    "    # - desired: 当前上下文对应的目标 token\n",
    "    # - enc_sample[i]:\n",
    "    #     1) 第 i 个 token（索引 i）\n",
    "    #     2) 即 context 序列之后的下一个 token\n",
    "    # - 教学用途：\n",
    "    #     - 演示 x → y 对应关系\n",
    "    #     - x 可以长度不同，y 始终是 context 的下一个 token\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 4. 输出当前上下文与目标 token\n",
    "    # ------------------------------------------------\n",
    "    print(context, \"---->\", desired)\n",
    "    # - 输出示例（假设 enc_sample 前几个 token 为 [50256, 15496, 11, 703, 345]）：\n",
    "    #   [50256] ----> 15496\n",
    "    #   [50256, 15496] ----> 11\n",
    "    #   [50256, 15496, 11] ----> 703\n",
    "    #   [50256, 15496, 11, 703] ----> 345\n",
    "    # - 教学用途：\n",
    "    #     1) 直观展示逐步增加上下文长度预测下一个 token 的过程\n",
    "# ================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f57bd746-dcbf-4433-8e24-ee213a8c34a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何将上下文 token 和目标 token 解码为可读文本进行观察\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 遍历 1 到 context_size 的长度\n",
    "# ------------------------------------------------\n",
    "for i in range(1, context_size+1):\n",
    "    # range(1, context_size+1):\n",
    "    # - 从 1 开始，到 context_size 结束（包含 context_size）\n",
    "    # - 用于动态生成不同长度的上下文\n",
    "    # - 教学用途：\n",
    "    #     1) 观察短上下文到完整上下文对下一个 token 的预测\n",
    "    #     2) 帮助理解语言模型的上下文感受野\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. 提取当前上下文 token\n",
    "    # ------------------------------------------------\n",
    "    context = enc_sample[:i]\n",
    "    # - context: 当前上下文 token 列表\n",
    "    # - enc_sample[:i]:\n",
    "    #     1) 切片操作，从索引 0 到 i-1\n",
    "    #     2) 动态生成长度为 i 的上下文\n",
    "    # - 教学用途：\n",
    "    #     - 用 context 作为模型输入观察语言模型预测目标 token\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3. 提取当前目标 token\n",
    "    # ------------------------------------------------\n",
    "    desired = enc_sample[i]\n",
    "    # - desired: 当前上下文对应的目标 token\n",
    "    # - enc_sample[i]: 索引 i 的 token\n",
    "    # - 教学用途：\n",
    "    #     - 对比 context 的下一个 token，用于理解语言模型训练\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 4. 解码上下文和目标 token 为可读文本\n",
    "    # ------------------------------------------------\n",
    "    # 解码上下文 token 列表为文本\n",
    "    decoded_context = tokenizer.decode(context)\n",
    "    # - tokenizer.decode(context):\n",
    "    #     1) 将 token id 列表 context 转回可读文本字符串\n",
    "    #     2) 子词拼接为原始单词，保留空格和标点\n",
    "\n",
    "    # 解码单个目标 token 列表为文本\n",
    "    decoded_desired = tokenizer.decode([desired])\n",
    "    # - tokenizer.decode([desired]):\n",
    "    #     1) 注意必须传入列表形式 [desired]\n",
    "    #     2) 将单个 token id 转为文本字符串\n",
    "    #     3) 保留 token 对应的字符或单词\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 5. 输出上下文和目标 token 的可读文本\n",
    "    # ------------------------------------------------\n",
    "    print(decoded_context, \"---->\", decoded_desired)\n",
    "    # - 输出示例（假设 context_size = 4，enc_sample 前几个 token 对应文本）：\n",
    "    #   Hello ----> ,\n",
    "    #   Hello, ----> do\n",
    "    #   Hello, do ----> you\n",
    "    #   Hello, do you ----> like\n",
    "    # - 教学用途：\n",
    "    #     1) 直观展示“上下文文本 → 下一个 token 文本”的对应关系\n",
    "    #     2) 帮助理解语言模型训练时 x → y 的逻辑\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d2dd9-fc20-4927-8d3d-1466cf41aae1",
   "metadata": {},
   "source": [
    "- 下一章节中在讲解注意力机制后，我们将处理下一个单词预测  \n",
    "- 目前，我们实现一个简单的数据加载器，它遍历输入数据集并返回向右移动一位的输入和目标\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1b47a-f646-49d1-bc70-fddf2c840796",
   "metadata": {},
   "source": [
    "- 安装并导入 PyTorch（安装提示请参见附录 A）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 演示如何导入 PyTorch 并查看其版本号\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 导入 PyTorch\n",
    "# ------------------------------------------------\n",
    "import torch\n",
    "# - torch: PyTorch 的核心库\n",
    "# - 提供：\n",
    "#     1) 张量（tensor）操作和计算\n",
    "#     2) GPU 加速\n",
    "#     3) 神经网络构建（torch.nn）\n",
    "#     4) 自动求导（autograd）\n",
    "# - 教学用途：\n",
    "#     1) 确认 PyTorch 已正确安装\n",
    "#     2) 为后续深度学习任务准备环境\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 打印 PyTorch 版本\n",
    "# ------------------------------------------------\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "# - torch.__version__:\n",
    "#     1) 获取当前安装的 PyTorch 版本号（字符串类型）\n",
    "#     2) 例如：\"2.1.0+cpu\" 或 \"2.1.0+cu118\"\n",
    "# - print(...):\n",
    "#     - 输出示例：\n",
    "#       PyTorch version: 2.1.0+cu118\n",
    "# - 教学用途：\n",
    "#     1) 确认版本，便于复现实验\n",
    "#     2) 避免因版本差异导致 API 或功能不同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
   "metadata": {},
   "source": [
    "- 我们使用滑动窗口方法，每次位置向右移动 +1：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
   "metadata": {},
   "source": [
    "- 创建数据集和数据加载器，从输入文本数据集中提取文本块\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 自定义 PyTorch Dataset，用于 GPT 风格语言模型训练\n",
    "# ================================================================\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# - Dataset: PyTorch 数据集基类，用于自定义数据加载逻辑\n",
    "# - DataLoader: 数据加载器，可对 Dataset 进行批量加载、shuffle 和多线程加速\n",
    "\n",
    "# ================================================================\n",
    "# 自定义 GPTDatasetV1 类\n",
    "# ================================================================\n",
    "class GPTDatasetV1(Dataset):\n",
    "    # ------------------------------------------------\n",
    "    # 1. 初始化函数\n",
    "    # ------------------------------------------------\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        GPTDatasetV1 初始化函数\n",
    "\n",
    "        参数：\n",
    "        - txt: str，完整文本字符串，例如书籍全文\n",
    "        - tokenizer: 分词器对象，需要提供 encode(text, allowed_special) 方法\n",
    "        - max_length: int，每个训练样本的最大 token 数量（上下文长度）\n",
    "        - stride: int，滑动窗口步长，用于生成重叠训练样本\n",
    "        \"\"\"\n",
    "        # 初始化存储输入和目标 token 的列表\n",
    "        self.input_ids = []   # 存储每个训练样本的输入序列 x 的 token id\n",
    "        self.target_ids = []  # 存储每个训练样本的目标序列 y 的 token id\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # 2. 对整个文本进行编码\n",
    "        # ------------------------------------------------\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        # - tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"}):\n",
    "        #     1) 将完整文本 txt 编码为整数 token id 列表\n",
    "        #     2) allowed_special 参数：\n",
    "        #        - {\"<|endoftext|>\"} 表示保留特殊 token，不拆分或忽略\n",
    "        #     3) 返回值 token_ids 为列表，例如：\n",
    "        #        [50256, 15496, 11, 703, 345, ...]\n",
    "        #     4) 教学用途：\n",
    "        #        - 展示 BPE 分词器对完整文本的 token 化过程\n",
    "        #        - 为语言模型训练提供整数序列输入\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # 3. 检查文本长度是否足够\n",
    "        # ------------------------------------------------\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "        # - 保证文本 token 数量大于 max_length\n",
    "        # - 如果 token 数量不足，无法生成至少一个训练样本\n",
    "        # - 教学用途：\n",
    "        #     1) 演示数据预处理时常见的边界检查\n",
    "        #     2) 提示用户文本长度必须满足训练需求\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # 4. 使用滑动窗口生成训练样本\n",
    "        # ------------------------------------------------\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            # - i: 滑动窗口起始索引\n",
    "            # - range(0, len(token_ids) - max_length, stride):\n",
    "            #     1) 从 0 到 len(token_ids)-max_length（保证切片长度为 max_length）\n",
    "            #     2) 步长为 stride，控制样本之间的重叠长度\n",
    "            #     3) 教学用途：\n",
    "            #        - 演示滑动窗口如何生成重叠训练样本\n",
    "            #        - 增加训练样本数量，提高模型学习上下文能力\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # 4.1 构建输入序列 x\n",
    "            # ------------------------------------------------\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            # - 输入序列 token id 列表\n",
    "            # - 切片范围：[i, i + max_length-1]\n",
    "            # - 长度固定为 max_length\n",
    "            # - 教学用途：\n",
    "            #     - 模型输入 x\n",
    "            #     - 用于语言模型训练\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # 4.2 构建目标序列 y\n",
    "            # ------------------------------------------------\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            # - 目标序列 token id 列表\n",
    "            # - 每个 token 对应输入序列的下一个 token\n",
    "            # - 切片范围：[i+1, i+max_length]\n",
    "            # - 长度同样为 max_length\n",
    "            # - 教学用途：\n",
    "            #     - 模型输出 y（监督信号）\n",
    "            #     - 体现语言模型的“下一个 token 预测”机制\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # 4.3 将列表转换为 PyTorch 张量并存储\n",
    "            # ------------------------------------------------\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            # - torch.tensor(list):\n",
    "            #     1) 将 Python 列表转换为 PyTorch 张量\n",
    "            #     2) 默认 dtype 为 torch.long（整数）\n",
    "            #     3) 可以直接用于模型训练\n",
    "            # - 教学用途：\n",
    "            #     - 演示数据预处理流程：文本 → token → tensor\n",
    "            #     - 为 DataLoader 提供可迭代张量数据\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 5. 返回数据集长度\n",
    "    # ------------------------------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        # - 返回训练样本数量\n",
    "        # - DataLoader 会根据此值生成迭代器\n",
    "        # - 教学用途：\n",
    "        #     - 展示 Dataset 接口要求\n",
    "        #     - 核心方法 __len__ 用于迭代样本数量控制\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 6. 根据索引获取样本\n",
    "    # ------------------------------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取指定索引的训练样本\n",
    "        \n",
    "        参数：\n",
    "        - idx: int，样本索引，0 <= idx < len(self.input_ids)\n",
    "\n",
    "        返回：\n",
    "        - (input_tensor, target_tensor)\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "        # - 返回值：\n",
    "        #     1) input_tensor: 输入 token 张量 x\n",
    "        #     2) target_tensor: 目标 token 张量 y\n",
    "        # - 教学用途：\n",
    "        #     - 展示 Dataset 获取样本的标准方法\n",
    "        #     - DataLoader 调用 __getitem__ 来批量加载训练数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 创建用于 GPT 语言模型训练的 PyTorch DataLoader\n",
    "# ================================================================\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \"\"\"\n",
    "    创建 DataLoader 用于批量训练 GPT 风格语言模型\n",
    "\n",
    "    参数：\n",
    "    - txt: str，完整文本字符串（训练语料）\n",
    "    - batch_size: int，每个批次包含的样本数量（默认 4）\n",
    "    - max_length: int，每个训练样本的最大 token 长度（上下文长度，默认 256）\n",
    "    - stride: int，滑动窗口步长，用于生成重叠样本（默认 128）\n",
    "    - shuffle: bool，是否对样本顺序进行随机打乱（默认 True）\n",
    "    - drop_last: bool，是否丢弃最后不足 batch_size 的样本（默认 True）\n",
    "    - num_workers: int，DataLoader 使用的子进程数量（默认 0，表示使用主进程）\n",
    "    \n",
    "    返回：\n",
    "    - dataloader: PyTorch DataLoader 对象，可迭代生成训练批次\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. 初始化 tokenizer\n",
    "    # ------------------------------------------------\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    # - tiktoken.get_encoding(\"gpt2\"):\n",
    "    #     1) 获取 GPT-2 分词器\n",
    "    #     2) 提供 encode(text) 和 decode(token_ids) 方法\n",
    "    # - 教学用途：\n",
    "    #     - 演示如何加载标准 GPT-2 分词器\n",
    "    #     - 为 dataset 构建 token id 序列\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. 创建 GPTDatasetV1 数据集\n",
    "    # ------------------------------------------------\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    # - GPTDatasetV1(txt, tokenizer, max_length, stride):\n",
    "    #     1) 自定义 Dataset，将文本切片为重叠 token 序列\n",
    "    #     2) dataset.input_ids / dataset.target_ids 包含训练样本\n",
    "    # - 教学用途：\n",
    "    #     - 展示文本如何预处理为模型可训练样本\n",
    "    #     - 输入文本 → token id → dataset 张量序列\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3. 创建 PyTorch DataLoader\n",
    "    # ------------------------------------------------\n",
    "    dataloader = DataLoader(\n",
    "        dataset,             # 数据集对象\n",
    "        batch_size=batch_size, # 每个批次样本数量\n",
    "        shuffle=shuffle,       # 是否打乱样本顺序\n",
    "        drop_last=drop_last,   # 是否丢弃最后不足 batch_size 的批次\n",
    "        num_workers=num_workers # DataLoader 使用的子进程数量\n",
    "    )\n",
    "    # - DataLoader:\n",
    "    #     1) 自动对 Dataset 进行批量采样\n",
    "    #     2) 支持 shuffle（随机打乱）和多线程加速\n",
    "    # - 教学用途：\n",
    "    #     - 展示如何用 Dataset 创建可迭代的训练数据\n",
    "    #     - 模拟训练循环中常用的数据加载方式\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 4. 返回 DataLoader\n",
    "    # ------------------------------------------------\n",
    "    return dataloader\n",
    "    # - dataloader:\n",
    "    #     1) 可迭代对象\n",
    "    #     2) 每次迭代返回一个 batch: (input_tensor_batch, target_tensor_batch)\n",
    "    #     3) input_tensor_batch 和 target_tensor_batch 形状为：\n",
    "    #        [batch_size, max_length]，类型为 torch.LongTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd",
   "metadata": {},
   "source": [
    "- 让我们使用批量大小为 1 来测试数据加载器，LLM 的上下文长度为 4：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df31d96c-6bfd-4564-a956-6192242d7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 读取文本文件内容到 Python 字符串\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 打开文件\n",
    "# ------------------------------------------------\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    # - open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\"):\n",
    "    #     1) \"the-verdict.txt\" 是文件路径\n",
    "    #     2) \"r\" 表示以只读模式打开文件\n",
    "    #     3) encoding=\"utf-8\" 表示按 UTF-8 编码读取文本\n",
    "    #        - 确保非 ASCII 字符（如中文、特殊符号）正确读取\n",
    "    # - with 语句：\n",
    "    #     - 上下文管理器，确保文件使用完毕后自动关闭\n",
    "    #     - 避免忘记调用 f.close() 导致资源泄漏\n",
    "    # - f:\n",
    "    #     - 文件对象（file object）\n",
    "    #     - 提供 read()、readline() 等方法读取文件内容\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. 读取文件全部内容\n",
    "    # ------------------------------------------------\n",
    "    raw_text = f.read()\n",
    "    # - f.read():\n",
    "    #     1) 读取文件中所有内容，并返回字符串类型\n",
    "    #     2) 教学用途：\n",
    "    #         - 将文本内容加载到内存中进行处理\n",
    "    #         - 方便后续分词、编码、Dataset 构建等操作\n",
    "    # - raw_text:\n",
    "    #     - str 类型，保存整个文件的文本\n",
    "    #     - 可以直接传入 tokenizer.encode 或自定义 Dataset 使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 使用 create_dataloader_v1 创建 DataLoader 并获取第一个训练批次\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 创建 DataLoader\n",
    "# ------------------------------------------------\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,        # 原始文本字符串\n",
    "    batch_size=1,    # 每个批次只包含 1 个训练样本\n",
    "    max_length=4,    # 每个训练样本的 token 序列长度为 4\n",
    "    stride=1,        # 滑动窗口步长为 1，即每次生成的样本只错开 1 个 token\n",
    "    shuffle=False    # 不打乱顺序，保证样本顺序与文本顺序一致\n",
    ")\n",
    "# - 返回 dataloader: PyTorch DataLoader 对象，可迭代生成训练批次\n",
    "# - 教学用途：\n",
    "#     1) 演示如何从文本创建可迭代的数据加载器\n",
    "#     2) small batch_size 和小 max_length 方便调试和展示\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 将 DataLoader 转换为迭代器\n",
    "# ------------------------------------------------\n",
    "data_iter = iter(dataloader)\n",
    "# - iter(dataloader):\n",
    "#     1) 将 DataLoader 转换为迭代器\n",
    "#     2) 可使用 next() 获取下一个 batch\n",
    "# - 教学用途：\n",
    "#     - 演示 DataLoader 内部是可迭代对象\n",
    "#     - 方便直接访问 batch 进行调试或教学展示\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 获取第一个训练批次\n",
    "# ------------------------------------------------\n",
    "first_batch = next(data_iter)\n",
    "# - next(data_iter):\n",
    "#     1) 获取迭代器的下一个元素，即第一个 batch\n",
    "#     2) 返回值类型为元组：\n",
    "#        (input_tensor_batch, target_tensor_batch)\n",
    "#     3) 每个 tensor 形状为 [batch_size, max_length]，数据类型 torch.LongTensor\n",
    "# - 教学用途：\n",
    "#     - 直观展示训练批次的输入和目标\n",
    "#     - 帮助学生理解 batch 结构\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. 打印第一个训练批次\n",
    "# ------------------------------------------------\n",
    "print(first_batch)\n",
    "# - 输出示例：\n",
    "#     (\n",
    "#       tensor([[50256, 15496, 11, 703]]),  # input_tensor_batch\n",
    "#       tensor([[15496, 11, 703, 345]])     # target_tensor_batch\n",
    "#     )\n",
    "# - 教学用途：\n",
    "#     1) 查看输入序列与目标序列对应关系\n",
    "#     2) 便于理解 GPT 语言模型训练中 x → y 的映射\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 获取 DataLoader 中的第二个训练批次并打印内容\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 从迭代器中获取第二个批次\n",
    "# ------------------------------------------------\n",
    "second_batch = next(data_iter)\n",
    "# - next(data_iter):\n",
    "#     1) 从 dataloader 迭代器中继续取下一个批次数据\n",
    "#     2) 因为上一个批次（first_batch）已经取出，\n",
    "#        所以此处取出的即为第二个批次\n",
    "# - 返回结果：\n",
    "#     一个元组 (input_tensor_batch, target_tensor_batch)\n",
    "# - 详细说明：\n",
    "#     - input_tensor_batch：模型输入（即上下文 token 序列）\n",
    "#     - target_tensor_batch：模型目标输出（即预测目标 token 序列）\n",
    "# - 二者形状均为 [batch_size, max_length]\n",
    "# - 数据类型：torch.LongTensor（整型张量）\n",
    "# - 教学要点：\n",
    "#     - 展示 DataLoader 是可连续迭代的\n",
    "#     - 理解“滑动窗口”如何在文本上依次滑动形成多个样本\n",
    "#     - 当 stride=1 时，每个样本只向后滑动一个 token\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 打印第二个训练批次内容\n",
    "# ------------------------------------------------\n",
    "print(second_batch)\n",
    "# - 输出示例：\n",
    "#     (\n",
    "#       tensor([[15496, 11, 703, 345]]),\n",
    "#       tensor([[11, 703, 345, 257]])\n",
    "#     )\n",
    "# - 说明：\n",
    "#     - 第一个张量是输入序列 input_tensor_batch\n",
    "#     - 第二个张量是目标序列 target_tensor_batch\n",
    "# - 教学用途：\n",
    "#     1) 观察第二个 batch 的 token 序列变化\n",
    "#     2) 理解输入与目标的“错位”关系：\n",
    "#        target 是 input 向右平移一个位置后的结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006212f-de45-468d-bdee-5806216d1679",
   "metadata": {},
   "source": [
    "- 示例：使用与上下文长度相等的步长（此处为 4），如下所示：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
   "metadata": {},
   "source": [
    "- 我们还可以创建批量输出  \n",
    "- 注意，这里我们增加了步长，以避免批次之间的重叠，因为过多重叠可能导致过拟合增加\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 使用自定义函数 create_dataloader_v1 创建 DataLoader，\n",
    "# 从文本中按滑动窗口分块生成训练样本，\n",
    "# 并打印第一个 batch 的输入张量与目标张量。\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 创建 DataLoader\n",
    "# ------------------------------------------------\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,      # txt: 输入的完整文本内容（之前从 \"the-verdict.txt\" 读取）\n",
    "    batch_size=8,  # batch_size: 每个批次包含 8 个样本\n",
    "                   # -> 每次 DataLoader 返回的 input_ids 和 target_ids\n",
    "                   #    张量形状为 [8, 4]\n",
    "\n",
    "    max_length=4,  # max_length: 每个样本的上下文长度（即序列长度为 4 个 token）\n",
    "                   # -> 语言模型一次看到 4 个 token 来预测第 5 个 token\n",
    "\n",
    "    stride=4,      # stride: 滑动窗口步长（一次滑动 4 个 token）\n",
    "                   # -> 表示每个样本之间不重叠\n",
    "                   # -> 第一个样本是 tokens[0:4]，第二个样本是 tokens[4:8]，以此类推\n",
    "\n",
    "    shuffle=False  # shuffle=False：不打乱样本顺序，保持按文本顺序生成数据\n",
    "                   # -> 用于教学演示时方便观察 token 顺序变化\n",
    ")\n",
    "# DataLoader 内部会自动分批返回 (input_tensor_batch, target_tensor_batch)\n",
    "# 每次迭代会得到：\n",
    "#   input_tensor_batch  -> 模型输入（即上下文 token）\n",
    "#   target_tensor_batch -> 预测目标（即下一个 token）\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 创建迭代器对象\n",
    "# ------------------------------------------------\n",
    "data_iter = iter(dataloader)\n",
    "# - iter(dataloader):\n",
    "#     将 DataLoader 转换为一个 Python 可迭代对象\n",
    "# - 用于从 DataLoader 中逐批取出样本（即 batch）\n",
    "# - 每次执行 next(data_iter) 时，会返回下一个批次的数据\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 从迭代器中取出第一个批次的数据\n",
    "# ------------------------------------------------\n",
    "inputs, targets = next(data_iter)\n",
    "# - next(data_iter):\n",
    "#     从 dataloader 中取出第一个 batch\n",
    "# - 返回值是一个元组 (inputs, targets)\n",
    "# - 变量含义：\n",
    "#     inputs:  shape = [batch_size, max_length]\n",
    "#               每行是一个样本的输入 token 序列\n",
    "#     targets: shape = [batch_size, max_length]\n",
    "#               每行是输入序列右移一位后的目标 token 序列\n",
    "# - 举例说明：\n",
    "#     若 inputs 第1行为 [100, 200, 300, 400]\n",
    "#     则 targets 第1行为 [200, 300, 400, 500]\n",
    "#     模型任务：根据输入预测目标（即下一个 token）\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. 打印第一个批次的输入张量\n",
    "# ------------------------------------------------\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "# - 打印当前批次的输入 token ID 张量\n",
    "# - 张量形状：[8, 4]\n",
    "# - 用于查看 token 序列的整数编码形式\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5. 打印第一个批次的目标张量\n",
    "# ------------------------------------------------\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "# - 打印目标 token ID 张量\n",
    "# - 与 inputs 相同形状，但整体右移一个 token\n",
    "# - 教学要点：\n",
    "#     1) 帮助理解“语言模型的训练目标”\n",
    "#     2) 观察输入与输出的“预测关系”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1",
   "metadata": {},
   "source": [
    "## 2.7 创建 token 嵌入\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a301068-6ab2-44ff-a915-1ba11688274f",
   "metadata": {},
   "source": [
    "- 数据几乎已经准备好用于 LLM  \n",
    "- 最后，让我们使用嵌入层将 token 转换为连续的向量表示  \n",
    "- 通常，这些嵌入层是 LLM 本身的一部分，并会在模型训练过程中更新（训练）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e014ca-1fc5-4b90-b6fa-c2097bb92c0b",
   "metadata": {},
   "source": [
    "- 假设我们有以下四个输入示例，输入 ID 分别为 2、3、5 和 1（经过分词后）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15a6304c-9474-4470-b85d-3991a49fa653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 创建一个包含整数序列的 PyTorch 张量（tensor），\n",
    "# 通常用于模拟 token 的整数编码序列（input_ids）。\n",
    "# ================================================================\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 创建张量（tensor）\n",
    "# ------------------------------------------------\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "# - 语法：torch.tensor(data)\n",
    "# - 参数 data: \n",
    "#     一个 Python 列表、元组或嵌套结构（如二维列表）\n",
    "#     此处为 [2, 3, 5, 1]，即一个长度为 4 的一维整数列表\n",
    "# - 功能说明：\n",
    "#     将该列表转换为 PyTorch 的张量对象（Tensor）\n",
    "# - 默认数据类型：\n",
    "#     若列表中全为整数，则 dtype 默认为 torch.int64\n",
    "#     （也称为 torch.long，用于索引或 token ID）\n",
    "# - 教学背景：\n",
    "#     在语言模型（如 GPT）中，句子经过分词器 (tokenizer)\n",
    "#     后会变为整数序列，例如：\n",
    "#         “Hello world.” → [15496, 995, 13]\n",
    "#     这些整数即“token IDs”，通常存储在张量中以便送入模型。\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 张量的形状与内容\n",
    "# ------------------------------------------------\n",
    "# - input_ids 的形状（Shape）：torch.Size([4])\n",
    "#   表示为 1 维张量（向量），共 4 个元素\n",
    "# - 张量内容（Values）：[2, 3, 5, 1]\n",
    "# - 数据类型（dtype）：torch.int64（长整型）\n",
    "# - 存放设备（device）：\n",
    "#     默认在 CPU 上创建（除非手动指定 device='cuda'）\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 实际应用场景\n",
    "# ------------------------------------------------\n",
    "# - 在 NLP 模型训练中，该张量通常代表一个句子的 token 序列：\n",
    "#     例：\n",
    "#         input_ids = [2, 3, 5, 1]\n",
    "#         可能表示 \"I like tea <EOS>\"\n",
    "# - 模型会将这些整数查表（embedding lookup）\n",
    "#   转换成对应的向量嵌入（embeddings），\n",
    "#   然后送入神经网络进行训练或推理。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da6344-2c71-4837-858d-dd120005ba05",
   "metadata": {},
   "source": [
    "- 为简单起见，假设我们有一个仅包含 6 个单词的小词汇表，并且希望创建维度为 3 的嵌入向量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93cb2cee-9aa6-4bb8-8977-c65661d16eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 模块功能说明：\n",
    "# 创建一个用于词向量映射（embedding）的层（torch.nn.Embedding），\n",
    "# 用于将离散的 token ID（整数索引）映射为连续的向量表示。\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1. 定义词表大小（vocab_size）\n",
    "# ------------------------------------------------\n",
    "vocab_size = 6\n",
    "# - 含义：\n",
    "#     表示词汇表（vocabulary）的总 token 数量。\n",
    "# - 即模型可以识别的不同“单词”或“符号”的总数。\n",
    "# - 在此例中，假设词汇表中有 6 个唯一的 token，\n",
    "#   例如：\n",
    "#       0 → \"Hello\"\n",
    "#       1 → \"world\"\n",
    "#       2 → \"I\"\n",
    "#       3 → \"like\"\n",
    "#       4 → \"tea\"\n",
    "#       5 → \"<|endoftext|>\"\n",
    "# - 说明：\n",
    "#     每个 token 都会被分配一个唯一的整数 ID（0 到 5）。\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2. 定义词向量维度（output_dim）\n",
    "# ------------------------------------------------\n",
    "output_dim = 3\n",
    "# - 含义：\n",
    "#     每个 token 会被映射为一个 3 维向量（embedding vector）。\n",
    "# - 举例说明：\n",
    "#     若某个 token ID = 2，\n",
    "#     则 embedding 层会返回一个形状为 [3] 的张量，\n",
    "#     例如 [0.12, -0.05, 0.33]。\n",
    "# - 通常在实际模型中，embedding 维度可能设为 128、256、768 等较大值，\n",
    "#   但在教学或演示时可用较小值（如 3）方便观察。\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3. 固定随机数种子（manual_seed）\n",
    "# ------------------------------------------------\n",
    "torch.manual_seed(123)\n",
    "# - 含义：\n",
    "#     固定随机数生成器的初始状态，使结果可重复。\n",
    "# - PyTorch 中很多层（如 Embedding、Linear）初始化时权重是随机的。\n",
    "# - 通过设置随机种子：\n",
    "#     每次运行本代码，embedding 层初始化的参数都会相同，\n",
    "#     方便教学与调试。\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4. 创建词嵌入层（Embedding Layer）\n",
    "# ------------------------------------------------\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "# - 模块说明：\n",
    "#     torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "#     是 PyTorch 中专门用于“整数索引 → 连续向量”映射的层。\n",
    "# - 参数解释：\n",
    "#     num_embeddings = vocab_size\n",
    "#         表示词汇表中可查找的 token 数量（此处为 6）\n",
    "#     embedding_dim = output_dim\n",
    "#         表示每个 token 映射后的向量维度（此处为 3）\n",
    "# - 内部机制：\n",
    "#     创建一个形状为 [vocab_size, output_dim] 的可学习参数矩阵：\n",
    "#         权重矩阵 weight.shape = [6, 3]\n",
    "#     每一行对应一个 token 的向量表示。\n",
    "# - 示例：\n",
    "#     embedding_layer.weight 可能如下（随机初始化）：\n",
    "#       tensor([[ 0.3374, -0.3352,  0.1015],\n",
    "#               [-0.2652,  0.2919,  0.0536],\n",
    "#               [ 0.2631, -0.1948,  0.4081],\n",
    "#               [ 0.2316, -0.3052, -0.2831],\n",
    "#               [ 0.1130, -0.1020,  0.4342],\n",
    "#               [ 0.2748, -0.0529, -0.2417]])\n",
    "# - 使用方式：\n",
    "#     当输入 token ID 序列 [2, 3, 5] 传入时，\n",
    "#     embedding_layer 会自动返回对应行的向量：\n",
    "#       [[0.2631, -0.1948,  0.4081],\n",
    "#        [0.2316, -0.3052, -0.2831],\n",
    "#        [0.2748, -0.0529, -0.2417]]\n",
    "# - 教学要点：\n",
    "#     Embedding 层的作用类似一个“查表”操作：\n",
    "#     它不执行矩阵乘法，仅根据索引提取对应行。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff241f6-78eb-4e4a-a55f-5b2b6196d5b0",
   "metadata": {},
   "source": [
    "- 这将产生一个 6x3 的权重矩阵：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a686eb61-e737-4351-8f1c-222913d47468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 打印嵌入层（Embedding Layer）的权重矩阵\n",
    "# -------------------------------------------------------------\n",
    "# embedding_layer 是一个 torch.nn.Embedding 对象，用于将离散的词 ID 映射为连续的向量表示。\n",
    "# 每一个“词”或“token”在这个嵌入层中对应一行向量。\n",
    "# \n",
    "# 举例说明：\n",
    "#   若 vocab_size = 6，表示词汇表中共有 6 个不同的 token（编号 0~5）\n",
    "#   若 output_dim = 3，表示每个 token 被映射为 3 维的向量\n",
    "# 因此 embedding_layer.weight 的形状是 [6, 3]\n",
    "#\n",
    "# 这些权重最初是随机初始化的（因为我们调用了 torch.manual_seed(123)，所以结果可复现）\n",
    "# 在训练过程中，这些权重会被不断更新，以便更好地表示不同 token 的语义关系。\n",
    "#\n",
    "# 输出示例：\n",
    "# tensor([[ 0.1339, -0.2116,  0.3899],\n",
    "#         [-2.2323,  0.1258, -0.4833],\n",
    "#         [ 1.2467,  0.8545,  0.3032],\n",
    "#         [ 0.2348, -0.5379,  0.8874],\n",
    "#         [ 0.1145, -0.2932, -0.1399],\n",
    "#         [ 0.7143, -0.2890, -1.0458]])\n",
    "# 每一行表示一个词（token）的向量表示\n",
    "print(embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcf4f5-0801-4eb4-bb90-acce87935ac7",
   "metadata": {},
   "source": [
    "- 对于熟悉独热编码（one-hot encoding）的人来说，上述嵌入层方法本质上只是实现独热编码后进行矩阵乘法的一种更高效方式，这在补充代码 [./embedding_vs_matmul](../03_bonus_embedding-vs-matmul) 中有描述  \n",
    "- 由于嵌入层只是独热编码加矩阵乘法方法的更高效实现，因此它可以被视为一个神经网络层，可以通过反向传播进行优化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d58c3-83c0-4205-aca2-9c48b19fd4a7",
   "metadata": {},
   "source": [
    "- 要将 ID 为 3 的 token 转换为 3 维向量，我们执行以下操作：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e43600ba-f287-4746-8ddf-d0f71a9023ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 通过嵌入层（Embedding Layer）查询一个 token 的向量表示\n",
    "# -------------------------------------------------------------\n",
    "# torch.tensor([3])：\n",
    "#   创建一个包含单个整数 3 的张量。\n",
    "#   这个整数 3 表示词汇表中编号为 3 的 token。\n",
    "#   （假设 vocab_size = 6，则 token 的编号范围为 0~5）\n",
    "#\n",
    "# embedding_layer(torch.tensor([3]))：\n",
    "#   将输入的 token ID 张量传入嵌入层。\n",
    "#   PyTorch 会自动在 embedding_layer.weight 查找第 3 行的向量，\n",
    "#   即返回对应 token ID 的嵌入表示。\n",
    "#\n",
    "# 输出结果形状为 [1, output_dim]：\n",
    "#   - “1” 表示输入中有 1 个 token\n",
    "#   - “output_dim” 表示每个 token 被映射成的向量维度（此处为 3）\n",
    "#\n",
    "# 举例说明：\n",
    "#   假设 embedding_layer.weight 第 3 行为 [0.2348, -0.5379, 0.8874]\n",
    "#   则输出结果为：\n",
    "#   tensor([[ 0.2348, -0.5379,  0.8874]])\n",
    "#\n",
    "# 这个输出向量就是 token ID = 3 的语义表示（embedding）\n",
    "print(embedding_layer(torch.tensor([3])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbf625-4f36-491d-87b4-3969efb784b0",
   "metadata": {},
   "source": [
    "- 注意，上述向量对应 `embedding_layer` 权重矩阵的第 4 行  \n",
    "- 要嵌入上述四个 `input_ids` 的所有值，我们执行：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50280ead-0363-44c8-8c35-bb885d92c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 通过嵌入层（Embedding Layer）将多个 token ID 映射为向量表示\n",
    "# -------------------------------------------------------------\n",
    "# embedding_layer(input_ids)：\n",
    "#   将输入张量 input_ids 中的每个整数（token ID）映射为对应的向量。\n",
    "#   embedding_layer 会查找 embedding_layer.weight 的对应行。\n",
    "#\n",
    "# input_ids：\n",
    "#   是一个一维张量，例如：\n",
    "#   tensor([2, 3, 5, 1])\n",
    "#   表示有 4 个 token，ID 分别为 2、3、5、1。\n",
    "#\n",
    "# embedding_layer：\n",
    "#   是一个 nn.Embedding(vocab_size=6, output_dim=3)\n",
    "#   - vocab_size = 6：词汇表中共有 6 个不同的 token（编号 0~5）\n",
    "#   - output_dim = 3：每个 token 被映射成一个 3 维向量\n",
    "#\n",
    "# embedding_layer(input_ids) 的计算逻辑：\n",
    "#   逐个读取 input_ids 中的 token ID，\n",
    "#   查找 embedding_layer.weight 对应行的向量：\n",
    "#\n",
    "#   输出形状为 [len(input_ids), output_dim]：\n",
    "#       即 [4, 3]\n",
    "#\n",
    "#   例如：\n",
    "#   embedding_layer.weight =\n",
    "#   tensor([[-0.0869,  0.1345, -0.0892],\n",
    "#           [ 0.5422, -0.5231, -0.1597],\n",
    "#           [-0.2610, -0.2392,  0.4909],\n",
    "#           [ 0.3217, -0.1079,  0.4561],\n",
    "#           [ 0.1347,  0.2301, -0.5634],\n",
    "#           [-0.3382,  0.3178,  0.1586]])\n",
    "#\n",
    "#   input_ids = tensor([2, 3, 5, 1])\n",
    "#   则输出：\n",
    "#   tensor([[-0.2610, -0.2392,  0.4909],   # ID 2\n",
    "#           [ 0.3217, -0.1079,  0.4561],   # ID 3\n",
    "#           [-0.3382,  0.3178,  0.1586],   # ID 5\n",
    "#           [ 0.5422, -0.5231, -0.1597]])  # ID 1\n",
    "#\n",
    "# 总结：\n",
    "#   - 输入：token ID 序列 → tensor([2, 3, 5, 1])\n",
    "#   - 输出：每个 token 对应的向量 → 形状 [4, 3]\n",
    "#   - 这个过程相当于查表（Look-up Table）\n",
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97ced4-bd13-42b7-866a-4d699a17e155",
   "metadata": {},
   "source": [
    "- 嵌入层本质上就是一个查找操作：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c2741-bf1b-4c60-b7fd-61409d556646",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08218d9f-aa1a-4afb-a105-72ff96a54e73",
   "metadata": {},
   "source": [
    "- **你可能会对比较嵌入层与普通线性层的补充内容感兴趣：[../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6",
   "metadata": {},
   "source": [
    "## 2.8 编码单词位置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24940068-1099-4698-bdc0-e798515e2902",
   "metadata": {},
   "source": [
    "- 嵌入层将 ID 转换为向量表示时，不考虑它们在输入序列中的位置：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430",
   "metadata": {},
   "source": [
    "- 位置嵌入（Positional embeddings）与 token 嵌入向量结合，形成大语言模型的输入嵌入：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55",
   "metadata": {},
   "source": [
    "- BytePair 编码器的词汇表大小为 50,257  \n",
    "- 假设我们希望将输入 token 编码为 256 维向量表示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 定义 GPT 模型的“词嵌入层”（Token Embedding Layer）\n",
    "# -------------------------------------------------------------\n",
    "# nn.Embedding 是 PyTorch 中的嵌入层（Embedding Layer）类，\n",
    "# 作用：将“整数形式的 token ID”映射为“连续的向量表示”。\n",
    "# 这一步是 Transformer / GPT 模型中将文本转为数值向量的关键步骤。\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# 定义词汇表大小（vocab_size）\n",
    "vocab_size = 50257\n",
    "# 含义：\n",
    "#   - GPT-2 的词汇表大小为 50257（包括普通词、标点符号、特殊标记等）\n",
    "#   - 每个 token 都有一个唯一的 ID（范围：0 ~ 50256）\n",
    "#   - 例如：'Hello' → 15496,  'world' → 2159,  '<|endoftext|>' → 50256\n",
    "\n",
    "# 定义嵌入向量的维度（output_dim）\n",
    "output_dim = 256\n",
    "# 含义：\n",
    "#   - 每个 token 将被映射为一个 256 维的向量\n",
    "#   - 这个维度控制模型的容量（维度越高，模型能表达的信息越多，但计算量也更大）\n",
    "#   - GPT-2 的 small 版本通常使用 768 维，这里用 256 维是简化演示版\n",
    "\n",
    "# 创建嵌入层（Token Embedding Layer）\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "# 参数解释：\n",
    "#   - vocab_size：嵌入矩阵的“行数”，即词汇表大小（50257）\n",
    "#   - output_dim：嵌入矩阵的“列数”，即每个 token 的向量长度（256）\n",
    "#\n",
    "# 内部机制：\n",
    "#   - embedding_matrix = nn.Parameter(torch.empty(vocab_size, output_dim))\n",
    "#     也就是一个形状为 [50257, 256] 的矩阵\n",
    "#   - 每个 token ID 对应 embedding_matrix 的一行\n",
    "#   - 当我们输入 token ID（如 tensor([15496, 2159])）时，\n",
    "#     嵌入层会查找 embedding_matrix 中第 15496 行和第 2159 行，\n",
    "#     输出这两行对应的向量。\n",
    "#\n",
    "# 输出形状：\n",
    "#   输入形状：[sequence_length]\n",
    "#   输出形状：[sequence_length, output_dim]\n",
    "#\n",
    "# 举例说明：\n",
    "#   输入：tensor([15496, 2159])   # ['Hello', 'world']\n",
    "#   输出：tensor([[v1, v2, ..., v256],\n",
    "#                 [w1, w2, ..., w256]])   # 形状 [2, 256]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2654722-24e4-4b0d-a43c-436a461eb70b",
   "metadata": {},
   "source": [
    "- 如果从数据加载器中采样数据，我们将每个批次中的 token 嵌入为 256 维向量  \n",
    "- 如果批量大小为 8，每个批次有 4 个 token，则得到一个 8 x 4 x 256 的张量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 通过自定义函数 create_dataloader_v1 创建训练 DataLoader\n",
    "# ================================================================\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. 设置上下文长度（max_length）\n",
    "# -------------------------------------------------------------\n",
    "max_length = 4\n",
    "# - max_length 表示每个训练样本的上下文长度（sequence length）\n",
    "# - 对于语言模型：\n",
    "#     输入序列长度 = max_length\n",
    "#     输出目标序列长度 = max_length（右移一个 token）\n",
    "# - 例如：\n",
    "#     输入 token 序列：[t1, t2, t3, t4]\n",
    "#     目标 token 序列：[t2, t3, t4, t5]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. 创建 DataLoader\n",
    "# -------------------------------------------------------------\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,           # txt: 完整文本数据\n",
    "    batch_size=8,       # batch_size: 每批次包含 8 个样本\n",
    "    max_length=max_length,  # 每个样本序列长度为 4\n",
    "    stride=max_length,      # stride = max_length\n",
    "                            # -> 样本之间不重叠\n",
    "    shuffle=False        # 不打乱样本顺序\n",
    ")\n",
    "# DataLoader 功能：\n",
    "#   - 根据文本和滑动窗口生成输入序列和目标序列\n",
    "#   - 每次迭代返回一个 batch: (inputs, targets)\n",
    "#   - inputs.shape = [batch_size, max_length]\n",
    "#   - targets.shape = [batch_size, max_length]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. 创建迭代器\n",
    "# -------------------------------------------------------------\n",
    "data_iter = iter(dataloader)\n",
    "# - 将 DataLoader 转换为迭代器\n",
    "# - 可以通过 next() 逐批获取训练数据\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. 获取第一个批次数据\n",
    "# -------------------------------------------------------------\n",
    "inputs, targets = next(data_iter)\n",
    "# - inputs: [batch_size, max_length] 的张量，表示输入 token 序列\n",
    "# - targets: [batch_size, max_length] 的张量，表示右移一个 token 的目标序列\n",
    "# - 举例说明：\n",
    "#     若 batch_size=8，max_length=4：\n",
    "#     inputs.shape = [8, 4], targets.shape = [8, 4]\n",
    "#     每一行表示一个样本序列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 打印输入的 token ID 张量\n",
    "# -------------------------------------------------------------\n",
    "# inputs 是从 DataLoader 中获取的一个批次输入\n",
    "# 形状为 [batch_size, max_length]，每个元素是一个整数 token ID\n",
    "# 打印示例：\n",
    "# tensor([[15496,   995,    13,  50256],\n",
    "#         [  2159,  1337,   432,  50256],\n",
    "#         ...])\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 打印 inputs 张量的形状\n",
    "# -------------------------------------------------------------\n",
    "# inputs.shape:\n",
    "# - 输出张量的维度信息\n",
    "# - 形状为 [batch_size, max_length]\n",
    "# - 举例：\n",
    "#     batch_size = 8, max_length = 4\n",
    "#     则输出 torch.Size([8, 4])\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "tensor([[[ 0.4913,  1.1239,  1.4588,  ..., -0.3995, -1.8735, -0.1445],\n",
      "         [ 0.4481,  0.2536, -0.2655,  ...,  0.4997, -1.1991, -1.1844],\n",
      "         [-0.2507, -0.0546,  0.6687,  ...,  0.9618,  2.3737, -0.0528],\n",
      "         [ 0.9457,  0.8657,  1.6191,  ..., -0.4544, -0.7460,  0.3483]],\n",
      "\n",
      "        [[ 1.5460,  1.7368, -0.7848,  ..., -0.1004,  0.8584, -0.3421],\n",
      "         [-1.8622, -0.1914, -0.3812,  ...,  1.1220, -0.3496,  0.6091],\n",
      "         [ 1.9847, -0.6483, -0.1415,  ..., -0.3841, -0.9355,  1.4478],\n",
      "         [ 0.9647,  1.2974, -1.6207,  ...,  1.1463,  1.5797,  0.3969]],\n",
      "\n",
      "        [[-0.7713,  0.6572,  0.1663,  ..., -0.8044,  0.0542,  0.7426],\n",
      "         [ 0.8046,  0.5047,  1.2922,  ...,  1.4648,  0.4097,  0.3205],\n",
      "         [ 0.0795, -1.7636,  0.5750,  ...,  2.1823,  1.8231, -0.3635],\n",
      "         [ 0.4267, -0.0647,  0.5686,  ..., -0.5209,  1.3065,  0.8473]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6156,  0.9610, -2.6437,  ..., -0.9645,  1.0888,  1.6383],\n",
      "         [-0.3985, -0.9235, -1.3163,  ..., -1.1582, -1.1314,  0.9747],\n",
      "         [ 0.6089,  0.5329,  0.1980,  ..., -0.6333, -1.1023,  1.6292],\n",
      "         [ 0.3677, -0.1701, -1.3787,  ...,  0.7048,  0.5028, -0.0573]],\n",
      "\n",
      "        [[-0.1279,  0.6154,  1.7173,  ...,  0.3789, -0.4752,  1.5258],\n",
      "         [ 0.4861, -1.7105,  0.4416,  ...,  0.1475, -1.8394,  1.8755],\n",
      "         [-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n",
      "         [ 0.2002, -0.7605, -1.5170,  ..., -0.0305, -0.3656, -0.1398]],\n",
      "\n",
      "        [[-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n",
      "         [-0.0632, -0.6548, -1.0296,  ..., -0.9538, -0.5026, -0.1128],\n",
      "         [ 0.6032,  0.8983,  2.0722,  ...,  1.5242,  0.2030, -0.3002],\n",
      "         [ 1.1274, -0.1082, -0.2195,  ...,  0.5059, -1.8138, -0.0700]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 将输入的 token IDs 转换为向量表示（Token Embeddings）\n",
    "# ================================================================\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. 使用词嵌入层（Embedding Layer）将 token IDs 映射为向量\n",
    "# -------------------------------------------------------------\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "# - token_embedding_layer: nn.Embedding(vocab_size, output_dim)\n",
    "#   - vocab_size = 50257\n",
    "#   - output_dim = 256\n",
    "# - inputs: [batch_size, max_length] 的整数张量\n",
    "# - token_embeddings 的输出：\n",
    "#   - 每个 token ID 对应 embedding_matrix 的一行向量\n",
    "#   - 输出张量形状 = [batch_size, max_length, output_dim]\n",
    "#   - 举例：\n",
    "#       inputs.shape = [8, 4]\n",
    "#       output token_embeddings.shape = [8, 4, 256]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. 打印 token_embeddings 的形状\n",
    "# -------------------------------------------------------------\n",
    "# 了解嵌入结果的维度信息\n",
    "print(token_embeddings.shape)\n",
    "# 输出示例：\n",
    "# torch.Size([8, 4, 256])\n",
    "# - 8: 批次大小（batch_size）\n",
    "# - 4: 序列长度（max_length）\n",
    "# - 256: 每个 token 的嵌入向量维度（output_dim）\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. 打印具体的嵌入向量\n",
    "# -------------------------------------------------------------\n",
    "# - token_embeddings 中的每个元素都是一个 256 维向量\n",
    "# - 仅用于教学观察，实际训练时通常不直接打印\n",
    "print(token_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f",
   "metadata": {},
   "source": [
    "- GPT-2 使用绝对位置嵌入，因此我们只需创建另一个嵌入层：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 创建位置嵌入层（Positional Embedding Layer）\n",
    "# ================================================================\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. 设置上下文长度（context_length）\n",
    "# -------------------------------------------------------------\n",
    "context_length = max_length\n",
    "# - context_length 表示序列的最大长度\n",
    "# - 位置嵌入的作用：为序列中的每个位置添加唯一的向量表示\n",
    "# - 举例：\n",
    "#     如果 max_length = 4，则 context_length = 4\n",
    "#     对应 4 个位置的嵌入：位置 0、1、2、3\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. 创建位置嵌入层\n",
    "# -------------------------------------------------------------\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "# - 参数解释：\n",
    "#   - num_embeddings = context_length\n",
    "#       表示有多少个不同的位置（0 ~ context_length-1）\n",
    "#   - embedding_dim = output_dim\n",
    "#       每个位置对应的向量维度（与 token embedding 维度相同，便于相加）\n",
    "# - 输出：\n",
    "#   pos_embedding_layer.weight.shape = [context_length, output_dim]\n",
    "# - 内部机制：\n",
    "#   - 为每个位置生成一个可学习的向量\n",
    "#   - 在 Transformer/GPT 模型中，位置向量会与 token 嵌入向量相加\n",
    "#     从而保留序列位置信息\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. 打印位置嵌入层权重\n",
    "# -------------------------------------------------------------\n",
    "# - 每一行表示一个位置（0~context_length-1）的向量\n",
    "# - 用于教学观察\n",
    "print(pos_embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 获取序列中每个位置的嵌入向量（Positional Embeddings）\n",
    "# ================================================================\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. 生成位置索引张量\n",
    "# -------------------------------------------------------------\n",
    "# torch.arange(max_length)：\n",
    "# - 创建一个从 0 到 max_length-1 的整数序列\n",
    "# - 形状：[max_length]\n",
    "# - 举例：\n",
    "#     如果 max_length = 4，则 torch.arange(max_length) = tensor([0, 1, 2, 3])\n",
    "# - 每个整数表示序列中的一个位置 ID\n",
    "#   位置 0 对应第一个 token，位置 1 对应第二个 token，以此类推\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. 查询位置嵌入\n",
    "# -------------------------------------------------------------\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "# - pos_embedding_layer: nn.Embedding(context_length, output_dim)\n",
    "# - 输入：位置 ID 张量 [0, 1, 2, ..., max_length-1]\n",
    "# - 输出：\n",
    "#   - 每个位置 ID 对应一个 output_dim 维向量\n",
    "#   - 输出形状：[max_length, output_dim]\n",
    "#   - 举例：\n",
    "#       max_length = 4, output_dim = 256\n",
    "#       pos_embeddings.shape = [4, 256]\n",
    "# - 作用：\n",
    "#   - 给每个 token 添加唯一的位置信息\n",
    "#   - 在 GPT/Transformer 中，通常将 token_embedding + pos_embedding 作为最终输入\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. 打印 pos_embeddings 的形状\n",
    "# -------------------------------------------------------------\n",
    "print(pos_embeddings.shape)\n",
    "# 输出示例：\n",
    "# torch.Size([4, 256])\n",
    "# - 4: 序列长度（max_length）\n",
    "# - 256: 嵌入维度（output_dim）\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. 打印具体位置嵌入向量\n",
    "# -------------------------------------------------------------\n",
    "# - pos_embeddings 中每一行是一个位置向量\n",
    "# - 用于教学观察\n",
    "print(pos_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788b2ca-a253-4647-9845-c7b219db2589",
   "metadata": {},
   "source": [
    "| 名称                    | 类型           | 形状                           | 用途                  |\n",
    "| --------------------- | ------------ | ---------------------------- | ------------------- |\n",
    "| `pos_embedding_layer` | nn.Embedding | [context_length, output_dim] | 可训练层本身，存储位置向量       |\n",
    "| `pos_embeddings`      | torch.Tensor | [max_length, output_dim]     | 经过查表后的具体位置向量，用于模型计算 |\n",
    "\n",
    "\n",
    "一句话理解：\n",
    "\n",
    "pos_embedding_layer 是“位置向量表”（表格 + 可训练），\n",
    "\n",
    "pos_embeddings 是“具体取出的行向量”，可以直接与 token embedding 相加。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e9d9f-2935-461a-9518-6d1386b976d6",
   "metadata": {},
   "source": [
    "- 要创建 LLM 使用的输入嵌入，我们只需将 token 嵌入与位置嵌入相加：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b22fab89-526e-43c8-9035-5b7018e34288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "tensor([[[ 2.2288,  0.5619,  0.8286,  ..., -0.6272, -0.2987,  0.8900],\n",
      "         [ 2.0903, -0.4664, -0.0593,  ...,  0.9115, -1.0493, -1.6473],\n",
      "         [-0.7158, -0.8304,  1.2494,  ...,  2.3952,  1.8773,  0.8051],\n",
      "         [ 0.2703,  0.4029,  3.0514,  ...,  0.3595, -1.4548,  0.8310]],\n",
      "\n",
      "        [[ 3.2835,  1.1749, -1.4150,  ..., -0.3281,  2.4332,  0.6924],\n",
      "         [-0.2199, -0.9114, -0.1750,  ...,  1.5337, -0.1998,  0.1462],\n",
      "         [ 1.5197, -1.4240,  0.4391,  ...,  1.0494, -1.4318,  2.3057],\n",
      "         [ 0.2893,  0.8346, -0.1884,  ...,  1.9602,  0.8709,  0.8796]],\n",
      "\n",
      "        [[ 0.9662,  0.0952, -0.4640,  ..., -1.0320,  1.6290,  1.7771],\n",
      "         [ 2.4468, -0.2154,  1.4984,  ...,  1.8766,  0.5595, -0.1423],\n",
      "         [-0.3856, -2.5393,  1.1556,  ...,  3.6157,  1.3267,  0.4944],\n",
      "         [-0.2487, -0.5275,  2.0009,  ...,  0.2930,  0.5977,  1.3300]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1219,  0.3991, -3.2740,  ..., -1.1921,  2.6637,  2.6728],\n",
      "         [ 1.2438, -1.6436, -1.1101,  ..., -0.7464, -0.9816,  0.5118],\n",
      "         [ 0.1439, -0.2428,  0.7786,  ...,  0.8001, -1.5986,  2.4871],\n",
      "         [-0.3077, -0.6329,  0.0536,  ...,  1.5188, -0.2060,  0.4254]],\n",
      "\n",
      "        [[ 1.6095,  0.0535,  1.0871,  ...,  0.1512,  1.0996,  2.5603],\n",
      "         [ 2.1284, -2.4306,  0.6478,  ...,  0.5593, -1.6896,  1.4126],\n",
      "         [-1.4224, -0.0750,  1.9386,  ...,  3.3712, -2.4016, -0.3237],\n",
      "         [-0.4752, -1.2234, -0.0847,  ...,  0.7834, -1.0744,  0.3429]],\n",
      "\n",
      "        [[ 0.7802,  0.1387,  0.7277,  ...,  1.7101, -0.3304, -0.1471],\n",
      "         [ 1.5791, -1.3749, -0.8234,  ..., -0.5420, -0.3528, -0.5756],\n",
      "         [ 0.1382,  0.1226,  2.6528,  ...,  2.9576, -0.2933,  0.5577],\n",
      "         [ 0.4520, -0.5711,  1.2128,  ...,  1.3198, -2.5226,  0.4127]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 将 token 嵌入与位置嵌入相加，形成最终输入向量\n",
    "# ================================================================\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. 形状对齐\n",
    "# -------------------------------------------------------------\n",
    "# token_embeddings.shape = [batch_size, max_length, output_dim]\n",
    "# pos_embeddings.shape   = [max_length, output_dim]\n",
    "# 为了能相加，PyTorch 会自动广播（broadcasting）：\n",
    "#   pos_embeddings 会在 batch_size 维度上复制\n",
    "#   形状对齐后：[batch_size, max_length, output_dim]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. 相加\n",
    "# -------------------------------------------------------------\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "# - 每个 token 的最终输入向量 = token embedding + position embedding\n",
    "# - 这样模型既知道 token 的语义，也知道 token 在序列中的位置\n",
    "# - output shape = [batch_size, max_length, output_dim]\n",
    "#   - batch_size: 批次大小\n",
    "#   - max_length: 序列长度\n",
    "#   - output_dim: 嵌入向量维度\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. 打印形状\n",
    "# -------------------------------------------------------------\n",
    "print(input_embeddings.shape)\n",
    "# 输出示例：\n",
    "# torch.Size([8, 4, 256])\n",
    "# - 8: batch_size\n",
    "# - 4: max_length\n",
    "# - 256: embedding dimension\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. 打印具体向量\n",
    "# -------------------------------------------------------------\n",
    "# - input_embeddings 每一行是 token embedding + position embedding\n",
    "# - 用于模型输入\n",
    "print(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbda581-6f9b-476f-8ea7-d244e6a4eaec",
   "metadata": {},
   "source": [
    "- 在输入处理工作流程的初始阶段，输入文本被分割成单独的 token  \n",
    "- 在分割之后，这些 token 会根据预定义词汇表转换为 token ID：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb0f7e-460d-44db-b366-096adcd84fff",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63230f2e-258f-4497-9e2e-8deee4530364",
   "metadata": {},
   "source": [
    "# 总结与收获\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3293a6-45a5-47cd-aa00-b23e3ca0a73f",
   "metadata": {},
   "source": [
    "请参阅 [./dataloader.ipynb](./dataloader.ipynb) 代码笔记本，这是本章中实现的数据加载器的简明版本，在后续章节训练 GPT 模型时会用到。\n",
    "\n",
    "请参阅 [./exercise-solutions.ipynb](./exercise-solutions.ipynb) 获取练习题答案。\n",
    "\n",
    "如果你有兴趣了解 GPT-2 的 tokenizer 如何从零实现和训练，请参阅 [Byte Pair Encoding (BPE) Tokenizer From Scratch](../02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb) 笔记本。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
